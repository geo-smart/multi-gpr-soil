{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b011d16-5e73-46f5-9e0d-3443ec2ff42b",
   "metadata": {},
   "source": [
    "# Pytorch Multi-output GPR\n",
    "\n",
    "The purpose of this notebook is to predict the soil type and soil thickness of Layer 1 as a Multi-Output GP model using a ModelList. Unlike a Multi-Task model, Multi-Output models do not represent correlations between outcomes, but treat outcomes independently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb804bf-e961-4804-a088-3f113bb7381f",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d69f4200-2066-4eab-96a2-c7eba0136384",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch --quiet\n",
    "!pip install gpytorch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e264b31-4228-4088-9235-9c5cc3f99f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.models import ApproximateGP, ExactGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "from gpytorch.likelihoods import SoftmaxLikelihood, GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13712b6b-e473-4e08-861a-1a4885d4eeff",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa664a05-0bd2-453a-9e5d-9068ce2cd200",
   "metadata": {},
   "source": [
    "#### Load training data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f4ce29-8f04-48e9-9944-01b8f6e4d434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOREHOLE_ID</th>\n",
       "      <th>BOREHOLE_NAME</th>\n",
       "      <th>BOREHOLE_TYPE</th>\n",
       "      <th>BOREHOLE_DEPTH_FT</th>\n",
       "      <th>ELEVATION_FT</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LAYER_NUMBER</th>\n",
       "      <th>TOP_DEPTH_FT</th>\n",
       "      <th>BOTTOM_DEPTH_FT</th>\n",
       "      <th>USCS</th>\n",
       "      <th>SIMPLE_USCS</th>\n",
       "      <th>LAYER_THICKNESS_FT</th>\n",
       "      <th>geometry</th>\n",
       "      <th>MAPPED_UNIT</th>\n",
       "      <th>SLOPE</th>\n",
       "      <th>ROUGHNESS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7138</td>\n",
       "      <td>B-1</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>20.3</td>\n",
       "      <td>167.8</td>\n",
       "      <td>47.656719</td>\n",
       "      <td>-122.305728</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SP</td>\n",
       "      <td>S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>POINT (-122.30573 47.65672)</td>\n",
       "      <td>Qvt</td>\n",
       "      <td>3.668571</td>\n",
       "      <td>18.199593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7143</td>\n",
       "      <td>B-1-92</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>48.5</td>\n",
       "      <td>123.2</td>\n",
       "      <td>47.653642</td>\n",
       "      <td>-122.306837</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ML</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "      <td>POINT (-122.30684 47.65364)</td>\n",
       "      <td>Qvt</td>\n",
       "      <td>2.222722</td>\n",
       "      <td>10.232103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7144</td>\n",
       "      <td>B-2-92</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>42.5</td>\n",
       "      <td>122.9</td>\n",
       "      <td>47.653766</td>\n",
       "      <td>-122.306468</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ML</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "      <td>POINT (-122.30647 47.65377)</td>\n",
       "      <td>Qvt</td>\n",
       "      <td>1.996012</td>\n",
       "      <td>9.385303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7145</td>\n",
       "      <td>B-3-92</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>43.0</td>\n",
       "      <td>117.3</td>\n",
       "      <td>47.653256</td>\n",
       "      <td>-122.306638</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ML</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "      <td>POINT (-122.30664 47.65326)</td>\n",
       "      <td>Qvt</td>\n",
       "      <td>3.011285</td>\n",
       "      <td>14.187020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7146</td>\n",
       "      <td>B-4-92</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>43.0</td>\n",
       "      <td>123.4</td>\n",
       "      <td>47.653709</td>\n",
       "      <td>-122.306259</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>SM</td>\n",
       "      <td>S</td>\n",
       "      <td>0.5</td>\n",
       "      <td>POINT (-122.30626 47.65371)</td>\n",
       "      <td>Qvt</td>\n",
       "      <td>1.996012</td>\n",
       "      <td>9.385303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>143845</td>\n",
       "      <td>MW-1</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>42.0</td>\n",
       "      <td>217.4</td>\n",
       "      <td>47.661044</td>\n",
       "      <td>-122.342453</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>SM</td>\n",
       "      <td>S</td>\n",
       "      <td>7.5</td>\n",
       "      <td>POINT (-122.34245 47.66104)</td>\n",
       "      <td>Qvr</td>\n",
       "      <td>1.594538</td>\n",
       "      <td>10.106370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>143846</td>\n",
       "      <td>MW-2</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>43.0</td>\n",
       "      <td>218.2</td>\n",
       "      <td>47.661138</td>\n",
       "      <td>-122.342516</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>SM</td>\n",
       "      <td>S</td>\n",
       "      <td>7.5</td>\n",
       "      <td>POINT (-122.34252 47.66114)</td>\n",
       "      <td>Qvr</td>\n",
       "      <td>1.340698</td>\n",
       "      <td>8.735672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>143847</td>\n",
       "      <td>MW-3</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>45.5</td>\n",
       "      <td>219.3</td>\n",
       "      <td>47.661276</td>\n",
       "      <td>-122.342492</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>SM</td>\n",
       "      <td>S</td>\n",
       "      <td>18.0</td>\n",
       "      <td>POINT (-122.34249 47.66128)</td>\n",
       "      <td>Qvr</td>\n",
       "      <td>1.594538</td>\n",
       "      <td>10.106370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>143848</td>\n",
       "      <td>MW-4</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>43.0</td>\n",
       "      <td>219.1</td>\n",
       "      <td>47.661177</td>\n",
       "      <td>-122.342383</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>SM</td>\n",
       "      <td>S</td>\n",
       "      <td>10.0</td>\n",
       "      <td>POINT (-122.34238 47.66118)</td>\n",
       "      <td>Qvr</td>\n",
       "      <td>1.594538</td>\n",
       "      <td>10.106370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>143849</td>\n",
       "      <td>MW-5</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>43.0</td>\n",
       "      <td>218.9</td>\n",
       "      <td>47.661273</td>\n",
       "      <td>-122.342365</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>SM</td>\n",
       "      <td>S</td>\n",
       "      <td>8.5</td>\n",
       "      <td>POINT (-122.34237 47.66127)</td>\n",
       "      <td>Qvr</td>\n",
       "      <td>1.594538</td>\n",
       "      <td>10.106370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BOREHOLE_ID BOREHOLE_NAME BOREHOLE_TYPE  BOREHOLE_DEPTH_FT  ELEVATION_FT  \\\n",
       "0           7138           B-1  Geotechnical               20.3         167.8   \n",
       "1           7143        B-1-92  Geotechnical               48.5         123.2   \n",
       "2           7144        B-2-92  Geotechnical               42.5         122.9   \n",
       "3           7145        B-3-92  Geotechnical               43.0         117.3   \n",
       "4           7146        B-4-92  Geotechnical               43.0         123.4   \n",
       "..           ...           ...           ...                ...           ...   \n",
       "501       143845          MW-1  Geotechnical               42.0         217.4   \n",
       "502       143846          MW-2  Geotechnical               43.0         218.2   \n",
       "503       143847          MW-3  Geotechnical               45.5         219.3   \n",
       "504       143848          MW-4  Geotechnical               43.0         219.1   \n",
       "505       143849          MW-5  Geotechnical               43.0         218.9   \n",
       "\n",
       "      LATITUDE   LONGITUDE  LAYER_NUMBER  TOP_DEPTH_FT  BOTTOM_DEPTH_FT USCS  \\\n",
       "0    47.656719 -122.305728             1           0.0              1.0   SP   \n",
       "1    47.653642 -122.306837             1           0.0              1.0   ML   \n",
       "2    47.653766 -122.306468             1           0.0              1.0   ML   \n",
       "3    47.653256 -122.306638             1           0.0              1.0   ML   \n",
       "4    47.653709 -122.306259             1           0.0              0.5   SM   \n",
       "..         ...         ...           ...           ...              ...  ...   \n",
       "501  47.661044 -122.342453             1           0.0              7.5   SM   \n",
       "502  47.661138 -122.342516             1           0.0              7.5   SM   \n",
       "503  47.661276 -122.342492             1           0.0             18.0   SM   \n",
       "504  47.661177 -122.342383             1           0.0             10.0   SM   \n",
       "505  47.661273 -122.342365             1           0.0              8.5   SM   \n",
       "\n",
       "    SIMPLE_USCS  LAYER_THICKNESS_FT                     geometry MAPPED_UNIT  \\\n",
       "0             S                 1.0  POINT (-122.30573 47.65672)         Qvt   \n",
       "1             M                 1.0  POINT (-122.30684 47.65364)         Qvt   \n",
       "2             M                 1.0  POINT (-122.30647 47.65377)         Qvt   \n",
       "3             M                 1.0  POINT (-122.30664 47.65326)         Qvt   \n",
       "4             S                 0.5  POINT (-122.30626 47.65371)         Qvt   \n",
       "..          ...                 ...                          ...         ...   \n",
       "501           S                 7.5  POINT (-122.34245 47.66104)         Qvr   \n",
       "502           S                 7.5  POINT (-122.34252 47.66114)         Qvr   \n",
       "503           S                18.0  POINT (-122.34249 47.66128)         Qvr   \n",
       "504           S                10.0  POINT (-122.34238 47.66118)         Qvr   \n",
       "505           S                 8.5  POINT (-122.34237 47.66127)         Qvr   \n",
       "\n",
       "        SLOPE  ROUGHNESS  \n",
       "0    3.668571  18.199593  \n",
       "1    2.222722  10.232103  \n",
       "2    1.996012   9.385303  \n",
       "3    3.011285  14.187020  \n",
       "4    1.996012   9.385303  \n",
       "..        ...        ...  \n",
       "501  1.594538  10.106370  \n",
       "502  1.340698   8.735672  \n",
       "503  1.594538  10.106370  \n",
       "504  1.594538  10.106370  \n",
       "505  1.594538  10.106370  \n",
       "\n",
       "[506 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training data CSV file compiled in `1-data_access.ipynb`\n",
    "input_file = '../data/2-uw_layer1_trainingdata.csv'\n",
    "training_data = pd.read_csv(input_file)\n",
    "\n",
    "# Create a GeoDataFrame from the DataFrame\n",
    "training_data = gpd.GeoDataFrame(training_data, geometry=gpd.points_from_xy(training_data.LONGITUDE, training_data.LATITUDE))\n",
    "    \n",
    "# Set the CRS to WGS84 (latitude and longitude)\n",
    "training_data.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb9989-deeb-486b-8e0f-fe3f457fd7f6",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b471b6f-3014-4f81-9d7b-379e2d8e682c",
   "metadata": {},
   "source": [
    "#### Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ffa9f1-ed2e-460a-8148-035ba4f004fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BOREHOLE_ID', 'BOREHOLE_NAME', 'BOREHOLE_TYPE', 'BOREHOLE_DEPTH_FT',\n",
       "       'ELEVATION_FT', 'LATITUDE', 'LONGITUDE', 'LAYER_NUMBER', 'TOP_DEPTH_FT',\n",
       "       'BOTTOM_DEPTH_FT', 'USCS', 'SIMPLE_USCS', 'LAYER_THICKNESS_FT',\n",
       "       'geometry', 'MAPPED_UNIT', 'SLOPE', 'ROUGHNESS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c46a3a7a-5e3a-4e14-b974-91b4a6720104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506\n",
      "504\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))\n",
    "training_data = training_data[training_data['USCS']!='CH']\n",
    "training_data = training_data[training_data['USCS']!='GP-GM']\n",
    "\n",
    "print(len(training_data))\n",
    "# Assuming training_data is your DataFrame\n",
    "X = training_data[['MAPPED_UNIT', 'SLOPE', 'ROUGHNESS']]\n",
    "y_1 = training_data['USCS']\n",
    "y_2 = training_data['LAYER_THICKNESS_FT']\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), ['MAPPED_UNIT']),  # One-hot encode categorical features\n",
    "        ('num', StandardScaler(), ['SLOPE', 'ROUGHNESS'])  # Standardize numerical features\n",
    "    ])\n",
    "\n",
    "# Apply transformations and convert to dense array\n",
    "X_processed = preprocessor.fit_transform(X).toarray()  # Convert to array if sparse\n",
    "\n",
    "# Convert processed data to tensors\n",
    "X_tensor = torch.tensor(X_processed, dtype=torch.float32)\n",
    "## When dataset gets large and the one-hot encoding results in a sparse matrix, consider using a sparse tensor in PyTorch. This can help with memory efficiency\n",
    "# X_tensor = torch.tensor(X_processed.todense(), dtype=torch.float32) if scipy.sparse.issparse(X_processed) else torch.tensor(X_processed, dtype=torch.float32) \n",
    "\n",
    "# Handling target for SIMPLE_USCS\n",
    "label_encoder = LabelEncoder()\n",
    "y_1_encoded = label_encoder.fit_transform(y_1) # Ensure target is in integer form\n",
    "y_1_tensor = torch.tensor(y_1_encoded, dtype=torch.long) # Convert to tensor\n",
    "\n",
    "# Handling target for LAYER_THICKNESS_FT\n",
    "scaler = StandardScaler()\n",
    "y_2_scaled = scaler.fit_transform(y_2.values.reshape(-1, 1))\n",
    "y_2_tensor = torch.tensor(y_2_scaled, dtype=torch.float32).squeeze() \n",
    "\n",
    "# Split the data into training and test sets for both models\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_tensor, y_1_tensor, test_size=0.2, random_state=42, stratify=y_1)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_tensor, y_2_tensor, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "225f371b-ed50-4f84-a6bd-e100f1b876fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'G': 'Course',\n",
       " 'S': 'Course',\n",
       " 'M': 'Fine',\n",
       " 'C': 'Fine',\n",
       " 'O': 'Fine',\n",
       " 'P': 'Organic'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = {} # empty dict for parent classes\n",
    "root['G'] = 'Course'\n",
    "root['S'] = 'Course'\n",
    "root['M'] = 'Fine'\n",
    "root['C'] = 'Fine'\n",
    "root['O'] = 'Fine'\n",
    "root['P'] = 'Organic'\n",
    "\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ebd61f-470e-49c5-b9af-e71bf7180a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CL', 'GM', 'GP', 'GW', 'MH', 'ML', 'OH', 'OL', 'PT', 'SC', 'SM',\n",
       "       'SP', 'SP-SM', 'SW', 'SW-SM'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(label_encoder.inverse_transform(y_1_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ef35e99-112c-4e01-a699-680c84fd94c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.transform(['GM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b08ee339-cfd4-4f0e-947c-2317596978e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CL': 'C',\n",
       " 'GM': 'G',\n",
       " 'GP': 'G',\n",
       " 'GW': 'G',\n",
       " 'MH': 'M',\n",
       " 'ML': 'M',\n",
       " 'OH': 'O',\n",
       " 'OL': 'O',\n",
       " 'PT': 'P',\n",
       " 'SC': 'S',\n",
       " 'SM': 'S',\n",
       " 'SP': 'S',\n",
       " 'SP-SM': 'S',\n",
       " 'SW': 'S',\n",
       " 'SW-SM': 'S'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent = {}\n",
    "for i, label in enumerate(np.unique(label_encoder.inverse_transform(y_1_encoded))):\n",
    "    parent[label] = label[0]\n",
    "parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac7ead20-7840-4c4f-b4df-ee9efd93118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = {}\n",
    "root['G'] = 'Course'\n",
    "root['S'] = 'Course'\n",
    "root['M'] = 'Fine'    \n",
    "root['C'] = 'Fine'\n",
    "root['O'] = 'Fine'\n",
    "root['P'] = 'Organic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "340a3725-457e-4ffe-bdb3-f3ba152bf81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(ypred, ytrue):\n",
    "    # evaluate results\n",
    "    ycheck = ypred - ytrue\n",
    "    \n",
    "    # transform results\n",
    "    ypred = label_encoder.inverse_transform(ypred)\n",
    "    ytrue = label_encoder.inverse_transform(ytrue)\n",
    "\n",
    "    print(ypred[0])\n",
    "    \n",
    "    sim, loss = 1, 0 # init similarity and loss values\n",
    "    for i in range(len(ycheck)):\n",
    "        parent_pred, parent_true = parent[ypred[i]], parent[ytrue[i]]\n",
    "        root_pred, root_true = root[parent_pred], root[parent_true]\n",
    "        \n",
    "        if int(ycheck[i]) == 0:\n",
    "            continue\n",
    "        elif parent_pred == parent_true:\n",
    "            sim = 0.5\n",
    "        elif root_pred == root_true:\n",
    "            sim = 0.25\n",
    "        else:\n",
    "            sim = 0\n",
    "\n",
    "        loss += (1-sim)*0.1\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90866c21-8195-430d-8e42-61261992ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPClassificationModel(ApproximateGP):\n",
    "    def __init__(self, train_x, num_classes):\n",
    "        variational_distribution = CholeskyVariationalDistribution(train_x.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, train_x, variational_distribution, learn_inducing_locations=True)\n",
    "        super(GPClassificationModel, self).__init__(variational_strategy)\n",
    "        \n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x).expand([self.num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09725995-8ee9-4094-a996-483cfa12c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the likelihood and model for multiclass classification\n",
    "num_classes = len(label_encoder.classes_)\n",
    "likelihood_1 = gpytorch.likelihoods.SoftmaxLikelihood(num_classes=num_classes, mixing_weights=None)\n",
    "model_1 = GPClassificationModel(X_train_1, num_classes=num_classes)\n",
    "\n",
    "# Initialize the likelihood and model for regression\n",
    "# likelihood_2 = GaussianLikelihood()\n",
    "# model_2 = GPRegressionModel(X_train_2, y_train_2, likelihood_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31ec59-1b17-4549-b339-1790ae7c1f2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL\n",
      "Classification Model Iter 1/1000 - Loss: 41.75834655761719\n",
      "CL\n",
      "Classification Model Iter 2/1000 - Loss: 90.03057861328125\n",
      "CL\n",
      "Classification Model Iter 3/1000 - Loss: 105.72122192382812\n",
      "CL\n",
      "Classification Model Iter 4/1000 - Loss: 80.48345947265625\n",
      "CL\n",
      "Classification Model Iter 5/1000 - Loss: 47.65947341918945\n",
      "CL\n",
      "Classification Model Iter 6/1000 - Loss: 60.3503532409668\n",
      "CL\n",
      "Classification Model Iter 7/1000 - Loss: 63.50490188598633\n",
      "CL\n",
      "Classification Model Iter 8/1000 - Loss: 55.07194519042969\n",
      "CL\n",
      "Classification Model Iter 9/1000 - Loss: 51.54804992675781\n",
      "CL\n",
      "Classification Model Iter 10/1000 - Loss: 51.795955657958984\n",
      "CL\n",
      "Classification Model Iter 11/1000 - Loss: 50.10940170288086\n",
      "CL\n",
      "Classification Model Iter 12/1000 - Loss: 48.77857208251953\n",
      "CL\n",
      "Classification Model Iter 13/1000 - Loss: 49.562435150146484\n",
      "CL\n",
      "Classification Model Iter 14/1000 - Loss: 49.9392204284668\n",
      "CL\n",
      "Classification Model Iter 15/1000 - Loss: 47.89002227783203\n",
      "CL\n",
      "Classification Model Iter 16/1000 - Loss: 45.30705642700195\n",
      "CL\n",
      "Classification Model Iter 17/1000 - Loss: 44.81557083129883\n",
      "CL\n",
      "Classification Model Iter 18/1000 - Loss: 45.981346130371094\n",
      "CL\n",
      "Classification Model Iter 19/1000 - Loss: 46.54153060913086\n",
      "CL\n",
      "Classification Model Iter 20/1000 - Loss: 45.64992904663086\n",
      "CL\n",
      "Classification Model Iter 21/1000 - Loss: 44.335941314697266\n",
      "CL\n",
      "Classification Model Iter 22/1000 - Loss: 43.655521392822266\n",
      "CL\n",
      "Classification Model Iter 23/1000 - Loss: 43.64628219604492\n",
      "CL\n",
      "Classification Model Iter 24/1000 - Loss: 43.85693359375\n",
      "CL\n",
      "Classification Model Iter 25/1000 - Loss: 43.911705017089844\n",
      "CL\n",
      "Classification Model Iter 26/1000 - Loss: 43.64326858520508\n",
      "CL\n",
      "Classification Model Iter 27/1000 - Loss: 43.13525390625\n",
      "CL\n",
      "Classification Model Iter 28/1000 - Loss: 42.71895980834961\n",
      "CL\n",
      "Classification Model Iter 29/1000 - Loss: 42.64949417114258\n",
      "CL\n",
      "Classification Model Iter 30/1000 - Loss: 42.782264709472656\n",
      "CL\n",
      "Classification Model Iter 31/1000 - Loss: 42.76994323730469\n",
      "CL\n",
      "Classification Model Iter 32/1000 - Loss: 42.549713134765625\n",
      "CL\n",
      "Classification Model Iter 33/1000 - Loss: 42.33722686767578\n",
      "CL\n",
      "Classification Model Iter 34/1000 - Loss: 42.23408508300781\n",
      "CL\n",
      "Classification Model Iter 35/1000 - Loss: 42.16276168823242\n",
      "CL\n",
      "Classification Model Iter 36/1000 - Loss: 42.10480499267578\n",
      "CL\n",
      "Classification Model Iter 37/1000 - Loss: 42.105987548828125\n",
      "CL\n",
      "Classification Model Iter 38/1000 - Loss: 42.09825134277344\n",
      "CL\n",
      "Classification Model Iter 39/1000 - Loss: 41.97390365600586\n",
      "CL\n",
      "Classification Model Iter 40/1000 - Loss: 41.809539794921875\n",
      "CL\n",
      "Classification Model Iter 41/1000 - Loss: 41.77703857421875\n",
      "CL\n",
      "Classification Model Iter 42/1000 - Loss: 41.85154342651367\n",
      "CL\n",
      "Classification Model Iter 43/1000 - Loss: 41.85185623168945\n",
      "CL\n",
      "Classification Model Iter 44/1000 - Loss: 41.74064636230469\n",
      "CL\n",
      "Classification Model Iter 45/1000 - Loss: 41.65373611450195\n",
      "CL\n",
      "Classification Model Iter 46/1000 - Loss: 41.654117584228516\n",
      "CL\n",
      "Classification Model Iter 47/1000 - Loss: 41.66972351074219\n",
      "CL\n",
      "Classification Model Iter 48/1000 - Loss: 41.64607620239258\n",
      "CL\n",
      "Classification Model Iter 49/1000 - Loss: 41.612030029296875\n",
      "CL\n",
      "Classification Model Iter 50/1000 - Loss: 41.58744430541992\n",
      "CL\n",
      "Classification Model Iter 51/1000 - Loss: 41.56202697753906\n",
      "CL\n",
      "Classification Model Iter 52/1000 - Loss: 41.54509353637695\n",
      "CL\n",
      "Classification Model Iter 53/1000 - Loss: 41.545345306396484\n",
      "CL\n",
      "Classification Model Iter 54/1000 - Loss: 41.541114807128906\n",
      "CL\n",
      "Classification Model Iter 55/1000 - Loss: 41.51957321166992\n",
      "CL\n",
      "Classification Model Iter 56/1000 - Loss: 41.498870849609375\n",
      "CL\n",
      "Classification Model Iter 57/1000 - Loss: 41.49187088012695\n",
      "CL\n",
      "Classification Model Iter 58/1000 - Loss: 41.48944091796875\n",
      "CL\n",
      "Classification Model Iter 59/1000 - Loss: 41.485294342041016\n",
      "CL\n",
      "Classification Model Iter 60/1000 - Loss: 41.47823715209961\n",
      "CL\n",
      "Classification Model Iter 61/1000 - Loss: 41.464717864990234\n",
      "CL\n",
      "Classification Model Iter 62/1000 - Loss: 41.45433044433594\n",
      "CL\n",
      "Classification Model Iter 63/1000 - Loss: 41.456207275390625\n",
      "CL\n",
      "Classification Model Iter 64/1000 - Loss: 41.458290100097656\n",
      "CL\n",
      "Classification Model Iter 65/1000 - Loss: 41.44847106933594\n",
      "CL\n",
      "Classification Model Iter 66/1000 - Loss: 41.43846893310547\n",
      "CL\n",
      "Classification Model Iter 67/1000 - Loss: 41.438846588134766\n",
      "CL\n",
      "Classification Model Iter 68/1000 - Loss: 41.439186096191406\n",
      "CL\n",
      "Classification Model Iter 69/1000 - Loss: 41.43464279174805\n",
      "CL\n",
      "Classification Model Iter 70/1000 - Loss: 41.43099594116211\n",
      "CL\n",
      "Classification Model Iter 71/1000 - Loss: 41.42984390258789\n",
      "CL\n",
      "Classification Model Iter 72/1000 - Loss: 41.42670440673828\n",
      "CL\n",
      "Classification Model Iter 73/1000 - Loss: 41.42446517944336\n",
      "CL\n",
      "Classification Model Iter 74/1000 - Loss: 41.42493438720703\n",
      "CL\n",
      "Classification Model Iter 75/1000 - Loss: 41.4232063293457\n",
      "CL\n",
      "Classification Model Iter 76/1000 - Loss: 41.420265197753906\n",
      "CL\n",
      "Classification Model Iter 77/1000 - Loss: 41.418983459472656\n",
      "CL\n",
      "Classification Model Iter 78/1000 - Loss: 41.419193267822266\n",
      "CL\n",
      "Classification Model Iter 79/1000 - Loss: 41.418296813964844\n",
      "CL\n",
      "Classification Model Iter 80/1000 - Loss: 41.41699981689453\n",
      "CL\n",
      "Classification Model Iter 81/1000 - Loss: 41.415679931640625\n",
      "CL\n",
      "Classification Model Iter 82/1000 - Loss: 41.41486740112305\n",
      "CL\n",
      "Classification Model Iter 83/1000 - Loss: 41.41487503051758\n",
      "CL\n",
      "Classification Model Iter 84/1000 - Loss: 41.41434860229492\n",
      "CL\n",
      "Classification Model Iter 85/1000 - Loss: 41.41306686401367\n",
      "CL\n",
      "Classification Model Iter 86/1000 - Loss: 41.41272735595703\n",
      "CL\n",
      "Classification Model Iter 87/1000 - Loss: 41.4127311706543\n",
      "CL\n",
      "Classification Model Iter 88/1000 - Loss: 41.411888122558594\n",
      "CL\n",
      "Classification Model Iter 89/1000 - Loss: 41.41172409057617\n",
      "CL\n",
      "Classification Model Iter 90/1000 - Loss: 41.411415100097656\n",
      "CL\n",
      "Classification Model Iter 91/1000 - Loss: 41.41095733642578\n",
      "CL\n",
      "Classification Model Iter 92/1000 - Loss: 41.41074752807617\n",
      "CL\n",
      "Classification Model Iter 93/1000 - Loss: 41.41019058227539\n",
      "CL\n",
      "Classification Model Iter 94/1000 - Loss: 41.410430908203125\n",
      "CL\n",
      "Classification Model Iter 95/1000 - Loss: 41.40953826904297\n",
      "CL\n",
      "Classification Model Iter 96/1000 - Loss: 41.40984344482422\n",
      "CL\n",
      "Classification Model Iter 97/1000 - Loss: 41.409706115722656\n",
      "CL\n",
      "Classification Model Iter 98/1000 - Loss: 41.409759521484375\n",
      "CL\n",
      "Classification Model Iter 99/1000 - Loss: 41.40946578979492\n",
      "CL\n",
      "Classification Model Iter 100/1000 - Loss: 41.409542083740234\n",
      "CL\n",
      "Classification Model Iter 101/1000 - Loss: 41.40894317626953\n",
      "CL\n",
      "Classification Model Iter 102/1000 - Loss: 41.40910339355469\n",
      "CL\n",
      "Classification Model Iter 103/1000 - Loss: 41.40897750854492\n",
      "CL\n",
      "Classification Model Iter 104/1000 - Loss: 41.40859603881836\n",
      "CL\n",
      "Classification Model Iter 105/1000 - Loss: 41.408607482910156\n",
      "CL\n",
      "Classification Model Iter 106/1000 - Loss: 41.408531188964844\n",
      "CL\n",
      "Classification Model Iter 107/1000 - Loss: 41.40850067138672\n",
      "CL\n",
      "Classification Model Iter 108/1000 - Loss: 41.40894317626953\n",
      "CL\n",
      "Classification Model Iter 109/1000 - Loss: 41.40858459472656\n",
      "CL\n",
      "Classification Model Iter 110/1000 - Loss: 41.408260345458984\n",
      "CL\n",
      "Classification Model Iter 111/1000 - Loss: 41.40851974487305\n",
      "CL\n",
      "Classification Model Iter 112/1000 - Loss: 41.40876388549805\n",
      "CL\n",
      "Classification Model Iter 113/1000 - Loss: 41.40864181518555\n",
      "CL\n",
      "Classification Model Iter 114/1000 - Loss: 41.408206939697266\n",
      "CL\n",
      "Classification Model Iter 115/1000 - Loss: 41.40843963623047\n",
      "CL\n",
      "Classification Model Iter 116/1000 - Loss: 41.40851974487305\n",
      "CL\n",
      "Classification Model Iter 117/1000 - Loss: 41.40841293334961\n",
      "CL\n",
      "Classification Model Iter 118/1000 - Loss: 41.40837097167969\n",
      "CL\n",
      "Classification Model Iter 119/1000 - Loss: 41.408267974853516\n",
      "CL\n",
      "Classification Model Iter 120/1000 - Loss: 41.408477783203125\n",
      "CL\n",
      "Classification Model Iter 121/1000 - Loss: 41.40821075439453\n",
      "CL\n",
      "Classification Model Iter 122/1000 - Loss: 41.40821838378906\n",
      "CL\n",
      "Classification Model Iter 123/1000 - Loss: 41.4080924987793\n",
      "CL\n",
      "Classification Model Iter 124/1000 - Loss: 41.40804672241211\n",
      "CL\n",
      "Classification Model Iter 125/1000 - Loss: 41.408443450927734\n",
      "CL\n",
      "Classification Model Iter 126/1000 - Loss: 41.40838623046875\n",
      "CL\n",
      "Classification Model Iter 127/1000 - Loss: 41.407989501953125\n",
      "CL\n",
      "Classification Model Iter 128/1000 - Loss: 41.40795135498047\n",
      "CL\n",
      "Classification Model Iter 129/1000 - Loss: 41.40815353393555\n",
      "CL\n",
      "Classification Model Iter 130/1000 - Loss: 41.40845489501953\n",
      "CL\n",
      "Classification Model Iter 131/1000 - Loss: 41.4083251953125\n",
      "CL\n",
      "Classification Model Iter 132/1000 - Loss: 41.40810012817383\n",
      "CL\n",
      "Classification Model Iter 133/1000 - Loss: 41.408180236816406\n",
      "CL\n",
      "Classification Model Iter 134/1000 - Loss: 41.408199310302734\n",
      "CL\n",
      "Classification Model Iter 135/1000 - Loss: 41.40785217285156\n",
      "CL\n",
      "Classification Model Iter 136/1000 - Loss: 41.4083366394043\n",
      "CL\n",
      "Classification Model Iter 137/1000 - Loss: 41.40827178955078\n",
      "CL\n",
      "Classification Model Iter 138/1000 - Loss: 41.40821075439453\n",
      "CL\n",
      "Classification Model Iter 139/1000 - Loss: 41.40812683105469\n",
      "CL\n",
      "Classification Model Iter 140/1000 - Loss: 41.40825653076172\n",
      "CL\n",
      "Classification Model Iter 141/1000 - Loss: 41.408382415771484\n",
      "CL\n",
      "Classification Model Iter 142/1000 - Loss: 41.40802001953125\n",
      "CL\n",
      "Classification Model Iter 143/1000 - Loss: 41.40855026245117\n",
      "CL\n",
      "Classification Model Iter 144/1000 - Loss: 41.40824890136719\n",
      "CL\n",
      "Classification Model Iter 145/1000 - Loss: 41.408042907714844\n",
      "CL\n",
      "Classification Model Iter 146/1000 - Loss: 41.40778350830078\n",
      "CL\n",
      "Classification Model Iter 147/1000 - Loss: 41.408199310302734\n",
      "CL\n",
      "Classification Model Iter 148/1000 - Loss: 41.40825653076172\n",
      "CL\n",
      "Classification Model Iter 149/1000 - Loss: 41.408172607421875\n",
      "CL\n",
      "Classification Model Iter 150/1000 - Loss: 41.408180236816406\n",
      "CL\n",
      "Classification Model Iter 151/1000 - Loss: 41.40806198120117\n",
      "CL\n",
      "Classification Model Iter 152/1000 - Loss: 41.40824890136719\n",
      "CL\n",
      "Classification Model Iter 153/1000 - Loss: 41.40772247314453\n",
      "CL\n",
      "Classification Model Iter 154/1000 - Loss: 41.40773391723633\n",
      "CL\n",
      "Classification Model Iter 155/1000 - Loss: 41.40822982788086\n",
      "CL\n",
      "Classification Model Iter 156/1000 - Loss: 41.408424377441406\n",
      "CL\n",
      "Classification Model Iter 157/1000 - Loss: 41.408206939697266\n",
      "CL\n",
      "Classification Model Iter 158/1000 - Loss: 41.40835952758789\n",
      "CL\n",
      "Classification Model Iter 159/1000 - Loss: 41.40827178955078\n",
      "CL\n",
      "Classification Model Iter 160/1000 - Loss: 41.40818786621094\n",
      "CL\n",
      "Classification Model Iter 161/1000 - Loss: 41.40870666503906\n",
      "CL\n",
      "Classification Model Iter 162/1000 - Loss: 41.40829086303711\n",
      "CL\n",
      "Classification Model Iter 163/1000 - Loss: 41.4085578918457\n",
      "CL\n",
      "Classification Model Iter 164/1000 - Loss: 41.408477783203125\n",
      "CL\n",
      "Classification Model Iter 165/1000 - Loss: 41.40812683105469\n",
      "CL\n",
      "Classification Model Iter 166/1000 - Loss: 41.40796661376953\n",
      "CL\n",
      "Classification Model Iter 167/1000 - Loss: 41.40812683105469\n",
      "CL\n",
      "Classification Model Iter 168/1000 - Loss: 41.4081916809082\n",
      "CL\n",
      "Classification Model Iter 169/1000 - Loss: 41.407798767089844\n",
      "CL\n",
      "Classification Model Iter 170/1000 - Loss: 41.40837860107422\n",
      "CL\n",
      "Classification Model Iter 171/1000 - Loss: 41.408206939697266\n",
      "CL\n",
      "Classification Model Iter 172/1000 - Loss: 41.407955169677734\n",
      "CL\n",
      "Classification Model Iter 173/1000 - Loss: 41.408138275146484\n",
      "CL\n",
      "Classification Model Iter 174/1000 - Loss: 41.407901763916016\n",
      "CL\n",
      "Classification Model Iter 175/1000 - Loss: 41.408226013183594\n",
      "CL\n",
      "Classification Model Iter 176/1000 - Loss: 41.407875061035156\n",
      "CL\n",
      "Classification Model Iter 177/1000 - Loss: 41.4082145690918\n",
      "CL\n",
      "Classification Model Iter 178/1000 - Loss: 41.408172607421875\n",
      "CL\n",
      "Classification Model Iter 179/1000 - Loss: 41.407894134521484\n",
      "CL\n",
      "Classification Model Iter 180/1000 - Loss: 41.4083251953125\n",
      "CL\n",
      "Classification Model Iter 181/1000 - Loss: 41.408023834228516\n",
      "CL\n",
      "Classification Model Iter 182/1000 - Loss: 41.40837097167969\n",
      "CL\n",
      "Classification Model Iter 183/1000 - Loss: 41.40850830078125\n",
      "CL\n",
      "Classification Model Iter 184/1000 - Loss: 41.40822982788086\n",
      "CL\n",
      "Classification Model Iter 185/1000 - Loss: 41.4085693359375\n",
      "CL\n",
      "Classification Model Iter 186/1000 - Loss: 41.40803527832031\n",
      "CL\n",
      "Classification Model Iter 187/1000 - Loss: 41.407894134521484\n",
      "CL\n",
      "Classification Model Iter 188/1000 - Loss: 41.408226013183594\n",
      "CL\n",
      "Classification Model Iter 189/1000 - Loss: 41.40849685668945\n",
      "CL\n",
      "Classification Model Iter 190/1000 - Loss: 41.40824890136719\n",
      "CL\n",
      "Classification Model Iter 191/1000 - Loss: 41.40815353393555\n",
      "CL\n",
      "Classification Model Iter 192/1000 - Loss: 41.4083366394043\n",
      "CL\n",
      "Classification Model Iter 193/1000 - Loss: 41.40825653076172\n",
      "CL\n",
      "Classification Model Iter 194/1000 - Loss: 41.4083251953125\n",
      "CL\n",
      "Classification Model Iter 195/1000 - Loss: 41.40803146362305\n",
      "CL\n",
      "Classification Model Iter 196/1000 - Loss: 41.408241271972656\n",
      "CL\n",
      "Classification Model Iter 197/1000 - Loss: 41.40806198120117\n",
      "CL\n",
      "Classification Model Iter 198/1000 - Loss: 41.408226013183594\n",
      "CL\n",
      "Classification Model Iter 199/1000 - Loss: 41.40793228149414\n",
      "CL\n",
      "Classification Model Iter 200/1000 - Loss: 41.40842819213867\n",
      "CL\n",
      "Classification Model Iter 201/1000 - Loss: 41.40806579589844\n",
      "CL\n",
      "Classification Model Iter 202/1000 - Loss: 41.408382415771484\n",
      "CL\n",
      "Classification Model Iter 203/1000 - Loss: 41.40846252441406\n",
      "CL\n",
      "Classification Model Iter 204/1000 - Loss: 41.408504486083984\n",
      "CL\n",
      "Classification Model Iter 205/1000 - Loss: 41.40817642211914\n",
      "CL\n",
      "Classification Model Iter 206/1000 - Loss: 41.407981872558594\n",
      "CL\n",
      "Classification Model Iter 207/1000 - Loss: 41.408321380615234\n",
      "CL\n",
      "Classification Model Iter 208/1000 - Loss: 41.40841293334961\n",
      "CL\n",
      "Classification Model Iter 209/1000 - Loss: 41.40843200683594\n",
      "CL\n",
      "Classification Model Iter 210/1000 - Loss: 41.40827941894531\n",
      "CL\n",
      "Classification Model Iter 211/1000 - Loss: 41.408241271972656\n",
      "CL\n",
      "Classification Model Iter 212/1000 - Loss: 41.408172607421875\n",
      "CL\n",
      "Classification Model Iter 213/1000 - Loss: 41.408164978027344\n",
      "CL\n",
      "Classification Model Iter 214/1000 - Loss: 41.4081916809082\n",
      "CL\n",
      "Classification Model Iter 215/1000 - Loss: 41.4083137512207\n",
      "CL\n",
      "Classification Model Iter 216/1000 - Loss: 41.40825271606445\n",
      "CL\n",
      "Classification Model Iter 217/1000 - Loss: 41.4083137512207\n",
      "CL\n",
      "Classification Model Iter 218/1000 - Loss: 41.408416748046875\n",
      "CL\n",
      "Classification Model Iter 219/1000 - Loss: 41.40813446044922\n",
      "CL\n",
      "Classification Model Iter 220/1000 - Loss: 41.40837097167969\n",
      "CL\n",
      "Classification Model Iter 221/1000 - Loss: 41.408172607421875\n",
      "CL\n",
      "Classification Model Iter 222/1000 - Loss: 41.40833282470703\n",
      "CL\n",
      "Classification Model Iter 223/1000 - Loss: 41.408329010009766\n",
      "CL\n",
      "Classification Model Iter 224/1000 - Loss: 41.40849304199219\n",
      "CL\n",
      "Classification Model Iter 225/1000 - Loss: 41.40837860107422\n",
      "CL\n",
      "Classification Model Iter 226/1000 - Loss: 41.40826416015625\n",
      "CL\n",
      "Classification Model Iter 227/1000 - Loss: 41.408485412597656\n",
      "CL\n",
      "Classification Model Iter 228/1000 - Loss: 41.40861511230469\n",
      "CL\n",
      "Classification Model Iter 229/1000 - Loss: 41.408531188964844\n",
      "CL\n",
      "Classification Model Iter 230/1000 - Loss: 41.40839767456055\n",
      "CL\n",
      "Classification Model Iter 231/1000 - Loss: 41.40835952758789\n",
      "CL\n",
      "Classification Model Iter 232/1000 - Loss: 41.408531188964844\n",
      "CL\n",
      "Classification Model Iter 233/1000 - Loss: 41.408321380615234\n",
      "CL\n",
      "Classification Model Iter 234/1000 - Loss: 41.40839767456055\n",
      "CL\n",
      "Classification Model Iter 235/1000 - Loss: 41.408634185791016\n",
      "CL\n",
      "Classification Model Iter 236/1000 - Loss: 41.40827560424805\n",
      "CL\n",
      "Classification Model Iter 237/1000 - Loss: 41.408077239990234\n",
      "CL\n",
      "Classification Model Iter 238/1000 - Loss: 41.408203125\n",
      "CL\n",
      "Classification Model Iter 239/1000 - Loss: 41.408294677734375\n",
      "CL\n",
      "Classification Model Iter 240/1000 - Loss: 41.408626556396484\n",
      "CL\n",
      "Classification Model Iter 241/1000 - Loss: 41.408451080322266\n",
      "CL\n",
      "Classification Model Iter 242/1000 - Loss: 41.408355712890625\n",
      "CL\n",
      "Classification Model Iter 243/1000 - Loss: 41.40850067138672\n",
      "CL\n",
      "Classification Model Iter 244/1000 - Loss: 41.40838623046875\n",
      "CL\n",
      "Classification Model Iter 245/1000 - Loss: 41.408164978027344\n",
      "CL\n",
      "Classification Model Iter 246/1000 - Loss: 41.40849685668945\n",
      "CL\n",
      "Classification Model Iter 247/1000 - Loss: 41.40850830078125\n",
      "CL\n",
      "Classification Model Iter 248/1000 - Loss: 41.4083251953125\n",
      "CL\n",
      "Classification Model Iter 249/1000 - Loss: 41.408538818359375\n",
      "CL\n",
      "Classification Model Iter 250/1000 - Loss: 41.408607482910156\n",
      "CL\n",
      "Classification Model Iter 251/1000 - Loss: 41.408451080322266\n",
      "CL\n",
      "Classification Model Iter 252/1000 - Loss: 41.4083137512207\n",
      "CL\n",
      "Classification Model Iter 253/1000 - Loss: 41.40863800048828\n",
      "CL\n",
      "Classification Model Iter 254/1000 - Loss: 41.40851974487305\n",
      "CL\n",
      "Classification Model Iter 255/1000 - Loss: 41.40873336791992\n",
      "CL\n",
      "Classification Model Iter 256/1000 - Loss: 41.408809661865234\n",
      "CL\n",
      "Classification Model Iter 257/1000 - Loss: 41.40843963623047\n",
      "CL\n",
      "Classification Model Iter 258/1000 - Loss: 41.40838623046875\n",
      "CL\n",
      "Classification Model Iter 259/1000 - Loss: 41.40863037109375\n",
      "CL\n",
      "Classification Model Iter 260/1000 - Loss: 41.4088134765625\n",
      "CL\n",
      "Classification Model Iter 261/1000 - Loss: 41.40862274169922\n",
      "CL\n",
      "Classification Model Iter 262/1000 - Loss: 41.4089469909668\n",
      "CL\n",
      "Classification Model Iter 263/1000 - Loss: 41.40848922729492\n",
      "CL\n",
      "Classification Model Iter 264/1000 - Loss: 41.408382415771484\n",
      "CL\n",
      "Classification Model Iter 265/1000 - Loss: 41.40890884399414\n",
      "CL\n",
      "Classification Model Iter 266/1000 - Loss: 41.40872573852539\n",
      "CL\n",
      "Classification Model Iter 267/1000 - Loss: 41.40898513793945\n",
      "CL\n",
      "Classification Model Iter 268/1000 - Loss: 41.4089469909668\n",
      "CL\n",
      "Classification Model Iter 269/1000 - Loss: 41.409156799316406\n",
      "CL\n",
      "Classification Model Iter 270/1000 - Loss: 41.40931701660156\n",
      "CL\n",
      "Classification Model Iter 271/1000 - Loss: 41.40959930419922\n",
      "CL\n",
      "Classification Model Iter 272/1000 - Loss: 41.409423828125\n",
      "CL\n",
      "Classification Model Iter 273/1000 - Loss: 41.409488677978516\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "training_iter = 1000  # Number of training iterations\n",
    "lr = 1  # Learning rate\n",
    "\n",
    "model_1.train()\n",
    "likelihood_1.train()\n",
    "\n",
    "optimizer_1 = torch.optim.Adam(model_1.parameters(), lr=lr)\n",
    "mll_1 = gpytorch.mlls.VariationalELBO(likelihood_1, model_1, num_data=X_train_1.size(0))\n",
    "\n",
    "# Implement Gradient Clipping\n",
    "for i in range(training_iter):\n",
    "    optimizer_1.zero_grad()\n",
    "    output_1 = model_1(X_train_1)\n",
    "    \n",
    "    # Check for NaNs in the output\n",
    "    if torch.isnan(output_1.mean).any():\n",
    "        print(f\"Warning: NaN detected in output at iteration {i + 1}\")\n",
    "        break\n",
    "\n",
    "    prediction = model_1(X_train_1)\n",
    "    predicted_probabilities = torch.softmax(prediction.mean, dim=0)\n",
    "    predicted_labels = predicted_probabilities.argmax(dim=0).detach()\n",
    "    \n",
    "    sim_score = torch.ones(1, requires_grad=True)*custom_loss(predicted_labels, y_train_1)\n",
    "    loss_1 = -mll_1(output_1, y_train_1) + sim_score\n",
    "    loss_1.backward()\n",
    "    \n",
    "    # Clip gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model_1.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer_1.step()\n",
    "    print(f'Classification Model Iter {i + 1}/{training_iter} - Loss: {loss_1.item()}')\n",
    "\n",
    "# Make predictions after training\n",
    "model_1.eval()\n",
    "likelihood_1.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    predictions = likelihood_1(model_1(X_test_1)).mean\n",
    "\n",
    "# Check for NaN predictions\n",
    "if torch.isnan(predictions).any():\n",
    "    print(\"NaN detected in predictions!\")\n",
    "else:\n",
    "    print(\"Predictions are valid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bedf33-d4cd-4ed1-bf78-55cfc3cff5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set models and likelihoods to evaluation mode\n",
    "model_1.eval()\n",
    "likelihood_1.eval()\n",
    "\n",
    "# Make predictions (use a small batch from X_test_1 for diagnosis)\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    small_X_test_1 = X_test_1[:5]  # Take the first 5 samples for a small test\n",
    "    prediction = model_1(small_X_test_1)\n",
    "    \n",
    "    # Inspect the raw prediction mean before applying softmax\n",
    "    print(f\"Raw prediction mean (pre-softmax): {prediction.mean}\")\n",
    "\n",
    "    # Apply softmax and check the outputs\n",
    "    predicted_probabilities = torch.softmax(prediction.mean, dim=0)\n",
    "    print(f\"Predicted probabilities: {predicted_probabilities}\")\n",
    "\n",
    "    predicted_labels = predicted_probabilities.argmax(dim=0).detach().numpy()\n",
    "    print(f\"Predicted labels: {predicted_labels}\")\n",
    "\n",
    "# If everything seems fine in the small batch, proceed with the full prediction\n",
    "if not torch.isnan(prediction.mean).any():\n",
    "    print(\"Small batch predictions are valid, proceeding with full test set.\")\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        full_prediction = model_1(X_test_1)\n",
    "        full_predicted_probabilities = torch.softmax(full_prediction.mean, dim=0)\n",
    "        if torch.isnan(full_predicted_probabilities).any():\n",
    "            print(\"NaN detected in full predictions!\")\n",
    "        else:\n",
    "            full_predicted_labels = full_predicted_probabilities.argmax(dim=0).detach().numpy()\n",
    "            print(f\"Full predicted labels: {full_predicted_labels}\")\n",
    "else:\n",
    "    print(\"NaN detected in small batch predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d24953-53de-4d23-bc71-b4d4567a2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = label_encoder.inverse_transform(full_predicted_labels)\n",
    "trues = label_encoder.inverse_transform(y_test_1)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "axs[0].hist(preds, edgecolor='white', bins=20)\n",
    "axs[0].set_title('prediction')\n",
    "axs[1].hist(trues, edgecolor='white', bins=20)\n",
    "axs[1].set_title('true')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9572ab2-b989-44a2-b38a-ce10837c2663",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd695f-7b66-46d9-a6d9-2c605f8cdca9",
   "metadata": {},
   "source": [
    "Sources:\n",
    "1. https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/ModelList_GP_Regression.html\n",
    "2. https://jamesbrind.uk/posts/2d-gaussian-process-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c0ff8-2357-4fc0-ace3-79768843c4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
