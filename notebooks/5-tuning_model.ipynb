{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b011d16-5e73-46f5-9e0d-3443ec2ff42b",
   "metadata": {},
   "source": [
    "# Pytorch Multi-output GPR\n",
    "\n",
    "The purpose of this notebook is to improve the predictions of Layer 1 soil type and soil thickness of Seattle from the preliminary work demostrated in notebook `4-pytorch_gpr.ipynb`.\n",
    "\n",
    "The tactics used to improve the model include: \n",
    "1. Performing a log scale transformation of the layer thickness target (instead of a standard scaler).\n",
    "2. Evaluating class imbalance of the classifier. \n",
    "3. Performing hyperparameter tuning, looking at different kernels and learning rates.\n",
    "4. Performing cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb804bf-e961-4804-a088-3f113bb7381f",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69f4200-2066-4eab-96a2-c7eba0136384",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch --quiet\n",
    "!pip install gpytorch --quiet\n",
    "!pip install imbalanced-learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e264b31-4228-4088-9235-9c5cc3f99f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.models import ApproximateGP, ExactGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "from gpytorch.likelihoods import SoftmaxLikelihood, GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13712b6b-e473-4e08-861a-1a4885d4eeff",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa664a05-0bd2-453a-9e5d-9068ce2cd200",
   "metadata": {},
   "source": [
    "#### Load training data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f4ce29-8f04-48e9-9944-01b8f6e4d434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOREHOLE_ID</th>\n",
       "      <th>BOREHOLE_NAME</th>\n",
       "      <th>BOREHOLE_TYPE</th>\n",
       "      <th>BOREHOLE_DEPTH_FT</th>\n",
       "      <th>ELEVATION_FT</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LAYER_NUMBER</th>\n",
       "      <th>TOP_DEPTH_FT</th>\n",
       "      <th>BOTTOM_DEPTH_FT</th>\n",
       "      <th>USCS</th>\n",
       "      <th>SIMPLE_USCS</th>\n",
       "      <th>LAYER_THICKNESS_FT</th>\n",
       "      <th>geometry</th>\n",
       "      <th>MAPPED_UNIT</th>\n",
       "      <th>SLOPE</th>\n",
       "      <th>ROUGHNESS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>EB-1</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>41.0</td>\n",
       "      <td>131.30</td>\n",
       "      <td>47.575005</td>\n",
       "      <td>-122.406037</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>OH</td>\n",
       "      <td>O</td>\n",
       "      <td>2.5</td>\n",
       "      <td>POINT (-122.40604 47.575)</td>\n",
       "      <td>Ql</td>\n",
       "      <td>8.126739</td>\n",
       "      <td>36.531616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>EB-2</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>40.0</td>\n",
       "      <td>130.90</td>\n",
       "      <td>47.574982</td>\n",
       "      <td>-122.406239</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>SC</td>\n",
       "      <td>S</td>\n",
       "      <td>5.0</td>\n",
       "      <td>POINT (-122.40624 47.57498)</td>\n",
       "      <td>Ql</td>\n",
       "      <td>8.056191</td>\n",
       "      <td>38.262520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>HB-1</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>10.5</td>\n",
       "      <td>46.30</td>\n",
       "      <td>47.512358</td>\n",
       "      <td>-122.394125</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>ML</td>\n",
       "      <td>M</td>\n",
       "      <td>7.5</td>\n",
       "      <td>POINT (-122.39412 47.51236)</td>\n",
       "      <td>Qss</td>\n",
       "      <td>12.098481</td>\n",
       "      <td>45.003693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>HB-2</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>6.9</td>\n",
       "      <td>47.70</td>\n",
       "      <td>47.512384</td>\n",
       "      <td>-122.394069</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>ML</td>\n",
       "      <td>M</td>\n",
       "      <td>5.0</td>\n",
       "      <td>POINT (-122.39407 47.51238)</td>\n",
       "      <td>Qss</td>\n",
       "      <td>12.098481</td>\n",
       "      <td>45.003693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>HB-3</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>13.5</td>\n",
       "      <td>40.90</td>\n",
       "      <td>47.512208</td>\n",
       "      <td>-122.394083</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>ML</td>\n",
       "      <td>M</td>\n",
       "      <td>8.6</td>\n",
       "      <td>POINT (-122.39408 47.51221)</td>\n",
       "      <td>Qss</td>\n",
       "      <td>12.098481</td>\n",
       "      <td>45.003693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10501</th>\n",
       "      <td>153201</td>\n",
       "      <td>B-1</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>44.0</td>\n",
       "      <td>26.35</td>\n",
       "      <td>47.437003</td>\n",
       "      <td>-122.242410</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>POINT (-122.24241 47.437)</td>\n",
       "      <td>Qaw</td>\n",
       "      <td>0.373783</td>\n",
       "      <td>1.591432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10502</th>\n",
       "      <td>153202</td>\n",
       "      <td>B-2</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.45</td>\n",
       "      <td>47.437208</td>\n",
       "      <td>-122.242890</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ML</td>\n",
       "      <td>M</td>\n",
       "      <td>2.0</td>\n",
       "      <td>POINT (-122.24289 47.43721)</td>\n",
       "      <td>Qaw</td>\n",
       "      <td>0.272720</td>\n",
       "      <td>1.453522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10503</th>\n",
       "      <td>153203</td>\n",
       "      <td>B-3</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>44.0</td>\n",
       "      <td>24.46</td>\n",
       "      <td>47.437427</td>\n",
       "      <td>-122.243398</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>POINT (-122.2434 47.43743)</td>\n",
       "      <td>Qaw</td>\n",
       "      <td>0.158798</td>\n",
       "      <td>1.009704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10504</th>\n",
       "      <td>153204</td>\n",
       "      <td>B-4</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>69.0</td>\n",
       "      <td>26.78</td>\n",
       "      <td>47.436984</td>\n",
       "      <td>-122.243486</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>POINT (-122.24349 47.43698)</td>\n",
       "      <td>Qaw</td>\n",
       "      <td>0.158798</td>\n",
       "      <td>1.009704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10505</th>\n",
       "      <td>153205</td>\n",
       "      <td>B-5</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>54.0</td>\n",
       "      <td>23.75</td>\n",
       "      <td>47.437412</td>\n",
       "      <td>-122.242456</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>POINT (-122.24246 47.43741)</td>\n",
       "      <td>Qaw</td>\n",
       "      <td>0.373783</td>\n",
       "      <td>1.591432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10506 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       BOREHOLE_ID BOREHOLE_NAME BOREHOLE_TYPE  BOREHOLE_DEPTH_FT  \\\n",
       "0                7          EB-1  Geotechnical               41.0   \n",
       "1                8          EB-2  Geotechnical               40.0   \n",
       "2               15          HB-1  Geotechnical               10.5   \n",
       "3               16          HB-2  Geotechnical                6.9   \n",
       "4               17          HB-3  Geotechnical               13.5   \n",
       "...            ...           ...           ...                ...   \n",
       "10501       153201           B-1  Geotechnical               44.0   \n",
       "10502       153202           B-2  Geotechnical               39.0   \n",
       "10503       153203           B-3  Geotechnical               44.0   \n",
       "10504       153204           B-4  Geotechnical               69.0   \n",
       "10505       153205           B-5  Geotechnical               54.0   \n",
       "\n",
       "       ELEVATION_FT   LATITUDE   LONGITUDE  LAYER_NUMBER  TOP_DEPTH_FT  \\\n",
       "0            131.30  47.575005 -122.406037             1           0.0   \n",
       "1            130.90  47.574982 -122.406239             1           0.0   \n",
       "2             46.30  47.512358 -122.394125             1           0.0   \n",
       "3             47.70  47.512384 -122.394069             1           0.0   \n",
       "4             40.90  47.512208 -122.394083             1           0.0   \n",
       "...             ...        ...         ...           ...           ...   \n",
       "10501         26.35  47.437003 -122.242410             1           0.0   \n",
       "10502         24.45  47.437208 -122.242890             1           0.0   \n",
       "10503         24.46  47.437427 -122.243398             1           0.0   \n",
       "10504         26.78  47.436984 -122.243486             1           0.0   \n",
       "10505         23.75  47.437412 -122.242456             1           0.0   \n",
       "\n",
       "       BOTTOM_DEPTH_FT USCS SIMPLE_USCS  LAYER_THICKNESS_FT  \\\n",
       "0                  2.5   OH           O                 2.5   \n",
       "1                  5.0   SC           S                 5.0   \n",
       "2                  7.5   ML           M                 7.5   \n",
       "3                  5.0   ML           M                 5.0   \n",
       "4                  8.6   ML           M                 8.6   \n",
       "...                ...  ...         ...                 ...   \n",
       "10501              0.5  NaN         NaN                 0.5   \n",
       "10502              2.0   ML           M                 2.0   \n",
       "10503              0.5  NaN         NaN                 0.5   \n",
       "10504              0.5  NaN         NaN                 0.5   \n",
       "10505              0.5  NaN         NaN                 0.5   \n",
       "\n",
       "                          geometry MAPPED_UNIT      SLOPE  ROUGHNESS  \n",
       "0        POINT (-122.40604 47.575)          Ql   8.126739  36.531616  \n",
       "1      POINT (-122.40624 47.57498)          Ql   8.056191  38.262520  \n",
       "2      POINT (-122.39412 47.51236)         Qss  12.098481  45.003693  \n",
       "3      POINT (-122.39407 47.51238)         Qss  12.098481  45.003693  \n",
       "4      POINT (-122.39408 47.51221)         Qss  12.098481  45.003693  \n",
       "...                            ...         ...        ...        ...  \n",
       "10501    POINT (-122.24241 47.437)         Qaw   0.373783   1.591432  \n",
       "10502  POINT (-122.24289 47.43721)         Qaw   0.272720   1.453522  \n",
       "10503   POINT (-122.2434 47.43743)         Qaw   0.158798   1.009704  \n",
       "10504  POINT (-122.24349 47.43698)         Qaw   0.158798   1.009704  \n",
       "10505  POINT (-122.24246 47.43741)         Qaw   0.373783   1.591432  \n",
       "\n",
       "[10506 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training data CSV file compiled in `1-data_access.ipynb`\n",
    "input_file = '../data/2.1-seattle_layer1_trainingdata.csv'\n",
    "training_data = pd.read_csv(input_file)\n",
    "\n",
    "# Create a GeoDataFrame from the DataFrame\n",
    "training_data = gpd.GeoDataFrame(training_data, geometry=gpd.points_from_xy(training_data.LONGITUDE, training_data.LATITUDE))\n",
    "    \n",
    "# Set the CRS to WGS84 (latitude and longitude)\n",
    "training_data.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb9989-deeb-486b-8e0f-fe3f457fd7f6",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b471b6f-3014-4f81-9d7b-379e2d8e682c",
   "metadata": {},
   "source": [
    "#### Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c46a3a7a-5e3a-4e14-b974-91b4a6720104",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_data[['MAPPED_UNIT', 'SLOPE', 'ROUGHNESS']]\n",
    "y_1 = training_data['SIMPLE_USCS']\n",
    "y_2 = training_data['LAYER_THICKNESS_FT']\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(sparse_output=True), ['MAPPED_UNIT']),  # One-hot encode categorical features\n",
    "        ('num', StandardScaler(), ['SLOPE', 'ROUGHNESS'])  # Standardize numerical features\n",
    "    ])\n",
    "\n",
    "# Drop rows with NaN values in X and corresponding y_1 and y_2\n",
    "X = X.dropna()\n",
    "y_1 = y_1.loc[X.index].dropna()  # Ensure y_1 is aligned with X and has no NaN values\n",
    "y_2 = y_2.loc[X.index].dropna()  # Ensure y_2 is aligned with X and has no NaN values\n",
    "\n",
    "# Handle cases where dropping NaN results in mismatched lengths\n",
    "X = X.loc[y_1.index]  # Align X with the remaining non-NaN y_1\n",
    "y_2 = y_2.loc[y_1.index]  # Align y_2 with the remaining non-NaN y_1\n",
    "\n",
    "# Apply transformations to X\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "X_tensor = torch.tensor(X_processed.todense(), dtype=torch.float32) if scipy.sparse.issparse(X_processed) else torch.tensor(X_processed, dtype=torch.float32) \n",
    "\n",
    "# Handling target for SIMPLE_USCS\n",
    "label_encoder = LabelEncoder()\n",
    "y_1_encoded = label_encoder.fit_transform(y_1)\n",
    "y_1_tensor = torch.tensor(y_1_encoded, dtype=torch.long)\n",
    "\n",
    "# Handling target for LAYER_THICKNESS_FT\n",
    "# scaler = StandardScaler()\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "y_2_scaled = log_transformer.fit_transform(y_2.values.reshape(-1, 1))\n",
    "y_2_tensor = torch.tensor(y_2_scaled, dtype=torch.float32).squeeze()\n",
    "\n",
    "# Split the data into training and test sets for both models\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_tensor, y_1_tensor, test_size=0.2, random_state=42, stratify=y_1_tensor)\n",
    "# Initialize the SMOTE sampler\n",
    "smote_sampler = SMOTE(random_state=42)\n",
    "\n",
    "# Resample the data\n",
    "X_train_resampled, y_train_resampled = smote_sampler.fit_resample(X_train_1, y_train_1)\n",
    "smote_sampler = SMOTE(random_state=42) # Initialize the SMOTE sampler\n",
    "X_train_resampled, y_train_resampled = smote_sampler.fit_resample(X_train_1, y_train_1)\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_tensor, y_2_tensor, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90866c21-8195-430d-8e42-61261992ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPClassificationModel(ApproximateGP):\n",
    "    def __init__(self, train_x, num_classes):\n",
    "        variational_distribution = CholeskyVariationalDistribution(train_x.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, train_x, variational_distribution, learn_inducing_locations=True)\n",
    "        super(GPClassificationModel, self).__init__(variational_strategy)\n",
    "        \n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x).expand([self.num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aae8782-58f7-402a-a911-b0ef4dc501c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b5e49e4-f88c-492c-877b-1e2845aea0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_score_gp(model_class, X, y, param_grid, cv=5):\n",
    "    kf = KFold(n_splits=cv)\n",
    "    best_score = float('inf')\n",
    "    best_params = None\n",
    "\n",
    "    for lr in param_grid['lr']:\n",
    "        for lengthscale in param_grid['lengthscale']:\n",
    "            for outputscale in param_grid['outputscale']:\n",
    "                fold_scores = []\n",
    "                \n",
    "                for train_index, val_index in kf.split(X):\n",
    "                    # print('running:',train_index, val_index)\n",
    "                    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "                    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "                    # Initialize model with specific hyperparameters\n",
    "                    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "                    model = model_class(X_train_fold, y_train_fold, likelihood)\n",
    "                    \n",
    "                    # Set hyperparameters\n",
    "                    model.covar_module.base_kernel.lengthscale = lengthscale\n",
    "                    model.covar_module.outputscale = outputscale\n",
    "                    \n",
    "                    # Training\n",
    "                    model.train()\n",
    "                    likelihood.train()\n",
    "\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "                    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "                    \n",
    "                    training_iter = 50\n",
    "                    for i in range(training_iter):\n",
    "                        optimizer.zero_grad()\n",
    "                        output = model(X_train_fold)\n",
    "                        loss = -mll(output, y_train_fold)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    likelihood.eval()\n",
    "                    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "                        val_output = model(X_val_fold)\n",
    "                        val_loss = -mll(val_output, y_val_fold)\n",
    "                        fold_scores.append(val_loss.item())\n",
    "\n",
    "                avg_score = np.mean(fold_scores)\n",
    "                \n",
    "                if avg_score < best_score:\n",
    "                    best_score = avg_score\n",
    "                    best_params = {'lr': lr, 'lengthscale': lengthscale, 'outputscale': outputscale}\n",
    "\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d72fdb86-f9be-452d-a59e-b30abc67a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lr': [0.01, 0.05, 0.1],\n",
    "    'lengthscale': [0.1, 0.5, 1.0, 2.0],\n",
    "    'outputscale': [0.5, 1.0, 2.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09725995-8ee9-4094-a996-483cfa12c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the likelihood and model for multiclass classification\n",
    "num_classes = len(label_encoder.classes_)\n",
    "likelihood_1 = gpytorch.likelihoods.SoftmaxLikelihood(num_classes=num_classes, mixing_weights=None)\n",
    "model_1 = GPClassificationModel(X_train_1, num_classes=num_classes)\n",
    "\n",
    "# Initialize the likelihood and model for regression\n",
    "likelihood_2 = GaussianLikelihood()\n",
    "model_2 = GPRegressionModel(X_train_2, y_train_2, likelihood_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "186fdab4-64a0-4155-8381-77c30651925f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): SoftmaxLikelihood()\n",
       "  (1): GaussianLikelihood(\n",
       "    (noise_covar): HomoskedasticNoise(\n",
       "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a ModuleList to store the models\n",
    "models = torch.nn.ModuleList([model_1, model_2])\n",
    "likelihoods = torch.nn.ModuleList([likelihood_1, likelihood_2])\n",
    "\n",
    "# Set into eval mode for all models and likelihoods\n",
    "models.eval()\n",
    "likelihoods.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d31ec59-1b17-4549-b339-1790ae7c1f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Model Iter 1/100 - Loss: 2.0620131492614746\n",
      "Classification Model Iter 2/100 - Loss: 3.249990463256836\n",
      "Classification Model Iter 3/100 - Loss: 5.505388259887695\n",
      "Classification Model Iter 4/100 - Loss: 4.000619888305664\n",
      "Classification Model Iter 5/100 - Loss: 4.725461959838867\n",
      "Classification Model Iter 6/100 - Loss: 5.565893650054932\n",
      "Classification Model Iter 7/100 - Loss: 4.291552543640137\n",
      "Classification Model Iter 8/100 - Loss: 3.26423978805542\n",
      "Classification Model Iter 9/100 - Loss: 3.525963306427002\n",
      "Classification Model Iter 10/100 - Loss: 3.844325304031372\n",
      "Classification Model Iter 11/100 - Loss: 3.4898078441619873\n",
      "Classification Model Iter 12/100 - Loss: 2.9164514541625977\n",
      "Classification Model Iter 13/100 - Loss: 2.699673652648926\n",
      "Classification Model Iter 14/100 - Loss: 2.8440022468566895\n",
      "Classification Model Iter 15/100 - Loss: 2.878390312194824\n",
      "Classification Model Iter 16/100 - Loss: 2.6781234741210938\n",
      "Classification Model Iter 17/100 - Loss: 2.467125415802002\n",
      "Classification Model Iter 18/100 - Loss: 2.402491331100464\n",
      "Classification Model Iter 19/100 - Loss: 2.4167661666870117\n",
      "Classification Model Iter 20/100 - Loss: 2.3887434005737305\n",
      "Classification Model Iter 21/100 - Loss: 2.3010334968566895\n",
      "Classification Model Iter 22/100 - Loss: 2.211385488510132\n",
      "Classification Model Iter 23/100 - Loss: 2.1732563972473145\n",
      "Classification Model Iter 24/100 - Loss: 2.166975259780884\n",
      "Classification Model Iter 25/100 - Loss: 2.148850202560425\n",
      "Classification Model Iter 26/100 - Loss: 2.1009902954101562\n",
      "Classification Model Iter 27/100 - Loss: 2.056037425994873\n",
      "Classification Model Iter 28/100 - Loss: 2.0336453914642334\n",
      "Classification Model Iter 29/100 - Loss: 2.0260133743286133\n",
      "Classification Model Iter 30/100 - Loss: 2.0049045085906982\n",
      "Classification Model Iter 31/100 - Loss: 1.978979468345642\n",
      "Classification Model Iter 32/100 - Loss: 1.9582326412200928\n",
      "Classification Model Iter 33/100 - Loss: 1.9472744464874268\n",
      "Classification Model Iter 34/100 - Loss: 1.935538649559021\n",
      "Classification Model Iter 35/100 - Loss: 1.9223294258117676\n",
      "Classification Model Iter 36/100 - Loss: 1.9084445238113403\n",
      "Classification Model Iter 37/100 - Loss: 1.89907705783844\n",
      "Classification Model Iter 38/100 - Loss: 1.8905774354934692\n",
      "Classification Model Iter 39/100 - Loss: 1.8817229270935059\n",
      "Classification Model Iter 40/100 - Loss: 1.8734028339385986\n",
      "Classification Model Iter 41/100 - Loss: 1.8658373355865479\n",
      "Classification Model Iter 42/100 - Loss: 1.8600053787231445\n",
      "Classification Model Iter 43/100 - Loss: 1.855366826057434\n",
      "Classification Model Iter 44/100 - Loss: 1.8490617275238037\n",
      "Classification Model Iter 45/100 - Loss: 1.844779372215271\n",
      "Classification Model Iter 46/100 - Loss: 1.8400843143463135\n",
      "Classification Model Iter 47/100 - Loss: 1.8365644216537476\n",
      "Classification Model Iter 48/100 - Loss: 1.8335498571395874\n",
      "Classification Model Iter 49/100 - Loss: 1.8301645517349243\n",
      "Classification Model Iter 50/100 - Loss: 1.828178882598877\n",
      "Classification Model Iter 51/100 - Loss: 1.824303388595581\n",
      "Classification Model Iter 52/100 - Loss: 1.8211448192596436\n",
      "Classification Model Iter 53/100 - Loss: 1.820356011390686\n",
      "Classification Model Iter 54/100 - Loss: 1.8180781602859497\n",
      "Classification Model Iter 55/100 - Loss: 1.8158683776855469\n",
      "Classification Model Iter 56/100 - Loss: 1.8145616054534912\n",
      "Classification Model Iter 57/100 - Loss: 1.8137744665145874\n",
      "Classification Model Iter 58/100 - Loss: 1.8116488456726074\n",
      "Classification Model Iter 59/100 - Loss: 1.8110907077789307\n",
      "Classification Model Iter 60/100 - Loss: 1.80918550491333\n",
      "Classification Model Iter 61/100 - Loss: 1.8073570728302002\n",
      "Classification Model Iter 62/100 - Loss: 1.807366967201233\n",
      "Classification Model Iter 63/100 - Loss: 1.806212067604065\n",
      "Classification Model Iter 64/100 - Loss: 1.805347204208374\n",
      "Classification Model Iter 65/100 - Loss: 1.8047056198120117\n",
      "Classification Model Iter 66/100 - Loss: 1.804583191871643\n",
      "Classification Model Iter 67/100 - Loss: 1.8037127256393433\n",
      "Classification Model Iter 68/100 - Loss: 1.803157091140747\n",
      "Classification Model Iter 69/100 - Loss: 1.8023453950881958\n",
      "Classification Model Iter 70/100 - Loss: 1.8024603128433228\n",
      "Classification Model Iter 71/100 - Loss: 1.8021847009658813\n",
      "Classification Model Iter 72/100 - Loss: 1.8012615442276\n",
      "Classification Model Iter 73/100 - Loss: 1.8016573190689087\n",
      "Classification Model Iter 74/100 - Loss: 1.8013027906417847\n",
      "Classification Model Iter 75/100 - Loss: 1.7997549772262573\n",
      "Classification Model Iter 76/100 - Loss: 1.8010752201080322\n",
      "Classification Model Iter 77/100 - Loss: 1.8008747100830078\n",
      "Classification Model Iter 78/100 - Loss: 1.8000829219818115\n",
      "Classification Model Iter 79/100 - Loss: 1.7991727590560913\n",
      "Classification Model Iter 80/100 - Loss: 1.7998864650726318\n",
      "Classification Model Iter 81/100 - Loss: 1.7991993427276611\n",
      "Classification Model Iter 82/100 - Loss: 1.7999294996261597\n",
      "Classification Model Iter 83/100 - Loss: 1.7995320558547974\n",
      "Classification Model Iter 84/100 - Loss: 1.7991373538970947\n",
      "Classification Model Iter 85/100 - Loss: 1.7988637685775757\n",
      "Classification Model Iter 86/100 - Loss: 1.7985246181488037\n",
      "Classification Model Iter 87/100 - Loss: 1.7988977432250977\n",
      "Classification Model Iter 88/100 - Loss: 1.7989470958709717\n",
      "Classification Model Iter 89/100 - Loss: 1.7985281944274902\n",
      "Classification Model Iter 90/100 - Loss: 1.7989639043807983\n",
      "Classification Model Iter 91/100 - Loss: 1.7986732721328735\n",
      "Classification Model Iter 92/100 - Loss: 1.7982218265533447\n",
      "Classification Model Iter 93/100 - Loss: 1.7983198165893555\n",
      "Classification Model Iter 94/100 - Loss: 1.7983537912368774\n",
      "Classification Model Iter 95/100 - Loss: 1.7980982065200806\n",
      "Classification Model Iter 96/100 - Loss: 1.7985217571258545\n",
      "Classification Model Iter 97/100 - Loss: 1.7978661060333252\n",
      "Classification Model Iter 98/100 - Loss: 1.7985285520553589\n",
      "Classification Model Iter 99/100 - Loss: 1.7980625629425049\n",
      "Classification Model Iter 100/100 - Loss: 1.7980599403381348\n"
     ]
    }
   ],
   "source": [
    "# Training Settings\n",
    "training_iter = 100  # Number of training iterations\n",
    "lr = 0.1\n",
    "lengthscale = 0.1\n",
    "outputscale = 2.0\n",
    "\n",
    "model_1.train()\n",
    "likelihood_1.train()\n",
    "\n",
    "optimizer_1 = torch.optim.Adam(model_1.parameters(), lr=lr)\n",
    "mll_1 = gpytorch.mlls.VariationalELBO(likelihood_1, model_1, num_data=X_train_1.size(0))\n",
    "\n",
    "# Implement Gradient Clipping\n",
    "for i in range(training_iter):\n",
    "    optimizer_1.zero_grad()\n",
    "    output_1 = model_1(X_train_1)\n",
    "    \n",
    "    # Check for NaNs in the output\n",
    "    if torch.isnan(output_1.mean).any():\n",
    "        print(f\"Warning: NaN detected in output at iteration {i + 1}\")\n",
    "        break\n",
    "    \n",
    "    loss_1 = -mll_1(output_1, y_train_1)\n",
    "    loss_1.backward()\n",
    "    \n",
    "    # Clip gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model_1.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer_1.step()\n",
    "    print(f'Classification Model Iter {i + 1}/{training_iter} - Loss: {loss_1.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8e94fa1-82f3-4f8f-b0ab-08485e5ba361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Regression Model Training\n",
    "# model_2.train()\n",
    "# likelihood_2.train()\n",
    "\n",
    "# optimizer_2 = torch.optim.Adam(model_2.parameters(), lr=0.1)\n",
    "# mll_2 = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood_2, model_2)\n",
    "\n",
    "# for i in range(training_iter):\n",
    "#     optimizer_2.zero_grad()\n",
    "#     output_2 = model_2(X_train_2)\n",
    "    \n",
    "#     # Calculate loss\n",
    "#     loss_2 = -mll_2(output_2, y_train_2)\n",
    "    \n",
    "#     loss_2.backward()\n",
    "#     optimizer_2.step()\n",
    "#     print(f'Regression Model Iter {i + 1}/{training_iter} - Loss: {loss_2.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aff3c7d-06f1-4005-8020-a6b8c941b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params, best_score = cross_val_score_gp(GPRegressionModel, X_train_2, y_train_2, param_grid, cv=5)\n",
    "# print(f'Best Hyperparameters: {best_params}')\n",
    "# print(f'Best Cross-Validation Score: {best_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a860a-8901-428c-9c80-dba98c01b96b",
   "metadata": {},
   "source": [
    "From hyperparamer tuning:\n",
    "Best Hyperparameters: {'lr': 0.1, 'lengthscale': 0.1, 'outputscale': 2.0}\n",
    "Best Cross-Validation Score: 1.1007592916488647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d3514b5-7659-4d24-85be-a31d80c0be5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Model Iter 1/100 - Loss: 1.4035917520523071\n",
      "Regression Model Iter 2/100 - Loss: 1.3693772554397583\n",
      "Regression Model Iter 3/100 - Loss: 1.3305561542510986\n",
      "Regression Model Iter 4/100 - Loss: 1.2998241186141968\n",
      "Regression Model Iter 5/100 - Loss: 1.2683820724487305\n",
      "Regression Model Iter 6/100 - Loss: 1.2461696863174438\n",
      "Regression Model Iter 7/100 - Loss: 1.2359533309936523\n",
      "Regression Model Iter 8/100 - Loss: 1.2253392934799194\n",
      "Regression Model Iter 9/100 - Loss: 1.212555170059204\n",
      "Regression Model Iter 10/100 - Loss: 1.208687663078308\n",
      "Regression Model Iter 11/100 - Loss: 1.1953576803207397\n",
      "Regression Model Iter 12/100 - Loss: 1.204393982887268\n",
      "Regression Model Iter 13/100 - Loss: 1.1930471658706665\n",
      "Regression Model Iter 14/100 - Loss: 1.188614845275879\n",
      "Regression Model Iter 15/100 - Loss: 1.1860389709472656\n",
      "Regression Model Iter 16/100 - Loss: 1.178457260131836\n",
      "Regression Model Iter 17/100 - Loss: 1.1718426942825317\n",
      "Regression Model Iter 18/100 - Loss: 1.1657187938690186\n",
      "Regression Model Iter 19/100 - Loss: 1.1624261140823364\n",
      "Regression Model Iter 20/100 - Loss: 1.148980736732483\n",
      "Regression Model Iter 21/100 - Loss: 1.152714729309082\n",
      "Regression Model Iter 22/100 - Loss: 1.1475425958633423\n",
      "Regression Model Iter 23/100 - Loss: 1.1452322006225586\n",
      "Regression Model Iter 24/100 - Loss: 1.1404120922088623\n",
      "Regression Model Iter 25/100 - Loss: 1.1431262493133545\n",
      "Regression Model Iter 26/100 - Loss: 1.140509843826294\n",
      "Regression Model Iter 27/100 - Loss: 1.1361409425735474\n",
      "Regression Model Iter 28/100 - Loss: 1.1357057094573975\n",
      "Regression Model Iter 29/100 - Loss: 1.134018898010254\n",
      "Regression Model Iter 30/100 - Loss: 1.1291937828063965\n",
      "Regression Model Iter 31/100 - Loss: 1.1289081573486328\n",
      "Regression Model Iter 32/100 - Loss: 1.1254671812057495\n",
      "Regression Model Iter 33/100 - Loss: 1.1228384971618652\n",
      "Regression Model Iter 34/100 - Loss: 1.1227339506149292\n",
      "Regression Model Iter 35/100 - Loss: 1.1227326393127441\n",
      "Regression Model Iter 36/100 - Loss: 1.1171983480453491\n",
      "Regression Model Iter 37/100 - Loss: 1.1189379692077637\n",
      "Regression Model Iter 38/100 - Loss: 1.1179900169372559\n",
      "Regression Model Iter 39/100 - Loss: 1.11614191532135\n",
      "Regression Model Iter 40/100 - Loss: 1.1138741970062256\n",
      "Regression Model Iter 41/100 - Loss: 1.1166833639144897\n",
      "Regression Model Iter 42/100 - Loss: 1.1108266115188599\n",
      "Regression Model Iter 43/100 - Loss: 1.1117888689041138\n",
      "Regression Model Iter 44/100 - Loss: 1.110220193862915\n",
      "Regression Model Iter 45/100 - Loss: 1.1088218688964844\n",
      "Regression Model Iter 46/100 - Loss: 1.1101561784744263\n",
      "Regression Model Iter 47/100 - Loss: 1.1104881763458252\n",
      "Regression Model Iter 48/100 - Loss: 1.1093487739562988\n",
      "Regression Model Iter 49/100 - Loss: 1.1100164651870728\n",
      "Regression Model Iter 50/100 - Loss: 1.1086270809173584\n",
      "Regression Model Iter 51/100 - Loss: 1.1096552610397339\n",
      "Regression Model Iter 52/100 - Loss: 1.1095445156097412\n",
      "Regression Model Iter 53/100 - Loss: 1.1104481220245361\n",
      "Regression Model Iter 54/100 - Loss: 1.108360767364502\n",
      "Regression Model Iter 55/100 - Loss: 1.1083465814590454\n",
      "Regression Model Iter 56/100 - Loss: 1.1087030172348022\n",
      "Regression Model Iter 57/100 - Loss: 1.1076126098632812\n",
      "Regression Model Iter 58/100 - Loss: 1.1078846454620361\n",
      "Regression Model Iter 59/100 - Loss: 1.108380913734436\n",
      "Regression Model Iter 60/100 - Loss: 1.1088608503341675\n",
      "Regression Model Iter 61/100 - Loss: 1.1086410284042358\n",
      "Regression Model Iter 62/100 - Loss: 1.1086537837982178\n",
      "Regression Model Iter 63/100 - Loss: 1.1075624227523804\n",
      "Regression Model Iter 64/100 - Loss: 1.1090703010559082\n",
      "Regression Model Iter 65/100 - Loss: 1.1072614192962646\n",
      "Regression Model Iter 66/100 - Loss: 1.1078078746795654\n",
      "Regression Model Iter 67/100 - Loss: 1.108285903930664\n",
      "Regression Model Iter 68/100 - Loss: 1.107263207435608\n",
      "Regression Model Iter 69/100 - Loss: 1.1073895692825317\n",
      "Regression Model Iter 70/100 - Loss: 1.1077474355697632\n",
      "Regression Model Iter 71/100 - Loss: 1.107549786567688\n",
      "Regression Model Iter 72/100 - Loss: 1.1080665588378906\n",
      "Regression Model Iter 73/100 - Loss: 1.1089917421340942\n",
      "Regression Model Iter 74/100 - Loss: 1.109232783317566\n",
      "Regression Model Iter 75/100 - Loss: 1.1074515581130981\n",
      "Regression Model Iter 76/100 - Loss: 1.1083649396896362\n",
      "Regression Model Iter 77/100 - Loss: 1.1068601608276367\n",
      "Regression Model Iter 78/100 - Loss: 1.1081081628799438\n",
      "Regression Model Iter 79/100 - Loss: 1.1066709756851196\n",
      "Regression Model Iter 80/100 - Loss: 1.1082830429077148\n",
      "Regression Model Iter 81/100 - Loss: 1.1072752475738525\n",
      "Regression Model Iter 82/100 - Loss: 1.1087614297866821\n",
      "Regression Model Iter 83/100 - Loss: 1.1084662675857544\n",
      "Regression Model Iter 84/100 - Loss: 1.1088463068008423\n",
      "Regression Model Iter 85/100 - Loss: 1.107521653175354\n",
      "Regression Model Iter 86/100 - Loss: 1.1086281538009644\n",
      "Regression Model Iter 87/100 - Loss: 1.1080069541931152\n",
      "Regression Model Iter 88/100 - Loss: 1.1087368726730347\n",
      "Regression Model Iter 89/100 - Loss: 1.1070899963378906\n",
      "Regression Model Iter 90/100 - Loss: 1.1083793640136719\n",
      "Regression Model Iter 91/100 - Loss: 1.108419418334961\n",
      "Regression Model Iter 92/100 - Loss: 1.1087547540664673\n",
      "Regression Model Iter 93/100 - Loss: 1.1087883710861206\n",
      "Regression Model Iter 94/100 - Loss: 1.1073857545852661\n",
      "Regression Model Iter 95/100 - Loss: 1.1074604988098145\n",
      "Regression Model Iter 96/100 - Loss: 1.105908751487732\n",
      "Regression Model Iter 97/100 - Loss: 1.1072388887405396\n",
      "Regression Model Iter 98/100 - Loss: 1.1085424423217773\n",
      "Regression Model Iter 99/100 - Loss: 1.108819842338562\n",
      "Regression Model Iter 100/100 - Loss: 1.1083728075027466\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "training_iter = 100  # Number of training iterations\n",
    "lr = 0.1\n",
    "lengthscale = 0.1\n",
    "outputscale = 2.0\n",
    "\n",
    "# From hyperparameter tuning: \n",
    "best_params = {'lr': 0.1, 'lengthscale': 0.1, 'outputscale': 2.0}\n",
    "\n",
    "# Initialize the final model with best hyperparameters\n",
    "likelihood_2 = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model_2 = GPRegressionModel(X_train_2, y_train_2, likelihood_2)\n",
    "model_2.covar_module.base_kernel.lengthscale = best_params['lengthscale']\n",
    "model_2.covar_module.outputscale = best_params['outputscale']\n",
    "\n",
    "optimizer_2 = torch.optim.Adam(model_2.parameters(), lr=best_params['lr'])\n",
    "mll_2 = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood_2, model_2)\n",
    "\n",
    "# Train the model\n",
    "model_2.train()\n",
    "likelihood_2.train()\n",
    "for i in range(training_iter):\n",
    "    optimizer_2.zero_grad()\n",
    "    output_2 = model_2(X_train_2)\n",
    "    loss_2 = -mll_2(output_2, y_train_2)\n",
    "    loss_2.backward()\n",
    "    optimizer_2.step()\n",
    "    print(f'Regression Model Iter {i + 1}/{training_iter} - Loss: {loss_2.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd2a9885-3c31-4912-9bd3-59dbd8f8c2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m lower, upper \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mconfidence_region()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Reverse the standardization for the mean and confidence intervals\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m mean_unscaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241m.\u001b[39minverse_transform(mean\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     44\u001b[0m lower_unscaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(lower\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     45\u001b[0m upper_unscaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(upper\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scaler' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set models and likelihoods to evaluation mode\n",
    "model_1.eval()\n",
    "model_2.eval()\n",
    "likelihood_1.eval()\n",
    "likelihood_2.eval()\n",
    "\n",
    "# Initialize plots\n",
    "f, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Make predictions (use the test tensors X_train_1 and X_train_2)\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    predictions = [likelihood(model(X_train)) for model, likelihood, X_train in zip(models, likelihoods, [X_train_1, X_train_2])]\n",
    "\n",
    "# Iterate through the models and predictions\n",
    "for i, (submodel, prediction, ax) in enumerate(zip(models, predictions, axs)):\n",
    "    if i == 0:  # Classification Model\n",
    "        predicted_probabilities = torch.softmax(prediction.mean, dim=0)  # Apply softmax across the class dimension\n",
    "        predicted_labels = predicted_probabilities.argmax(dim=0).detach().numpy()  # Use argmax on the class dimension\n",
    "        print(predicted_probabilities)\n",
    "        print(predicted_labels)\n",
    "        \n",
    "        # Ensure y_train_1 is flattened to match the predicted_labels shape\n",
    "        y_train_1_flat = y_train_1.numpy().flatten()\n",
    "\n",
    "        # Plot Predicted vs Observed labels\n",
    "        if y_train_1_flat.shape == predicted_labels.shape:\n",
    "            ax.scatter(y_train_1_flat, predicted_labels, c='b', marker='o')  # Scatter plot of predicted vs observed\n",
    "            ax.plot([0, len(label_encoder.classes_) - 1], [0, len(label_encoder.classes_) - 1], 'r--')  # Diagonal line for reference\n",
    "            ax.set_title('Classification: Predicted vs Observed')\n",
    "            ax.set_xlabel('Observed Labels')\n",
    "            ax.set_ylabel('Predicted Labels')\n",
    "            ax.set_xlim([0, len(label_encoder.classes_) - 1])\n",
    "            ax.set_ylim([0, len(label_encoder.classes_) - 1])\n",
    "            ax.legend(['Reference Line', 'Predicted vs Observed'])\n",
    "        else:\n",
    "            print(\"Warning: Shapes of y_train_1 and predicted_labels do not match!\")\n",
    "\n",
    "    else:  # Regression Model\n",
    "        mean = prediction.mean\n",
    "        lower, upper = prediction.confidence_region()\n",
    "\n",
    "        # Reverse the standardization for the mean and confidence intervals\n",
    "        mean_unscaled = scaler.inverse_transform(mean.numpy().reshape(-1, 1)).flatten()\n",
    "        lower_unscaled = scaler.inverse_transform(lower.numpy().reshape(-1, 1)).flatten()\n",
    "        upper_unscaled = scaler.inverse_transform(upper.numpy().reshape(-1, 1)).flatten()\n",
    "        y_train_unscaled = scaler.inverse_transform(y_train_2.numpy().reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Plot the unscaled test data and predictions\n",
    "        ax.plot(range(len(y_train_unscaled)), y_train_unscaled, 'k*')  # Plot the unscaled test targets\n",
    "        ax.plot(range(len(mean_unscaled)), mean_unscaled, 'b')  # Unscaled predictive mean as blue line\n",
    "        # Shade in confidence interval\n",
    "        ax.fill_between(range(len(mean_unscaled)), lower_unscaled, upper_unscaled, alpha=0.5)\n",
    "        ax.set_ylim([min(lower_unscaled), max(upper_unscaled)])  # Adjust the y-axis limits\n",
    "        ax.set_title('Regression: Mean and Confidence Interval (Unscaled)')\n",
    "        ax.legend(['Observed Data (Unscaled)', 'Mean (Unscaled)', 'Confidence Interval'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e76c9-8c6f-4ce9-8b47-17235d141861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize plots\n",
    "f, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Make predictions (use the test tensors X_test_1 and X_test_2)\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    predictions = [likelihood(model(X_test)) for model, likelihood, X_test in zip(models, likelihoods, [X_test_1, X_test_2])]\n",
    "\n",
    "# Iterate through the models and predictions\n",
    "for i, (submodel, prediction, ax) in enumerate(zip(models, predictions, axs)):\n",
    "    if i == 0:  # Classification Model\n",
    "        predicted_probabilities = torch.softmax(prediction.mean, dim=0)  # Apply softmax across the class dimension\n",
    "        predicted_labels = predicted_probabilities.argmax(dim=0).detach().numpy()  # Use argmax on the class dimension\n",
    "        print(predicted_probabilities)\n",
    "        print(predicted_labels)\n",
    "\n",
    "        # Ensure y_test_1 is flattened to match the predicted_labels shape\n",
    "        y_test_1_flat = y_test_1.numpy().flatten()\n",
    "\n",
    "        # Plot Predicted vs Observed labels\n",
    "        if y_test_1_flat.shape == predicted_labels.shape:\n",
    "            ax.scatter(y_test_1_flat, predicted_labels, c='b', marker='o')  # Scatter plot of predicted vs observed\n",
    "            ax.plot([0, len(label_encoder.classes_) - 1], [0, len(label_encoder.classes_) - 1], 'r--')  # Diagonal line for reference\n",
    "            ax.set_title('Classification: Predicted vs Observed')\n",
    "            ax.set_xlabel('Observed Labels')\n",
    "            ax.set_ylabel('Predicted Labels')\n",
    "            ax.set_xlim([0, len(label_encoder.classes_) - 1])\n",
    "            ax.set_ylim([0, len(label_encoder.classes_) - 1])\n",
    "            ax.legend(['Reference Line', 'Predicted vs Observed'])\n",
    "        else:\n",
    "            print(\"Warning: Shapes of y_test_1 and predicted_labels do not match!\")\n",
    "\n",
    "    else:  # Regression Model\n",
    "        mean = prediction.mean\n",
    "        lower, upper = prediction.confidence_region()\n",
    "\n",
    "        # Reverse the standardization for the mean and confidence intervals\n",
    "        mean_unscaled = scaler.inverse_transform(mean.numpy().reshape(-1, 1)).flatten()\n",
    "        lower_unscaled = scaler.inverse_transform(lower.numpy().reshape(-1, 1)).flatten()\n",
    "        upper_unscaled = scaler.inverse_transform(upper.numpy().reshape(-1, 1)).flatten()\n",
    "        y_test_unscaled = scaler.inverse_transform(y_test_2.numpy().reshape(-1, 1)).flatten()\n",
    "\n",
    "        # # Plot the unscaled test data and predictions\n",
    "        # ax.plot(range(len(y_test_unscaled)), y_test_unscaled, 'k*')  # Plot the unscaled test targets\n",
    "        # ax.plot(range(len(mean_unscaled)), mean_unscaled, 'b')  # Unscaled predictive mean as blue line\n",
    "        # # Shade in confidence interval\n",
    "        # ax.fill_between(range(len(mean_unscaled)), lower_unscaled, upper_unscaled, alpha=0.5)\n",
    "        # ax.set_ylim([min(lower_unscaled), max(upper_unscaled)])  # Adjust the y-axis limits\n",
    "        # ax.set_title('Regression: Mean and Confidence Interval (Unscaled)')\n",
    "        # ax.legend(['Observed Data (Unscaled)', 'Prediction Mean (Unscaled)', 'Confidence Interval'])        \n",
    "        ax.scatter(y_test_unscaled, mean_unscaled, c='blue', alpha=0.6, edgecolors='k', label='Predicted vs Observed')\n",
    "        ax.set_xlim(0,60)\n",
    "        ax.set_ylim(0,60)\n",
    "        ax.set_xlabel('Observed')\n",
    "        ax.set_ylabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9572ab2-b989-44a2-b38a-ce10837c2663",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd695f-7b66-46d9-a6d9-2c605f8cdca9",
   "metadata": {},
   "source": [
    "Sources:\n",
    "1. https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/ModelList_GP_Regression.html\n",
    "2. https://jamesbrind.uk/posts/2d-gaussian-process-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c0ff8-2357-4fc0-ace3-79768843c4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
