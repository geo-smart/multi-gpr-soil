{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b011d16-5e73-46f5-9e0d-3443ec2ff42b",
   "metadata": {},
   "source": [
    "# Pytorch Multi-output GPR\n",
    "\n",
    "The purpose of this notebook is to predict the soil type and soil thickness of Layer 1 as a Multi-Output GP model using a ModelList. Unlike a Multi-Task model, Multi-Output models do not represent correlations between outcomes, but treat outcomes independently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb804bf-e961-4804-a088-3f113bb7381f",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d69f4200-2066-4eab-96a2-c7eba0136384",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch --quiet\n",
    "!pip install gpytorch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e264b31-4228-4088-9235-9c5cc3f99f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.models import ApproximateGP, ExactGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "from gpytorch.likelihoods import SoftmaxLikelihood, GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13712b6b-e473-4e08-861a-1a4885d4eeff",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa664a05-0bd2-453a-9e5d-9068ce2cd200",
   "metadata": {},
   "source": [
    "#### Load training data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f4ce29-8f04-48e9-9944-01b8f6e4d434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOREHOLE_ID</th>\n",
       "      <th>BOREHOLE_NAME</th>\n",
       "      <th>BOREHOLE_TYPE</th>\n",
       "      <th>BOREHOLE_DEPTH_FT</th>\n",
       "      <th>ELEVATION_FT</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LAYER_NUMBER</th>\n",
       "      <th>TOP_DEPTH_FT</th>\n",
       "      <th>BOTTOM_DEPTH_FT</th>\n",
       "      <th>USCS</th>\n",
       "      <th>SIMPLE_USCS</th>\n",
       "      <th>LAYER_THICKNESS_FT</th>\n",
       "      <th>geometry</th>\n",
       "      <th>MAPPED_UNIT</th>\n",
       "      <th>SLOPE</th>\n",
       "      <th>ROUGHNESS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7138</td>\n",
       "      <td>B-1</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>20.3</td>\n",
       "      <td>167.8</td>\n",
       "      <td>47.656719</td>\n",
       "      <td>-122.305728</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SP</td>\n",
       "      <td>S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>POINT (-122.30573 47.65672)</td>\n",
       "      <td>Qvt</td>\n",
       "      <td>3.668571</td>\n",
       "      <td>18.199593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7143</td>\n",
       "      <td>B-1-92</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>48.5</td>\n",
       "      <td>123.2</td>\n",
       "      <td>47.653642</td>\n",
       "      <td>-122.306837</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ML</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "      <td>POINT (-122.30684 47.65364)</td>\n",
       "      <td>Qvt</td>\n",
       "      <td>2.222722</td>\n",
       "      <td>10.232103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7144</td>\n",
       "      <td>B-2-92</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>42.5</td>\n",
       "      <td>122.9</td>\n",
       "      <td>47.653766</td>\n",
       "      <td>-122.306468</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ML</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "      <td>POINT (-122.30647 47.65377)</td>\n",
       "      <td>Qvt</td>\n",
       "      <td>1.996012</td>\n",
       "      <td>9.385303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7145</td>\n",
       "      <td>B-3-92</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>43.0</td>\n",
       "      <td>117.3</td>\n",
       "      <td>47.653256</td>\n",
       "      <td>-122.306638</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ML</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "      <td>POINT (-122.30664 47.65326)</td>\n",
       "      <td>Qvt</td>\n",
       "      <td>3.011285</td>\n",
       "      <td>14.187020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7146</td>\n",
       "      <td>B-4-92</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>43.0</td>\n",
       "      <td>123.4</td>\n",
       "      <td>47.653709</td>\n",
       "      <td>-122.306259</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>SM</td>\n",
       "      <td>S</td>\n",
       "      <td>0.5</td>\n",
       "      <td>POINT (-122.30626 47.65371)</td>\n",
       "      <td>Qvt</td>\n",
       "      <td>1.996012</td>\n",
       "      <td>9.385303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>143845</td>\n",
       "      <td>MW-1</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>42.0</td>\n",
       "      <td>217.4</td>\n",
       "      <td>47.661044</td>\n",
       "      <td>-122.342453</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>SM</td>\n",
       "      <td>S</td>\n",
       "      <td>7.5</td>\n",
       "      <td>POINT (-122.34245 47.66104)</td>\n",
       "      <td>Qvr</td>\n",
       "      <td>1.594538</td>\n",
       "      <td>10.106370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>143846</td>\n",
       "      <td>MW-2</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>43.0</td>\n",
       "      <td>218.2</td>\n",
       "      <td>47.661138</td>\n",
       "      <td>-122.342516</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>SM</td>\n",
       "      <td>S</td>\n",
       "      <td>7.5</td>\n",
       "      <td>POINT (-122.34252 47.66114)</td>\n",
       "      <td>Qvr</td>\n",
       "      <td>1.340698</td>\n",
       "      <td>8.735672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>143847</td>\n",
       "      <td>MW-3</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>45.5</td>\n",
       "      <td>219.3</td>\n",
       "      <td>47.661276</td>\n",
       "      <td>-122.342492</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>SM</td>\n",
       "      <td>S</td>\n",
       "      <td>18.0</td>\n",
       "      <td>POINT (-122.34249 47.66128)</td>\n",
       "      <td>Qvr</td>\n",
       "      <td>1.594538</td>\n",
       "      <td>10.106370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>143848</td>\n",
       "      <td>MW-4</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>43.0</td>\n",
       "      <td>219.1</td>\n",
       "      <td>47.661177</td>\n",
       "      <td>-122.342383</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>SM</td>\n",
       "      <td>S</td>\n",
       "      <td>10.0</td>\n",
       "      <td>POINT (-122.34238 47.66118)</td>\n",
       "      <td>Qvr</td>\n",
       "      <td>1.594538</td>\n",
       "      <td>10.106370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>143849</td>\n",
       "      <td>MW-5</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>43.0</td>\n",
       "      <td>218.9</td>\n",
       "      <td>47.661273</td>\n",
       "      <td>-122.342365</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>SM</td>\n",
       "      <td>S</td>\n",
       "      <td>8.5</td>\n",
       "      <td>POINT (-122.34237 47.66127)</td>\n",
       "      <td>Qvr</td>\n",
       "      <td>1.594538</td>\n",
       "      <td>10.106370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BOREHOLE_ID BOREHOLE_NAME BOREHOLE_TYPE  BOREHOLE_DEPTH_FT  ELEVATION_FT  \\\n",
       "0           7138           B-1  Geotechnical               20.3         167.8   \n",
       "1           7143        B-1-92  Geotechnical               48.5         123.2   \n",
       "2           7144        B-2-92  Geotechnical               42.5         122.9   \n",
       "3           7145        B-3-92  Geotechnical               43.0         117.3   \n",
       "4           7146        B-4-92  Geotechnical               43.0         123.4   \n",
       "..           ...           ...           ...                ...           ...   \n",
       "501       143845          MW-1  Geotechnical               42.0         217.4   \n",
       "502       143846          MW-2  Geotechnical               43.0         218.2   \n",
       "503       143847          MW-3  Geotechnical               45.5         219.3   \n",
       "504       143848          MW-4  Geotechnical               43.0         219.1   \n",
       "505       143849          MW-5  Geotechnical               43.0         218.9   \n",
       "\n",
       "      LATITUDE   LONGITUDE  LAYER_NUMBER  TOP_DEPTH_FT  BOTTOM_DEPTH_FT USCS  \\\n",
       "0    47.656719 -122.305728             1           0.0              1.0   SP   \n",
       "1    47.653642 -122.306837             1           0.0              1.0   ML   \n",
       "2    47.653766 -122.306468             1           0.0              1.0   ML   \n",
       "3    47.653256 -122.306638             1           0.0              1.0   ML   \n",
       "4    47.653709 -122.306259             1           0.0              0.5   SM   \n",
       "..         ...         ...           ...           ...              ...  ...   \n",
       "501  47.661044 -122.342453             1           0.0              7.5   SM   \n",
       "502  47.661138 -122.342516             1           0.0              7.5   SM   \n",
       "503  47.661276 -122.342492             1           0.0             18.0   SM   \n",
       "504  47.661177 -122.342383             1           0.0             10.0   SM   \n",
       "505  47.661273 -122.342365             1           0.0              8.5   SM   \n",
       "\n",
       "    SIMPLE_USCS  LAYER_THICKNESS_FT                     geometry MAPPED_UNIT  \\\n",
       "0             S                 1.0  POINT (-122.30573 47.65672)         Qvt   \n",
       "1             M                 1.0  POINT (-122.30684 47.65364)         Qvt   \n",
       "2             M                 1.0  POINT (-122.30647 47.65377)         Qvt   \n",
       "3             M                 1.0  POINT (-122.30664 47.65326)         Qvt   \n",
       "4             S                 0.5  POINT (-122.30626 47.65371)         Qvt   \n",
       "..          ...                 ...                          ...         ...   \n",
       "501           S                 7.5  POINT (-122.34245 47.66104)         Qvr   \n",
       "502           S                 7.5  POINT (-122.34252 47.66114)         Qvr   \n",
       "503           S                18.0  POINT (-122.34249 47.66128)         Qvr   \n",
       "504           S                10.0  POINT (-122.34238 47.66118)         Qvr   \n",
       "505           S                 8.5  POINT (-122.34237 47.66127)         Qvr   \n",
       "\n",
       "        SLOPE  ROUGHNESS  \n",
       "0    3.668571  18.199593  \n",
       "1    2.222722  10.232103  \n",
       "2    1.996012   9.385303  \n",
       "3    3.011285  14.187020  \n",
       "4    1.996012   9.385303  \n",
       "..        ...        ...  \n",
       "501  1.594538  10.106370  \n",
       "502  1.340698   8.735672  \n",
       "503  1.594538  10.106370  \n",
       "504  1.594538  10.106370  \n",
       "505  1.594538  10.106370  \n",
       "\n",
       "[506 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training data CSV file compiled in `1-data_access.ipynb`\n",
    "input_file = '../data/2-uw_layer1_trainingdata.csv'\n",
    "training_data = pd.read_csv(input_file)\n",
    "\n",
    "# Create a GeoDataFrame from the DataFrame\n",
    "training_data = gpd.GeoDataFrame(training_data, geometry=gpd.points_from_xy(training_data.LONGITUDE, training_data.LATITUDE))\n",
    "    \n",
    "# Set the CRS to WGS84 (latitude and longitude)\n",
    "training_data.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb9989-deeb-486b-8e0f-fe3f457fd7f6",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b471b6f-3014-4f81-9d7b-379e2d8e682c",
   "metadata": {},
   "source": [
    "#### Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ffa9f1-ed2e-460a-8148-035ba4f004fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BOREHOLE_ID', 'BOREHOLE_NAME', 'BOREHOLE_TYPE', 'BOREHOLE_DEPTH_FT',\n",
       "       'ELEVATION_FT', 'LATITUDE', 'LONGITUDE', 'LAYER_NUMBER', 'TOP_DEPTH_FT',\n",
       "       'BOTTOM_DEPTH_FT', 'USCS', 'SIMPLE_USCS', 'LAYER_THICKNESS_FT',\n",
       "       'geometry', 'MAPPED_UNIT', 'SLOPE', 'ROUGHNESS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c46a3a7a-5e3a-4e14-b974-91b4a6720104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506\n",
      "504\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))\n",
    "training_data = training_data[training_data['USCS']!='CH']\n",
    "training_data = training_data[training_data['USCS']!='GP-GM']\n",
    "\n",
    "print(len(training_data))\n",
    "# Assuming training_data is your DataFrame\n",
    "X = training_data[['MAPPED_UNIT', 'SLOPE', 'ROUGHNESS']]\n",
    "y_1 = training_data['USCS']\n",
    "y_2 = training_data['LAYER_THICKNESS_FT']\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), ['MAPPED_UNIT']),  # One-hot encode categorical features\n",
    "        ('num', StandardScaler(), ['SLOPE', 'ROUGHNESS'])  # Standardize numerical features\n",
    "    ])\n",
    "\n",
    "# Apply transformations and convert to dense array\n",
    "X_processed = preprocessor.fit_transform(X).toarray()  # Convert to array if sparse\n",
    "\n",
    "# Convert processed data to tensors\n",
    "X_tensor = torch.tensor(X_processed, dtype=torch.float32)\n",
    "## When dataset gets large and the one-hot encoding results in a sparse matrix, consider using a sparse tensor in PyTorch. This can help with memory efficiency\n",
    "# X_tensor = torch.tensor(X_processed.todense(), dtype=torch.float32) if scipy.sparse.issparse(X_processed) else torch.tensor(X_processed, dtype=torch.float32) \n",
    "\n",
    "# Handling target for SIMPLE_USCS\n",
    "label_encoder = LabelEncoder()\n",
    "y_1_encoded = label_encoder.fit_transform(y_1) # Ensure target is in integer form\n",
    "y_1_tensor = torch.tensor(y_1_encoded, dtype=torch.long) # Convert to tensor\n",
    "\n",
    "# Handling target for LAYER_THICKNESS_FT\n",
    "scaler = StandardScaler()\n",
    "y_2_scaled = scaler.fit_transform(y_2.values.reshape(-1, 1))\n",
    "y_2_tensor = torch.tensor(y_2_scaled, dtype=torch.float32).squeeze() \n",
    "\n",
    "# Split the data into training and test sets for both models\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_tensor, y_1_tensor, test_size=0.2, random_state=42, stratify=y_1)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_tensor, y_2_tensor, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "225f371b-ed50-4f84-a6bd-e100f1b876fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'G': 'Course',\n",
       " 'S': 'Course',\n",
       " 'M': 'Fine',\n",
       " 'C': 'Fine',\n",
       " 'O': 'Fine',\n",
       " 'P': 'Organic'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = {} # empty dict for parent classes\n",
    "root['G'] = 'Course'\n",
    "root['S'] = 'Course'\n",
    "root['M'] = 'Fine'\n",
    "root['C'] = 'Fine'\n",
    "root['O'] = 'Fine'\n",
    "root['P'] = 'Organic'\n",
    "\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ebd61f-470e-49c5-b9af-e71bf7180a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CL', 'GM', 'GP', 'GW', 'MH', 'ML', 'OH', 'OL', 'PT', 'SC', 'SM',\n",
       "       'SP', 'SP-SM', 'SW', 'SW-SM'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(label_encoder.inverse_transform(y_1_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ef35e99-112c-4e01-a699-680c84fd94c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.transform(['GM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b08ee339-cfd4-4f0e-947c-2317596978e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CL': 'C',\n",
       " 'GM': 'G',\n",
       " 'GP': 'G',\n",
       " 'GW': 'G',\n",
       " 'MH': 'M',\n",
       " 'ML': 'M',\n",
       " 'OH': 'O',\n",
       " 'OL': 'O',\n",
       " 'PT': 'P',\n",
       " 'SC': 'S',\n",
       " 'SM': 'S',\n",
       " 'SP': 'S',\n",
       " 'SP-SM': 'S',\n",
       " 'SW': 'S',\n",
       " 'SW-SM': 'S'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent = {}\n",
    "for i, label in enumerate(np.unique(label_encoder.inverse_transform(y_1_encoded))):\n",
    "    parent[label] = label[0]\n",
    "parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac7ead20-7840-4c4f-b4df-ee9efd93118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = {}\n",
    "root['G'] = 'Course'\n",
    "root['S'] = 'Course'\n",
    "root['M'] = 'Fine'    \n",
    "root['C'] = 'Fine'\n",
    "root['O'] = 'Fine'\n",
    "root['P'] = 'Organic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "340a3725-457e-4ffe-bdb3-f3ba152bf81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(ypred, ytrue):\n",
    "    # evaluate results\n",
    "    ycheck = ypred - ytrue\n",
    "    \n",
    "    # transform results\n",
    "    ypred = label_encoder.inverse_transform(ypred)\n",
    "    ytrue = label_encoder.inverse_transform(ytrue)\n",
    "\n",
    "    print(ypred[0])\n",
    "    \n",
    "    sim, loss = 1, 0 # init similarity and loss values\n",
    "    for i in range(len(ycheck)):\n",
    "        parent_pred, parent_true = parent[ypred[i]], parent[ytrue[i]]\n",
    "        root_pred, root_true = root[parent_pred], root[parent_true]\n",
    "        \n",
    "        if int(ycheck[i]) == 0:\n",
    "            continue\n",
    "        elif parent_pred == parent_true:\n",
    "            sim = 0.5\n",
    "        elif root_pred == root_true:\n",
    "            sim = 0.25\n",
    "        else:\n",
    "            sim = 0\n",
    "\n",
    "        loss += (1-sim)\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90866c21-8195-430d-8e42-61261992ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPClassificationModel(ApproximateGP):\n",
    "    def __init__(self, train_x, num_classes):\n",
    "        variational_distribution = CholeskyVariationalDistribution(train_x.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, train_x, variational_distribution, learn_inducing_locations=True)\n",
    "        super(GPClassificationModel, self).__init__(variational_strategy)\n",
    "        \n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x).expand([self.num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09725995-8ee9-4094-a996-483cfa12c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the likelihood and model for multiclass classification\n",
    "num_classes = len(label_encoder.classes_)\n",
    "likelihood_1 = gpytorch.likelihoods.SoftmaxLikelihood(num_classes=num_classes, mixing_weights=None)\n",
    "model_1 = GPClassificationModel(X_train_1, num_classes=num_classes)\n",
    "\n",
    "# Initialize the likelihood and model for regression\n",
    "# likelihood_2 = GaussianLikelihood()\n",
    "# model_2 = GPRegressionModel(X_train_2, y_train_2, likelihood_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d31ec59-1b17-4549-b339-1790ae7c1f2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL\n",
      "Classification Model Iter 1/1000 - Loss: 41.4199104309082\n",
      "CL\n",
      "Classification Model Iter 2/1000 - Loss: 79.78706359863281\n",
      "CL\n",
      "Classification Model Iter 3/1000 - Loss: 109.98216247558594\n",
      "CL\n",
      "Classification Model Iter 4/1000 - Loss: 80.313720703125\n",
      "CL\n",
      "Classification Model Iter 5/1000 - Loss: 47.34782028198242\n",
      "CL\n",
      "Classification Model Iter 6/1000 - Loss: 58.726749420166016\n",
      "CL\n",
      "Classification Model Iter 7/1000 - Loss: 64.6380844116211\n",
      "CL\n",
      "Classification Model Iter 8/1000 - Loss: 55.5101432800293\n",
      "CL\n",
      "Classification Model Iter 9/1000 - Loss: 49.04360580444336\n",
      "CL\n",
      "Classification Model Iter 10/1000 - Loss: 50.3821907043457\n",
      "CL\n",
      "Classification Model Iter 11/1000 - Loss: 51.53897476196289\n",
      "CL\n",
      "Classification Model Iter 12/1000 - Loss: 49.79473876953125\n",
      "CL\n",
      "Classification Model Iter 13/1000 - Loss: 48.01490783691406\n",
      "CL\n",
      "Classification Model Iter 14/1000 - Loss: 48.11121368408203\n",
      "CL\n",
      "Classification Model Iter 15/1000 - Loss: 48.15381622314453\n",
      "CL\n",
      "Classification Model Iter 16/1000 - Loss: 46.323665618896484\n",
      "CL\n",
      "Classification Model Iter 17/1000 - Loss: 44.28281784057617\n",
      "CL\n",
      "Classification Model Iter 18/1000 - Loss: 44.31277084350586\n",
      "CL\n",
      "Classification Model Iter 19/1000 - Loss: 45.88297653198242\n",
      "CL\n",
      "Classification Model Iter 20/1000 - Loss: 46.338623046875\n",
      "CL\n",
      "Classification Model Iter 21/1000 - Loss: 44.69886016845703\n",
      "CL\n",
      "Classification Model Iter 22/1000 - Loss: 42.81708526611328\n",
      "CL\n",
      "Classification Model Iter 23/1000 - Loss: 42.637062072753906\n",
      "CL\n",
      "Classification Model Iter 24/1000 - Loss: 43.723411560058594\n",
      "CL\n",
      "Classification Model Iter 25/1000 - Loss: 44.189414978027344\n",
      "CL\n",
      "Classification Model Iter 26/1000 - Loss: 43.38325881958008\n",
      "CL\n",
      "Classification Model Iter 27/1000 - Loss: 42.40870666503906\n",
      "CL\n",
      "Classification Model Iter 28/1000 - Loss: 42.27465057373047\n",
      "CL\n",
      "Classification Model Iter 29/1000 - Loss: 42.6513786315918\n",
      "CL\n",
      "Classification Model Iter 30/1000 - Loss: 42.69922637939453\n",
      "CL\n",
      "Classification Model Iter 31/1000 - Loss: 42.33522415161133\n",
      "CL\n",
      "Classification Model Iter 32/1000 - Loss: 42.083290100097656\n",
      "CL\n",
      "Classification Model Iter 33/1000 - Loss: 42.11470413208008\n",
      "CL\n",
      "Classification Model Iter 34/1000 - Loss: 42.09267807006836\n",
      "CL\n",
      "Classification Model Iter 35/1000 - Loss: 41.86409378051758\n",
      "CL\n",
      "Classification Model Iter 36/1000 - Loss: 41.70553207397461\n",
      "CL\n",
      "Classification Model Iter 37/1000 - Loss: 41.79585266113281\n",
      "CL\n",
      "Classification Model Iter 38/1000 - Loss: 41.89521789550781\n",
      "CL\n",
      "Classification Model Iter 39/1000 - Loss: 41.747249603271484\n",
      "CL\n",
      "Classification Model Iter 40/1000 - Loss: 41.4986572265625\n",
      "CL\n",
      "Classification Model Iter 41/1000 - Loss: 41.44902420043945\n",
      "CL\n",
      "Classification Model Iter 42/1000 - Loss: 41.578636169433594\n",
      "CL\n",
      "Classification Model Iter 43/1000 - Loss: 41.61275863647461\n",
      "CL\n",
      "Classification Model Iter 44/1000 - Loss: 41.47299575805664\n",
      "CL\n",
      "Classification Model Iter 45/1000 - Loss: 41.35186767578125\n",
      "CL\n",
      "Classification Model Iter 46/1000 - Loss: 41.37205505371094\n",
      "CL\n",
      "Classification Model Iter 47/1000 - Loss: 41.423274993896484\n",
      "CL\n",
      "Classification Model Iter 48/1000 - Loss: 41.38645935058594\n",
      "CL\n",
      "Classification Model Iter 49/1000 - Loss: 41.31730651855469\n",
      "CL\n",
      "Classification Model Iter 50/1000 - Loss: 41.3028450012207\n",
      "CL\n",
      "Classification Model Iter 51/1000 - Loss: 41.311119079589844\n",
      "CL\n",
      "Classification Model Iter 52/1000 - Loss: 41.28563690185547\n",
      "CL\n",
      "Classification Model Iter 53/1000 - Loss: 41.25667190551758\n",
      "CL\n",
      "Classification Model Iter 54/1000 - Loss: 41.26265335083008\n",
      "CL\n",
      "Classification Model Iter 55/1000 - Loss: 41.26692581176758\n",
      "CL\n",
      "Classification Model Iter 56/1000 - Loss: 41.2365837097168\n",
      "CL\n",
      "Classification Model Iter 57/1000 - Loss: 41.20764923095703\n",
      "CL\n",
      "Classification Model Iter 58/1000 - Loss: 41.214508056640625\n",
      "CL\n",
      "Classification Model Iter 59/1000 - Loss: 41.22756576538086\n",
      "CL\n",
      "Classification Model Iter 60/1000 - Loss: 41.21089172363281\n",
      "CL\n",
      "Classification Model Iter 61/1000 - Loss: 41.1867790222168\n",
      "CL\n",
      "Classification Model Iter 62/1000 - Loss: 41.18572998046875\n",
      "CL\n",
      "Classification Model Iter 63/1000 - Loss: 41.19371032714844\n",
      "CL\n",
      "Classification Model Iter 64/1000 - Loss: 41.1878662109375\n",
      "CL\n",
      "Classification Model Iter 65/1000 - Loss: 41.17516326904297\n",
      "CL\n",
      "Classification Model Iter 66/1000 - Loss: 41.171390533447266\n",
      "CL\n",
      "Classification Model Iter 67/1000 - Loss: 41.17201232910156\n",
      "CL\n",
      "Classification Model Iter 68/1000 - Loss: 41.16780090332031\n",
      "CL\n",
      "Classification Model Iter 69/1000 - Loss: 41.16374206542969\n",
      "CL\n",
      "Classification Model Iter 70/1000 - Loss: 41.16334915161133\n",
      "CL\n",
      "Classification Model Iter 71/1000 - Loss: 41.161155700683594\n",
      "CL\n",
      "Classification Model Iter 72/1000 - Loss: 41.15541458129883\n",
      "CL\n",
      "Classification Model Iter 73/1000 - Loss: 41.15407180786133\n",
      "CL\n",
      "Classification Model Iter 74/1000 - Loss: 41.15583419799805\n",
      "CL\n",
      "Classification Model Iter 75/1000 - Loss: 41.15388870239258\n",
      "CL\n",
      "Classification Model Iter 76/1000 - Loss: 41.1496696472168\n",
      "CL\n",
      "Classification Model Iter 77/1000 - Loss: 41.148433685302734\n",
      "CL\n",
      "Classification Model Iter 78/1000 - Loss: 41.14950942993164\n",
      "CL\n",
      "Classification Model Iter 79/1000 - Loss: 41.14816665649414\n",
      "CL\n",
      "Classification Model Iter 80/1000 - Loss: 41.146053314208984\n",
      "CL\n",
      "Classification Model Iter 81/1000 - Loss: 41.145362854003906\n",
      "CL\n",
      "Classification Model Iter 82/1000 - Loss: 41.14458084106445\n",
      "CL\n",
      "Classification Model Iter 83/1000 - Loss: 41.1441764831543\n",
      "CL\n",
      "Classification Model Iter 84/1000 - Loss: 41.14360427856445\n",
      "CL\n",
      "Classification Model Iter 85/1000 - Loss: 41.142616271972656\n",
      "CL\n",
      "Classification Model Iter 86/1000 - Loss: 41.14213943481445\n",
      "CL\n",
      "Classification Model Iter 87/1000 - Loss: 41.141536712646484\n",
      "CL\n",
      "Classification Model Iter 88/1000 - Loss: 41.141475677490234\n",
      "CL\n",
      "Classification Model Iter 89/1000 - Loss: 41.141319274902344\n",
      "CL\n",
      "Classification Model Iter 90/1000 - Loss: 41.140342712402344\n",
      "CL\n",
      "Classification Model Iter 91/1000 - Loss: 41.1401252746582\n",
      "CL\n",
      "Classification Model Iter 92/1000 - Loss: 41.14024353027344\n",
      "CL\n",
      "Classification Model Iter 93/1000 - Loss: 41.13991165161133\n",
      "CL\n",
      "Classification Model Iter 94/1000 - Loss: 41.13967514038086\n",
      "CL\n",
      "Classification Model Iter 95/1000 - Loss: 41.139251708984375\n",
      "CL\n",
      "Classification Model Iter 96/1000 - Loss: 41.138702392578125\n",
      "CL\n",
      "Classification Model Iter 97/1000 - Loss: 41.13877487182617\n",
      "CL\n",
      "Classification Model Iter 98/1000 - Loss: 41.13877868652344\n",
      "CL\n",
      "Classification Model Iter 99/1000 - Loss: 41.13863754272461\n",
      "CL\n",
      "Classification Model Iter 100/1000 - Loss: 41.13841247558594\n",
      "CL\n",
      "Classification Model Iter 101/1000 - Loss: 41.13853454589844\n",
      "CL\n",
      "Classification Model Iter 102/1000 - Loss: 41.13825607299805\n",
      "CL\n",
      "Classification Model Iter 103/1000 - Loss: 41.137962341308594\n",
      "CL\n",
      "Classification Model Iter 104/1000 - Loss: 41.13806915283203\n",
      "CL\n",
      "Classification Model Iter 105/1000 - Loss: 41.13797378540039\n",
      "CL\n",
      "Classification Model Iter 106/1000 - Loss: 41.13783264160156\n",
      "CL\n",
      "Classification Model Iter 107/1000 - Loss: 41.137977600097656\n",
      "CL\n",
      "Classification Model Iter 108/1000 - Loss: 41.138099670410156\n",
      "CL\n",
      "Classification Model Iter 109/1000 - Loss: 41.13777160644531\n",
      "CL\n",
      "Classification Model Iter 110/1000 - Loss: 41.13782501220703\n",
      "CL\n",
      "Classification Model Iter 111/1000 - Loss: 41.13774490356445\n",
      "CL\n",
      "Classification Model Iter 112/1000 - Loss: 41.13798904418945\n",
      "CL\n",
      "Classification Model Iter 113/1000 - Loss: 41.13784408569336\n",
      "CL\n",
      "Classification Model Iter 114/1000 - Loss: 41.137603759765625\n",
      "CL\n",
      "Classification Model Iter 115/1000 - Loss: 41.13766098022461\n",
      "CL\n",
      "Classification Model Iter 116/1000 - Loss: 41.137550354003906\n",
      "CL\n",
      "Classification Model Iter 117/1000 - Loss: 41.13756561279297\n",
      "CL\n",
      "Classification Model Iter 118/1000 - Loss: 41.137691497802734\n",
      "CL\n",
      "Classification Model Iter 119/1000 - Loss: 41.13774871826172\n",
      "CL\n",
      "Classification Model Iter 120/1000 - Loss: 41.137603759765625\n",
      "CL\n",
      "Classification Model Iter 121/1000 - Loss: 41.137550354003906\n",
      "CL\n",
      "Classification Model Iter 122/1000 - Loss: 41.13751220703125\n",
      "CL\n",
      "Classification Model Iter 123/1000 - Loss: 41.13738250732422\n",
      "CL\n",
      "Classification Model Iter 124/1000 - Loss: 41.13780212402344\n",
      "CL\n",
      "Classification Model Iter 125/1000 - Loss: 41.13774490356445\n",
      "CL\n",
      "Classification Model Iter 126/1000 - Loss: 41.137489318847656\n",
      "CL\n",
      "Classification Model Iter 127/1000 - Loss: 41.137481689453125\n",
      "CL\n",
      "Classification Model Iter 128/1000 - Loss: 41.13746643066406\n",
      "CL\n",
      "Classification Model Iter 129/1000 - Loss: 41.13768768310547\n",
      "CL\n",
      "Classification Model Iter 130/1000 - Loss: 41.13747787475586\n",
      "CL\n",
      "Classification Model Iter 131/1000 - Loss: 41.1378059387207\n",
      "CL\n",
      "Classification Model Iter 132/1000 - Loss: 41.13737106323242\n",
      "CL\n",
      "Classification Model Iter 133/1000 - Loss: 41.137481689453125\n",
      "CL\n",
      "Classification Model Iter 134/1000 - Loss: 41.1374397277832\n",
      "CL\n",
      "Classification Model Iter 135/1000 - Loss: 41.137367248535156\n",
      "CL\n",
      "Classification Model Iter 136/1000 - Loss: 41.137428283691406\n",
      "CL\n",
      "Classification Model Iter 137/1000 - Loss: 41.13712692260742\n",
      "CL\n",
      "Classification Model Iter 138/1000 - Loss: 41.13742446899414\n",
      "CL\n",
      "Classification Model Iter 139/1000 - Loss: 41.137535095214844\n",
      "CL\n",
      "Classification Model Iter 140/1000 - Loss: 41.1376953125\n",
      "CL\n",
      "Classification Model Iter 141/1000 - Loss: 41.13756561279297\n",
      "CL\n",
      "Classification Model Iter 142/1000 - Loss: 41.137428283691406\n",
      "CL\n",
      "Classification Model Iter 143/1000 - Loss: 41.13739776611328\n",
      "CL\n",
      "Classification Model Iter 144/1000 - Loss: 41.13760757446289\n",
      "CL\n",
      "Classification Model Iter 145/1000 - Loss: 41.137454986572266\n",
      "CL\n",
      "Classification Model Iter 146/1000 - Loss: 41.137718200683594\n",
      "CL\n",
      "Classification Model Iter 147/1000 - Loss: 41.137420654296875\n",
      "CL\n",
      "Classification Model Iter 148/1000 - Loss: 41.13724136352539\n",
      "CL\n",
      "Classification Model Iter 149/1000 - Loss: 41.13720703125\n",
      "CL\n",
      "Classification Model Iter 150/1000 - Loss: 41.137420654296875\n",
      "CL\n",
      "Classification Model Iter 151/1000 - Loss: 41.13741683959961\n",
      "CL\n",
      "Classification Model Iter 152/1000 - Loss: 41.137550354003906\n",
      "CL\n",
      "Classification Model Iter 153/1000 - Loss: 41.137481689453125\n",
      "CL\n",
      "Classification Model Iter 154/1000 - Loss: 41.13782501220703\n",
      "CL\n",
      "Classification Model Iter 155/1000 - Loss: 41.13735580444336\n",
      "CL\n",
      "Classification Model Iter 156/1000 - Loss: 41.137332916259766\n",
      "CL\n",
      "Classification Model Iter 157/1000 - Loss: 41.13765335083008\n",
      "CL\n",
      "Classification Model Iter 158/1000 - Loss: 41.13746643066406\n",
      "CL\n",
      "Classification Model Iter 159/1000 - Loss: 41.13756561279297\n",
      "CL\n",
      "Classification Model Iter 160/1000 - Loss: 41.13743591308594\n",
      "CL\n",
      "Classification Model Iter 161/1000 - Loss: 41.13777160644531\n",
      "CL\n",
      "Classification Model Iter 162/1000 - Loss: 41.13739013671875\n",
      "CL\n",
      "Classification Model Iter 163/1000 - Loss: 41.137550354003906\n",
      "CL\n",
      "Classification Model Iter 164/1000 - Loss: 41.13772201538086\n",
      "CL\n",
      "Classification Model Iter 165/1000 - Loss: 41.1375732421875\n",
      "CL\n",
      "Classification Model Iter 166/1000 - Loss: 41.13742446899414\n",
      "CL\n",
      "Classification Model Iter 167/1000 - Loss: 41.13758087158203\n",
      "CL\n",
      "Classification Model Iter 168/1000 - Loss: 41.137699127197266\n",
      "CL\n",
      "Classification Model Iter 169/1000 - Loss: 41.13768005371094\n",
      "CL\n",
      "Classification Model Iter 170/1000 - Loss: 41.137672424316406\n",
      "CL\n",
      "Classification Model Iter 171/1000 - Loss: 41.137454986572266\n",
      "CL\n",
      "Classification Model Iter 172/1000 - Loss: 41.13731384277344\n",
      "CL\n",
      "Classification Model Iter 173/1000 - Loss: 41.13777160644531\n",
      "CL\n",
      "Classification Model Iter 174/1000 - Loss: 41.13762664794922\n",
      "CL\n",
      "Classification Model Iter 175/1000 - Loss: 41.137779235839844\n",
      "CL\n",
      "Classification Model Iter 176/1000 - Loss: 41.137577056884766\n",
      "CL\n",
      "Classification Model Iter 177/1000 - Loss: 41.13776779174805\n",
      "CL\n",
      "Classification Model Iter 178/1000 - Loss: 41.13772201538086\n",
      "CL\n",
      "Classification Model Iter 179/1000 - Loss: 41.137794494628906\n",
      "CL\n",
      "Classification Model Iter 180/1000 - Loss: 41.13764953613281\n",
      "CL\n",
      "Classification Model Iter 181/1000 - Loss: 41.137725830078125\n",
      "CL\n",
      "Classification Model Iter 182/1000 - Loss: 41.13759231567383\n",
      "CL\n",
      "Classification Model Iter 183/1000 - Loss: 41.137794494628906\n",
      "CL\n",
      "Classification Model Iter 184/1000 - Loss: 41.13762283325195\n",
      "CL\n",
      "Classification Model Iter 185/1000 - Loss: 41.13761520385742\n",
      "CL\n",
      "Classification Model Iter 186/1000 - Loss: 41.1375846862793\n",
      "CL\n",
      "Classification Model Iter 187/1000 - Loss: 41.13763427734375\n",
      "CL\n",
      "Classification Model Iter 188/1000 - Loss: 41.13752365112305\n",
      "CL\n",
      "Classification Model Iter 189/1000 - Loss: 41.13810729980469\n",
      "CL\n",
      "Classification Model Iter 190/1000 - Loss: 41.137821197509766\n",
      "CL\n",
      "Classification Model Iter 191/1000 - Loss: 41.13755798339844\n",
      "CL\n",
      "Classification Model Iter 192/1000 - Loss: 41.137733459472656\n",
      "CL\n",
      "Classification Model Iter 193/1000 - Loss: 41.13723373413086\n",
      "CL\n",
      "Classification Model Iter 194/1000 - Loss: 41.137413024902344\n",
      "CL\n",
      "Classification Model Iter 195/1000 - Loss: 41.1376838684082\n",
      "CL\n",
      "Classification Model Iter 196/1000 - Loss: 41.13756561279297\n",
      "CL\n",
      "Classification Model Iter 197/1000 - Loss: 41.13774490356445\n",
      "CL\n",
      "Classification Model Iter 198/1000 - Loss: 41.13780212402344\n",
      "CL\n",
      "Classification Model Iter 199/1000 - Loss: 41.1377067565918\n",
      "CL\n",
      "Classification Model Iter 200/1000 - Loss: 41.13777542114258\n",
      "CL\n",
      "Classification Model Iter 201/1000 - Loss: 41.13816452026367\n",
      "CL\n",
      "Classification Model Iter 202/1000 - Loss: 41.13789749145508\n",
      "CL\n",
      "Classification Model Iter 203/1000 - Loss: 41.137752532958984\n",
      "CL\n",
      "Classification Model Iter 204/1000 - Loss: 41.13776779174805\n",
      "CL\n",
      "Classification Model Iter 205/1000 - Loss: 41.13808059692383\n",
      "CL\n",
      "Classification Model Iter 206/1000 - Loss: 41.13764572143555\n",
      "CL\n",
      "Classification Model Iter 207/1000 - Loss: 41.13799285888672\n",
      "CL\n",
      "Classification Model Iter 208/1000 - Loss: 41.138214111328125\n",
      "CL\n",
      "Classification Model Iter 209/1000 - Loss: 41.13790512084961\n",
      "CL\n",
      "Classification Model Iter 210/1000 - Loss: 41.13822555541992\n",
      "CL\n",
      "Classification Model Iter 211/1000 - Loss: 41.13785171508789\n",
      "CL\n",
      "Classification Model Iter 212/1000 - Loss: 41.138145446777344\n",
      "CL\n",
      "Classification Model Iter 213/1000 - Loss: 41.13811492919922\n",
      "CL\n",
      "Classification Model Iter 214/1000 - Loss: 41.13793182373047\n",
      "CL\n",
      "Classification Model Iter 215/1000 - Loss: 41.138118743896484\n",
      "CL\n",
      "Classification Model Iter 216/1000 - Loss: 41.1381721496582\n",
      "CL\n",
      "Classification Model Iter 217/1000 - Loss: 41.13827133178711\n",
      "CL\n",
      "Classification Model Iter 218/1000 - Loss: 41.13816833496094\n",
      "CL\n",
      "Classification Model Iter 219/1000 - Loss: 41.13813781738281\n",
      "CL\n",
      "Classification Model Iter 220/1000 - Loss: 41.138431549072266\n",
      "CL\n",
      "Classification Model Iter 221/1000 - Loss: 41.138397216796875\n",
      "CL\n",
      "Classification Model Iter 222/1000 - Loss: 41.138267517089844\n",
      "CL\n",
      "Classification Model Iter 223/1000 - Loss: 41.138282775878906\n",
      "CL\n",
      "Classification Model Iter 224/1000 - Loss: 41.1385612487793\n",
      "CL\n",
      "Classification Model Iter 225/1000 - Loss: 41.13854217529297\n",
      "CL\n",
      "Classification Model Iter 226/1000 - Loss: 41.13862228393555\n",
      "CL\n",
      "Classification Model Iter 227/1000 - Loss: 41.138641357421875\n",
      "CL\n",
      "Classification Model Iter 228/1000 - Loss: 41.13871383666992\n",
      "CL\n",
      "Classification Model Iter 229/1000 - Loss: 41.138648986816406\n",
      "CL\n",
      "Classification Model Iter 230/1000 - Loss: 41.138492584228516\n",
      "CL\n",
      "Classification Model Iter 231/1000 - Loss: 41.1390380859375\n",
      "CL\n",
      "Classification Model Iter 232/1000 - Loss: 41.13878631591797\n",
      "CL\n",
      "Classification Model Iter 233/1000 - Loss: 41.13884735107422\n",
      "CL\n",
      "Classification Model Iter 234/1000 - Loss: 41.139041900634766\n",
      "CL\n",
      "Classification Model Iter 235/1000 - Loss: 41.139041900634766\n",
      "CL\n",
      "Classification Model Iter 236/1000 - Loss: 41.138954162597656\n",
      "CL\n",
      "Classification Model Iter 237/1000 - Loss: 41.139137268066406\n",
      "CL\n",
      "Classification Model Iter 238/1000 - Loss: 41.13918685913086\n",
      "CL\n",
      "Classification Model Iter 239/1000 - Loss: 41.138916015625\n",
      "CL\n",
      "Classification Model Iter 240/1000 - Loss: 41.139381408691406\n",
      "CL\n",
      "Classification Model Iter 241/1000 - Loss: 41.13980484008789\n",
      "CL\n",
      "Classification Model Iter 242/1000 - Loss: 41.13981246948242\n",
      "CL\n",
      "Classification Model Iter 243/1000 - Loss: 41.1395149230957\n",
      "CL\n",
      "Classification Model Iter 244/1000 - Loss: 41.13947677612305\n",
      "CL\n",
      "Classification Model Iter 245/1000 - Loss: 41.13935470581055\n",
      "CL\n",
      "Classification Model Iter 246/1000 - Loss: 41.13962936401367\n",
      "CL\n",
      "Classification Model Iter 247/1000 - Loss: 41.139747619628906\n",
      "CL\n",
      "Classification Model Iter 248/1000 - Loss: 41.139862060546875\n",
      "CL\n",
      "Classification Model Iter 249/1000 - Loss: 41.139434814453125\n",
      "CL\n",
      "Classification Model Iter 250/1000 - Loss: 41.139381408691406\n",
      "CL\n",
      "Classification Model Iter 251/1000 - Loss: 41.139312744140625\n",
      "CL\n",
      "Classification Model Iter 252/1000 - Loss: 41.139495849609375\n",
      "CL\n",
      "Classification Model Iter 253/1000 - Loss: 41.13919448852539\n",
      "CL\n",
      "Classification Model Iter 254/1000 - Loss: 41.139713287353516\n",
      "CL\n",
      "Classification Model Iter 255/1000 - Loss: 41.13966751098633\n",
      "CL\n",
      "Classification Model Iter 256/1000 - Loss: 41.13949966430664\n",
      "CL\n",
      "Classification Model Iter 257/1000 - Loss: 41.13975143432617\n",
      "CL\n",
      "Classification Model Iter 258/1000 - Loss: 41.139625549316406\n",
      "CL\n",
      "Classification Model Iter 259/1000 - Loss: 41.1397705078125\n",
      "CL\n",
      "Classification Model Iter 260/1000 - Loss: 41.13985061645508\n",
      "CL\n",
      "Classification Model Iter 261/1000 - Loss: 41.13954544067383\n",
      "CL\n",
      "Classification Model Iter 262/1000 - Loss: 41.13991928100586\n",
      "CL\n",
      "Classification Model Iter 263/1000 - Loss: 41.13963317871094\n",
      "CL\n",
      "Classification Model Iter 264/1000 - Loss: 41.13987350463867\n",
      "CL\n",
      "Classification Model Iter 265/1000 - Loss: 41.139434814453125\n",
      "CL\n",
      "Classification Model Iter 266/1000 - Loss: 41.139686584472656\n",
      "CL\n",
      "Classification Model Iter 267/1000 - Loss: 41.13959503173828\n",
      "CL\n",
      "Classification Model Iter 268/1000 - Loss: 41.139705657958984\n",
      "CL\n",
      "Classification Model Iter 269/1000 - Loss: 41.13949966430664\n",
      "CL\n",
      "Classification Model Iter 270/1000 - Loss: 41.13983154296875\n",
      "CL\n",
      "Classification Model Iter 271/1000 - Loss: 41.13954162597656\n",
      "CL\n",
      "Classification Model Iter 272/1000 - Loss: 41.1395149230957\n",
      "CL\n",
      "Classification Model Iter 273/1000 - Loss: 41.13945770263672\n",
      "CL\n",
      "Classification Model Iter 274/1000 - Loss: 41.13941955566406\n",
      "CL\n",
      "Classification Model Iter 275/1000 - Loss: 41.1392936706543\n",
      "CL\n",
      "Classification Model Iter 276/1000 - Loss: 41.139163970947266\n",
      "CL\n",
      "Classification Model Iter 277/1000 - Loss: 41.13915252685547\n",
      "CL\n",
      "Classification Model Iter 278/1000 - Loss: 41.139102935791016\n",
      "CL\n",
      "Classification Model Iter 279/1000 - Loss: 41.139007568359375\n",
      "CL\n",
      "Classification Model Iter 280/1000 - Loss: 41.13939666748047\n",
      "CL\n",
      "Classification Model Iter 281/1000 - Loss: 41.139122009277344\n",
      "CL\n",
      "Classification Model Iter 282/1000 - Loss: 41.138816833496094\n",
      "CL\n",
      "Classification Model Iter 283/1000 - Loss: 41.13863754272461\n",
      "CL\n",
      "Classification Model Iter 284/1000 - Loss: 41.13864517211914\n",
      "CL\n",
      "Classification Model Iter 285/1000 - Loss: 41.13895797729492\n",
      "CL\n",
      "Classification Model Iter 286/1000 - Loss: 41.13866424560547\n",
      "CL\n",
      "Classification Model Iter 287/1000 - Loss: 41.13864517211914\n",
      "CL\n",
      "Classification Model Iter 288/1000 - Loss: 41.13881301879883\n",
      "CL\n",
      "Classification Model Iter 289/1000 - Loss: 41.138919830322266\n",
      "CL\n",
      "Classification Model Iter 290/1000 - Loss: 41.13865280151367\n",
      "CL\n",
      "Classification Model Iter 291/1000 - Loss: 41.13890075683594\n",
      "CL\n",
      "Classification Model Iter 292/1000 - Loss: 41.138938903808594\n",
      "CL\n",
      "Classification Model Iter 293/1000 - Loss: 41.138858795166016\n",
      "CL\n",
      "Classification Model Iter 294/1000 - Loss: 41.1389274597168\n",
      "CL\n",
      "Classification Model Iter 295/1000 - Loss: 41.13897705078125\n",
      "CL\n",
      "Classification Model Iter 296/1000 - Loss: 41.13899230957031\n",
      "CL\n",
      "Classification Model Iter 297/1000 - Loss: 41.13908004760742\n",
      "CL\n",
      "Classification Model Iter 298/1000 - Loss: 41.138607025146484\n",
      "CL\n",
      "Classification Model Iter 299/1000 - Loss: 41.138607025146484\n",
      "CL\n",
      "Classification Model Iter 300/1000 - Loss: 41.13880157470703\n",
      "CL\n",
      "Classification Model Iter 301/1000 - Loss: 41.13860321044922\n",
      "CL\n",
      "Classification Model Iter 302/1000 - Loss: 41.13836669921875\n",
      "CL\n",
      "Classification Model Iter 303/1000 - Loss: 41.13859176635742\n",
      "CL\n",
      "Classification Model Iter 304/1000 - Loss: 41.13842010498047\n",
      "CL\n",
      "Classification Model Iter 305/1000 - Loss: 41.138343811035156\n",
      "CL\n",
      "Classification Model Iter 306/1000 - Loss: 41.137962341308594\n",
      "CL\n",
      "Classification Model Iter 307/1000 - Loss: 41.138370513916016\n",
      "CL\n",
      "Classification Model Iter 308/1000 - Loss: 41.13854217529297\n",
      "CL\n",
      "Classification Model Iter 309/1000 - Loss: 41.138465881347656\n",
      "CL\n",
      "Classification Model Iter 310/1000 - Loss: 41.13826370239258\n",
      "CL\n",
      "Classification Model Iter 311/1000 - Loss: 41.13848114013672\n",
      "CL\n",
      "Classification Model Iter 312/1000 - Loss: 41.138553619384766\n",
      "CL\n",
      "Classification Model Iter 313/1000 - Loss: 41.13880157470703\n",
      "CL\n",
      "Classification Model Iter 314/1000 - Loss: 41.13840866088867\n",
      "CL\n",
      "Classification Model Iter 315/1000 - Loss: 41.138553619384766\n",
      "CL\n",
      "Classification Model Iter 316/1000 - Loss: 41.138858795166016\n",
      "CL\n",
      "Classification Model Iter 317/1000 - Loss: 41.138545989990234\n",
      "CL\n",
      "Classification Model Iter 318/1000 - Loss: 41.13862609863281\n",
      "CL\n",
      "Classification Model Iter 319/1000 - Loss: 41.13871383666992\n",
      "CL\n",
      "Classification Model Iter 320/1000 - Loss: 41.138946533203125\n",
      "CL\n",
      "Classification Model Iter 321/1000 - Loss: 41.138893127441406\n",
      "CL\n",
      "Classification Model Iter 322/1000 - Loss: 41.13914108276367\n",
      "CL\n",
      "Classification Model Iter 323/1000 - Loss: 41.13917541503906\n",
      "CL\n",
      "Classification Model Iter 324/1000 - Loss: 41.13930130004883\n",
      "CL\n",
      "Classification Model Iter 325/1000 - Loss: 41.138877868652344\n",
      "CL\n",
      "Classification Model Iter 326/1000 - Loss: 41.13861846923828\n",
      "CL\n",
      "Classification Model Iter 327/1000 - Loss: 41.13880920410156\n",
      "CL\n",
      "Classification Model Iter 328/1000 - Loss: 41.13911437988281\n",
      "CL\n",
      "Classification Model Iter 329/1000 - Loss: 41.13899612426758\n",
      "CL\n",
      "Classification Model Iter 330/1000 - Loss: 41.139122009277344\n",
      "CL\n",
      "Classification Model Iter 331/1000 - Loss: 41.13880920410156\n",
      "CL\n",
      "Classification Model Iter 332/1000 - Loss: 41.13899612426758\n",
      "CL\n",
      "Classification Model Iter 333/1000 - Loss: 41.139060974121094\n",
      "CL\n",
      "Classification Model Iter 334/1000 - Loss: 41.13932418823242\n",
      "CL\n",
      "Classification Model Iter 335/1000 - Loss: 41.139183044433594\n",
      "CL\n",
      "Classification Model Iter 336/1000 - Loss: 41.139041900634766\n",
      "CL\n",
      "Classification Model Iter 337/1000 - Loss: 41.13902282714844\n",
      "CL\n",
      "Classification Model Iter 338/1000 - Loss: 41.13914108276367\n",
      "CL\n",
      "Classification Model Iter 339/1000 - Loss: 41.139095306396484\n",
      "CL\n",
      "Classification Model Iter 340/1000 - Loss: 41.13874053955078\n",
      "CL\n",
      "Classification Model Iter 341/1000 - Loss: 41.13911437988281\n",
      "CL\n",
      "Classification Model Iter 342/1000 - Loss: 41.13917541503906\n",
      "CL\n",
      "Classification Model Iter 343/1000 - Loss: 41.13898468017578\n",
      "CL\n",
      "Classification Model Iter 344/1000 - Loss: 41.13861846923828\n",
      "CL\n",
      "Classification Model Iter 345/1000 - Loss: 41.138736724853516\n",
      "CL\n",
      "Classification Model Iter 346/1000 - Loss: 41.13896560668945\n",
      "CL\n",
      "Classification Model Iter 347/1000 - Loss: 41.13840866088867\n",
      "CL\n",
      "Classification Model Iter 348/1000 - Loss: 41.13864517211914\n",
      "CL\n",
      "Classification Model Iter 349/1000 - Loss: 41.138580322265625\n",
      "CL\n",
      "Classification Model Iter 350/1000 - Loss: 41.139259338378906\n",
      "CL\n",
      "Classification Model Iter 351/1000 - Loss: 41.138916015625\n",
      "CL\n",
      "Classification Model Iter 352/1000 - Loss: 41.139163970947266\n",
      "CL\n",
      "Classification Model Iter 353/1000 - Loss: 41.13907241821289\n",
      "CL\n",
      "Classification Model Iter 354/1000 - Loss: 41.139095306396484\n",
      "CL\n",
      "Classification Model Iter 355/1000 - Loss: 41.1392936706543\n",
      "CL\n",
      "Classification Model Iter 356/1000 - Loss: 41.13933181762695\n",
      "CL\n",
      "Classification Model Iter 357/1000 - Loss: 41.13942337036133\n",
      "CL\n",
      "Classification Model Iter 358/1000 - Loss: 41.13962936401367\n",
      "CL\n",
      "Classification Model Iter 359/1000 - Loss: 41.139495849609375\n",
      "CL\n",
      "Classification Model Iter 360/1000 - Loss: 41.13924789428711\n",
      "CL\n",
      "Classification Model Iter 361/1000 - Loss: 41.13894271850586\n",
      "CL\n",
      "Classification Model Iter 362/1000 - Loss: 41.13914108276367\n",
      "CL\n",
      "Classification Model Iter 363/1000 - Loss: 41.13901138305664\n",
      "CL\n",
      "Classification Model Iter 364/1000 - Loss: 41.139408111572266\n",
      "CL\n",
      "Classification Model Iter 365/1000 - Loss: 41.1390266418457\n",
      "CL\n",
      "Classification Model Iter 366/1000 - Loss: 41.13887023925781\n",
      "CL\n",
      "Classification Model Iter 367/1000 - Loss: 41.13943099975586\n",
      "CL\n",
      "Classification Model Iter 368/1000 - Loss: 41.1392936706543\n",
      "CL\n",
      "Classification Model Iter 369/1000 - Loss: 41.1392707824707\n",
      "CL\n",
      "Classification Model Iter 370/1000 - Loss: 41.1391487121582\n",
      "CL\n",
      "Classification Model Iter 371/1000 - Loss: 41.13931655883789\n",
      "CL\n",
      "Classification Model Iter 372/1000 - Loss: 41.13943862915039\n",
      "CL\n",
      "Classification Model Iter 373/1000 - Loss: 41.13936233520508\n",
      "CL\n",
      "Classification Model Iter 374/1000 - Loss: 41.13985824584961\n",
      "CL\n",
      "Classification Model Iter 375/1000 - Loss: 41.14004135131836\n",
      "CL\n",
      "Classification Model Iter 376/1000 - Loss: 41.140045166015625\n",
      "CL\n",
      "Classification Model Iter 377/1000 - Loss: 41.14028549194336\n",
      "CL\n",
      "Classification Model Iter 378/1000 - Loss: 41.14065170288086\n",
      "CL\n",
      "Classification Model Iter 379/1000 - Loss: 41.14127731323242\n",
      "CL\n",
      "Classification Model Iter 380/1000 - Loss: 41.14091873168945\n",
      "CL\n",
      "Classification Model Iter 381/1000 - Loss: 41.14112091064453\n",
      "CL\n",
      "Classification Model Iter 382/1000 - Loss: 41.1408576965332\n",
      "CL\n",
      "Classification Model Iter 383/1000 - Loss: 41.140869140625\n",
      "CL\n",
      "Classification Model Iter 384/1000 - Loss: 41.140869140625\n",
      "CL\n",
      "Classification Model Iter 385/1000 - Loss: 41.14140701293945\n",
      "CL\n",
      "Classification Model Iter 386/1000 - Loss: 41.14094924926758\n",
      "CL\n",
      "Classification Model Iter 387/1000 - Loss: 41.14085006713867\n",
      "CL\n",
      "Classification Model Iter 388/1000 - Loss: 41.14030456542969\n",
      "CL\n",
      "Classification Model Iter 389/1000 - Loss: 41.1400146484375\n",
      "CL\n",
      "Classification Model Iter 390/1000 - Loss: 41.14030838012695\n",
      "CL\n",
      "Classification Model Iter 391/1000 - Loss: 41.14032745361328\n",
      "CL\n",
      "Classification Model Iter 392/1000 - Loss: 41.14002990722656\n",
      "CL\n",
      "Classification Model Iter 393/1000 - Loss: 41.1396369934082\n",
      "CL\n",
      "Classification Model Iter 394/1000 - Loss: 41.14014434814453\n",
      "CL\n",
      "Classification Model Iter 395/1000 - Loss: 41.13993453979492\n",
      "CL\n",
      "Classification Model Iter 396/1000 - Loss: 41.140018463134766\n",
      "CL\n",
      "Classification Model Iter 397/1000 - Loss: 41.140079498291016\n",
      "CL\n",
      "Classification Model Iter 398/1000 - Loss: 41.1401252746582\n",
      "CL\n",
      "Classification Model Iter 399/1000 - Loss: 41.14049530029297\n",
      "CL\n",
      "Classification Model Iter 400/1000 - Loss: 41.14017105102539\n",
      "CL\n",
      "Classification Model Iter 401/1000 - Loss: 41.140419006347656\n",
      "CL\n",
      "Classification Model Iter 402/1000 - Loss: 41.140541076660156\n",
      "CL\n",
      "Classification Model Iter 403/1000 - Loss: 41.14087677001953\n",
      "CL\n",
      "Classification Model Iter 404/1000 - Loss: 41.14094543457031\n",
      "CL\n",
      "Classification Model Iter 405/1000 - Loss: 41.14116287231445\n",
      "CL\n",
      "Classification Model Iter 406/1000 - Loss: 41.14169692993164\n",
      "CL\n",
      "Classification Model Iter 407/1000 - Loss: 41.14201736450195\n",
      "CL\n",
      "Classification Model Iter 408/1000 - Loss: 41.142478942871094\n",
      "CL\n",
      "Classification Model Iter 409/1000 - Loss: 41.142555236816406\n",
      "CL\n",
      "Classification Model Iter 410/1000 - Loss: 41.14253616333008\n",
      "CL\n",
      "Classification Model Iter 411/1000 - Loss: 41.142826080322266\n",
      "CL\n",
      "Classification Model Iter 412/1000 - Loss: 41.14267349243164\n",
      "CL\n",
      "Classification Model Iter 413/1000 - Loss: 41.142513275146484\n",
      "CL\n",
      "Classification Model Iter 414/1000 - Loss: 41.14297866821289\n",
      "CL\n",
      "Classification Model Iter 415/1000 - Loss: 41.142425537109375\n",
      "CL\n",
      "Classification Model Iter 416/1000 - Loss: 41.14190673828125\n",
      "CL\n",
      "Classification Model Iter 417/1000 - Loss: 41.142295837402344\n",
      "CL\n",
      "Classification Model Iter 418/1000 - Loss: 41.142189025878906\n",
      "CL\n",
      "Classification Model Iter 419/1000 - Loss: 41.141998291015625\n",
      "CL\n",
      "Classification Model Iter 420/1000 - Loss: 41.141937255859375\n",
      "CL\n",
      "Classification Model Iter 421/1000 - Loss: 41.142242431640625\n",
      "CL\n",
      "Classification Model Iter 422/1000 - Loss: 41.1422004699707\n",
      "CL\n",
      "Classification Model Iter 423/1000 - Loss: 41.142059326171875\n",
      "CL\n",
      "Classification Model Iter 424/1000 - Loss: 41.14183807373047\n",
      "CL\n",
      "Classification Model Iter 425/1000 - Loss: 41.14220428466797\n",
      "CL\n",
      "Classification Model Iter 426/1000 - Loss: 41.14207077026367\n",
      "CL\n",
      "Classification Model Iter 427/1000 - Loss: 41.141963958740234\n",
      "CL\n",
      "Classification Model Iter 428/1000 - Loss: 41.141746520996094\n",
      "CL\n",
      "Classification Model Iter 429/1000 - Loss: 41.1422233581543\n",
      "CL\n",
      "Classification Model Iter 430/1000 - Loss: 41.14251708984375\n",
      "CL\n",
      "Classification Model Iter 431/1000 - Loss: 41.14298629760742\n",
      "CL\n",
      "Classification Model Iter 432/1000 - Loss: 41.14274215698242\n",
      "CL\n",
      "Classification Model Iter 433/1000 - Loss: 41.14305114746094\n",
      "CL\n",
      "Classification Model Iter 434/1000 - Loss: 41.14277648925781\n",
      "CL\n",
      "Classification Model Iter 435/1000 - Loss: 41.142765045166016\n",
      "CL\n",
      "Classification Model Iter 436/1000 - Loss: 41.142791748046875\n",
      "CL\n",
      "Classification Model Iter 437/1000 - Loss: 41.14311218261719\n",
      "CL\n",
      "Classification Model Iter 438/1000 - Loss: 41.143287658691406\n",
      "CL\n",
      "Classification Model Iter 439/1000 - Loss: 41.14362335205078\n",
      "CL\n",
      "Classification Model Iter 440/1000 - Loss: 41.14379119873047\n",
      "CL\n",
      "Classification Model Iter 441/1000 - Loss: 41.143917083740234\n",
      "CL\n",
      "Classification Model Iter 442/1000 - Loss: 41.143524169921875\n",
      "CL\n",
      "Classification Model Iter 443/1000 - Loss: 41.143226623535156\n",
      "CL\n",
      "Classification Model Iter 444/1000 - Loss: 41.14338302612305\n",
      "CL\n",
      "Classification Model Iter 445/1000 - Loss: 41.143394470214844\n",
      "CL\n",
      "Classification Model Iter 446/1000 - Loss: 41.14351272583008\n",
      "CL\n",
      "Classification Model Iter 447/1000 - Loss: 41.143333435058594\n",
      "CL\n",
      "Classification Model Iter 448/1000 - Loss: 41.1429328918457\n",
      "CL\n",
      "Classification Model Iter 449/1000 - Loss: 41.14323425292969\n",
      "CL\n",
      "Classification Model Iter 450/1000 - Loss: 41.14351272583008\n",
      "CL\n",
      "Classification Model Iter 451/1000 - Loss: 41.14357376098633\n",
      "CL\n",
      "Classification Model Iter 452/1000 - Loss: 41.143898010253906\n",
      "CL\n",
      "Classification Model Iter 453/1000 - Loss: 41.14456558227539\n",
      "CL\n",
      "Classification Model Iter 454/1000 - Loss: 41.145050048828125\n",
      "CL\n",
      "Classification Model Iter 455/1000 - Loss: 41.14479064941406\n",
      "CL\n",
      "Classification Model Iter 456/1000 - Loss: 41.144752502441406\n",
      "CL\n",
      "Classification Model Iter 457/1000 - Loss: 41.14455795288086\n",
      "CL\n",
      "Classification Model Iter 458/1000 - Loss: 41.14462661743164\n",
      "CL\n",
      "Classification Model Iter 459/1000 - Loss: 41.144508361816406\n",
      "CL\n",
      "Classification Model Iter 460/1000 - Loss: 41.14405822753906\n",
      "CL\n",
      "Classification Model Iter 461/1000 - Loss: 41.14356994628906\n",
      "CL\n",
      "Classification Model Iter 462/1000 - Loss: 41.1431770324707\n",
      "CL\n",
      "Classification Model Iter 463/1000 - Loss: 41.14337158203125\n",
      "CL\n",
      "Classification Model Iter 464/1000 - Loss: 41.143123626708984\n",
      "CL\n",
      "Classification Model Iter 465/1000 - Loss: 41.143089294433594\n",
      "CL\n",
      "Classification Model Iter 466/1000 - Loss: 41.14265823364258\n",
      "CL\n",
      "Classification Model Iter 467/1000 - Loss: 41.14276885986328\n",
      "CL\n",
      "Classification Model Iter 468/1000 - Loss: 41.142826080322266\n",
      "CL\n",
      "Classification Model Iter 469/1000 - Loss: 41.142765045166016\n",
      "CL\n",
      "Classification Model Iter 470/1000 - Loss: 41.14283752441406\n",
      "CL\n",
      "Classification Model Iter 471/1000 - Loss: 41.14274215698242\n",
      "CL\n",
      "Classification Model Iter 472/1000 - Loss: 41.14244842529297\n",
      "CL\n",
      "Classification Model Iter 473/1000 - Loss: 41.142581939697266\n",
      "CL\n",
      "Classification Model Iter 474/1000 - Loss: 41.142696380615234\n",
      "CL\n",
      "Classification Model Iter 475/1000 - Loss: 41.142601013183594\n",
      "CL\n",
      "Classification Model Iter 476/1000 - Loss: 41.142520904541016\n",
      "CL\n",
      "Classification Model Iter 477/1000 - Loss: 41.14189910888672\n",
      "CL\n",
      "Classification Model Iter 478/1000 - Loss: 41.14174270629883\n",
      "CL\n",
      "Classification Model Iter 479/1000 - Loss: 41.14229965209961\n",
      "CL\n",
      "Classification Model Iter 480/1000 - Loss: 41.14234924316406\n",
      "CL\n",
      "Classification Model Iter 481/1000 - Loss: 41.14170837402344\n",
      "CL\n",
      "Classification Model Iter 482/1000 - Loss: 41.142005920410156\n",
      "CL\n",
      "Classification Model Iter 483/1000 - Loss: 41.14182662963867\n",
      "CL\n",
      "Classification Model Iter 484/1000 - Loss: 41.141963958740234\n",
      "CL\n",
      "Classification Model Iter 485/1000 - Loss: 41.14179992675781\n",
      "CL\n",
      "Classification Model Iter 486/1000 - Loss: 41.14194107055664\n",
      "CL\n",
      "Classification Model Iter 487/1000 - Loss: 41.14207077026367\n",
      "CL\n",
      "Classification Model Iter 488/1000 - Loss: 41.14231872558594\n",
      "CL\n",
      "Classification Model Iter 489/1000 - Loss: 41.142539978027344\n",
      "CL\n",
      "Classification Model Iter 490/1000 - Loss: 41.14236068725586\n",
      "CL\n",
      "Classification Model Iter 491/1000 - Loss: 41.14275360107422\n",
      "CL\n",
      "Classification Model Iter 492/1000 - Loss: 41.14276123046875\n",
      "CL\n",
      "Classification Model Iter 493/1000 - Loss: 41.142799377441406\n",
      "CL\n",
      "Classification Model Iter 494/1000 - Loss: 41.14257049560547\n",
      "CL\n",
      "Classification Model Iter 495/1000 - Loss: 41.14284896850586\n",
      "CL\n",
      "Classification Model Iter 496/1000 - Loss: 41.142852783203125\n",
      "CL\n",
      "Classification Model Iter 497/1000 - Loss: 41.143497467041016\n",
      "CL\n",
      "Classification Model Iter 498/1000 - Loss: 41.14373779296875\n",
      "CL\n",
      "Classification Model Iter 499/1000 - Loss: 41.143428802490234\n",
      "CL\n",
      "Classification Model Iter 500/1000 - Loss: 41.14295959472656\n",
      "CL\n",
      "Classification Model Iter 501/1000 - Loss: 41.143218994140625\n",
      "CL\n",
      "Classification Model Iter 502/1000 - Loss: 41.143157958984375\n",
      "CL\n",
      "Classification Model Iter 503/1000 - Loss: 41.143306732177734\n",
      "CL\n",
      "Classification Model Iter 504/1000 - Loss: 41.14292526245117\n",
      "CL\n",
      "Classification Model Iter 505/1000 - Loss: 41.142948150634766\n",
      "CL\n",
      "Classification Model Iter 506/1000 - Loss: 41.142967224121094\n",
      "CL\n",
      "Classification Model Iter 507/1000 - Loss: 41.14268493652344\n",
      "CL\n",
      "Classification Model Iter 508/1000 - Loss: 41.14252853393555\n",
      "CL\n",
      "Classification Model Iter 509/1000 - Loss: 41.14266586303711\n",
      "CL\n",
      "Classification Model Iter 510/1000 - Loss: 41.14267349243164\n",
      "CL\n",
      "Classification Model Iter 511/1000 - Loss: 41.14265060424805\n",
      "CL\n",
      "Classification Model Iter 512/1000 - Loss: 41.14295196533203\n",
      "CL\n",
      "Classification Model Iter 513/1000 - Loss: 41.14244079589844\n",
      "CL\n",
      "Classification Model Iter 514/1000 - Loss: 41.1425666809082\n",
      "CL\n",
      "Classification Model Iter 515/1000 - Loss: 41.14268493652344\n",
      "CL\n",
      "Classification Model Iter 516/1000 - Loss: 41.14271926879883\n",
      "CL\n",
      "Classification Model Iter 517/1000 - Loss: 41.142295837402344\n",
      "CL\n",
      "Classification Model Iter 518/1000 - Loss: 41.14205551147461\n",
      "CL\n",
      "Classification Model Iter 519/1000 - Loss: 41.142208099365234\n",
      "CL\n",
      "Classification Model Iter 520/1000 - Loss: 41.141902923583984\n",
      "CL\n",
      "Classification Model Iter 521/1000 - Loss: 41.1423225402832\n",
      "CL\n",
      "Classification Model Iter 522/1000 - Loss: 41.1425895690918\n",
      "CL\n",
      "Classification Model Iter 523/1000 - Loss: 41.142333984375\n",
      "CL\n",
      "Classification Model Iter 524/1000 - Loss: 41.142295837402344\n",
      "CL\n",
      "Classification Model Iter 525/1000 - Loss: 41.142608642578125\n",
      "CL\n",
      "Classification Model Iter 526/1000 - Loss: 41.1427001953125\n",
      "CL\n",
      "Classification Model Iter 527/1000 - Loss: 41.14313507080078\n",
      "CL\n",
      "Classification Model Iter 528/1000 - Loss: 41.14299774169922\n",
      "CL\n",
      "Classification Model Iter 529/1000 - Loss: 41.142921447753906\n",
      "CL\n",
      "Classification Model Iter 530/1000 - Loss: 41.14286422729492\n",
      "CL\n",
      "Classification Model Iter 531/1000 - Loss: 41.14303207397461\n",
      "CL\n",
      "Classification Model Iter 532/1000 - Loss: 41.142860412597656\n",
      "CL\n",
      "Classification Model Iter 533/1000 - Loss: 41.142520904541016\n",
      "CL\n",
      "Classification Model Iter 534/1000 - Loss: 41.1420783996582\n",
      "CL\n",
      "Classification Model Iter 535/1000 - Loss: 41.14213562011719\n",
      "CL\n",
      "Classification Model Iter 536/1000 - Loss: 41.14189910888672\n",
      "CL\n",
      "Classification Model Iter 537/1000 - Loss: 41.14159393310547\n",
      "CL\n",
      "Classification Model Iter 538/1000 - Loss: 41.141441345214844\n",
      "CL\n",
      "Classification Model Iter 539/1000 - Loss: 41.14165115356445\n",
      "CL\n",
      "Classification Model Iter 540/1000 - Loss: 41.141536712646484\n",
      "CL\n",
      "Classification Model Iter 541/1000 - Loss: 41.14149856567383\n",
      "CL\n",
      "Classification Model Iter 542/1000 - Loss: 41.141273498535156\n",
      "CL\n",
      "Classification Model Iter 543/1000 - Loss: 41.14146423339844\n",
      "CL\n",
      "Classification Model Iter 544/1000 - Loss: 41.14143371582031\n",
      "CL\n",
      "Classification Model Iter 545/1000 - Loss: 41.14174270629883\n",
      "CL\n",
      "Classification Model Iter 546/1000 - Loss: 41.14155960083008\n",
      "CL\n",
      "Classification Model Iter 547/1000 - Loss: 41.14175796508789\n",
      "CL\n",
      "Classification Model Iter 548/1000 - Loss: 41.141685485839844\n",
      "CL\n",
      "Classification Model Iter 549/1000 - Loss: 41.14155578613281\n",
      "CL\n",
      "Classification Model Iter 550/1000 - Loss: 41.14137268066406\n",
      "CL\n",
      "Classification Model Iter 551/1000 - Loss: 41.141265869140625\n",
      "CL\n",
      "Classification Model Iter 552/1000 - Loss: 41.14162826538086\n",
      "CL\n",
      "Classification Model Iter 553/1000 - Loss: 41.14155197143555\n",
      "CL\n",
      "Classification Model Iter 554/1000 - Loss: 41.141353607177734\n",
      "CL\n",
      "Classification Model Iter 555/1000 - Loss: 41.14139175415039\n",
      "CL\n",
      "Classification Model Iter 556/1000 - Loss: 41.14161682128906\n",
      "CL\n",
      "Classification Model Iter 557/1000 - Loss: 41.141666412353516\n",
      "CL\n",
      "Classification Model Iter 558/1000 - Loss: 41.14219665527344\n",
      "CL\n",
      "Classification Model Iter 559/1000 - Loss: 41.142520904541016\n",
      "CL\n",
      "Classification Model Iter 560/1000 - Loss: 41.143062591552734\n",
      "CL\n",
      "Classification Model Iter 561/1000 - Loss: 41.143348693847656\n",
      "CL\n",
      "Classification Model Iter 562/1000 - Loss: 41.14332962036133\n",
      "CL\n",
      "Classification Model Iter 563/1000 - Loss: 41.14328384399414\n",
      "CL\n",
      "Classification Model Iter 564/1000 - Loss: 41.14334487915039\n",
      "CL\n",
      "Classification Model Iter 565/1000 - Loss: 41.143310546875\n",
      "CL\n",
      "Classification Model Iter 566/1000 - Loss: 41.14318084716797\n",
      "CL\n",
      "Classification Model Iter 567/1000 - Loss: 41.143280029296875\n",
      "CL\n",
      "Classification Model Iter 568/1000 - Loss: 41.1428108215332\n",
      "CL\n",
      "Classification Model Iter 569/1000 - Loss: 41.14280319213867\n",
      "CL\n",
      "Classification Model Iter 570/1000 - Loss: 41.14262390136719\n",
      "CL\n",
      "Classification Model Iter 571/1000 - Loss: 41.14249801635742\n",
      "CL\n",
      "Classification Model Iter 572/1000 - Loss: 41.1427001953125\n",
      "CL\n",
      "Classification Model Iter 573/1000 - Loss: 41.14267349243164\n",
      "CL\n",
      "Classification Model Iter 574/1000 - Loss: 41.14289093017578\n",
      "CL\n",
      "Classification Model Iter 575/1000 - Loss: 41.142642974853516\n",
      "CL\n",
      "Classification Model Iter 576/1000 - Loss: 41.14257049560547\n",
      "CL\n",
      "Classification Model Iter 577/1000 - Loss: 41.142555236816406\n",
      "CL\n",
      "Classification Model Iter 578/1000 - Loss: 41.14238357543945\n",
      "CL\n",
      "Classification Model Iter 579/1000 - Loss: 41.14220428466797\n",
      "CL\n",
      "Classification Model Iter 580/1000 - Loss: 41.14256286621094\n",
      "CL\n",
      "Classification Model Iter 581/1000 - Loss: 41.14258575439453\n",
      "CL\n",
      "Classification Model Iter 582/1000 - Loss: 41.14286422729492\n",
      "CL\n",
      "Classification Model Iter 583/1000 - Loss: 41.142738342285156\n",
      "CL\n",
      "Classification Model Iter 584/1000 - Loss: 41.14257049560547\n",
      "CL\n",
      "Classification Model Iter 585/1000 - Loss: 41.142677307128906\n",
      "CL\n",
      "Classification Model Iter 586/1000 - Loss: 41.1423454284668\n",
      "CL\n",
      "Classification Model Iter 587/1000 - Loss: 41.14249801635742\n",
      "CL\n",
      "Classification Model Iter 588/1000 - Loss: 41.142093658447266\n",
      "CL\n",
      "Classification Model Iter 589/1000 - Loss: 41.142398834228516\n",
      "CL\n",
      "Classification Model Iter 590/1000 - Loss: 41.142723083496094\n",
      "CL\n",
      "Classification Model Iter 591/1000 - Loss: 41.14256286621094\n",
      "CL\n",
      "Classification Model Iter 592/1000 - Loss: 41.142635345458984\n",
      "CL\n",
      "Classification Model Iter 593/1000 - Loss: 41.14268493652344\n",
      "CL\n",
      "Classification Model Iter 594/1000 - Loss: 41.143165588378906\n",
      "CL\n",
      "Classification Model Iter 595/1000 - Loss: 41.143028259277344\n",
      "CL\n",
      "Classification Model Iter 596/1000 - Loss: 41.143089294433594\n",
      "CL\n",
      "Classification Model Iter 597/1000 - Loss: 41.14322280883789\n",
      "CL\n",
      "Classification Model Iter 598/1000 - Loss: 41.142974853515625\n",
      "CL\n",
      "Classification Model Iter 599/1000 - Loss: 41.142940521240234\n",
      "CL\n",
      "Classification Model Iter 600/1000 - Loss: 41.143157958984375\n",
      "CL\n",
      "Classification Model Iter 601/1000 - Loss: 41.14280700683594\n",
      "CL\n",
      "Classification Model Iter 602/1000 - Loss: 41.14272689819336\n",
      "CL\n",
      "Classification Model Iter 603/1000 - Loss: 41.14279556274414\n",
      "CL\n",
      "Classification Model Iter 604/1000 - Loss: 41.142913818359375\n",
      "CL\n",
      "Classification Model Iter 605/1000 - Loss: 41.142539978027344\n",
      "CL\n",
      "Classification Model Iter 606/1000 - Loss: 41.142852783203125\n",
      "CL\n",
      "Classification Model Iter 607/1000 - Loss: 41.14302444458008\n",
      "CL\n",
      "Classification Model Iter 608/1000 - Loss: 41.143104553222656\n",
      "CL\n",
      "Classification Model Iter 609/1000 - Loss: 41.143653869628906\n",
      "CL\n",
      "Classification Model Iter 610/1000 - Loss: 41.1441650390625\n",
      "CL\n",
      "Classification Model Iter 611/1000 - Loss: 41.14390563964844\n",
      "CL\n",
      "Classification Model Iter 612/1000 - Loss: 41.143680572509766\n",
      "CL\n",
      "Classification Model Iter 613/1000 - Loss: 41.14408874511719\n",
      "CL\n",
      "Classification Model Iter 614/1000 - Loss: 41.143890380859375\n",
      "CL\n",
      "Classification Model Iter 615/1000 - Loss: 41.14400100708008\n",
      "CL\n",
      "Classification Model Iter 616/1000 - Loss: 41.14368438720703\n",
      "CL\n",
      "Classification Model Iter 617/1000 - Loss: 41.1441650390625\n",
      "CL\n",
      "Classification Model Iter 618/1000 - Loss: 41.14408874511719\n",
      "CL\n",
      "Classification Model Iter 619/1000 - Loss: 41.14422607421875\n",
      "CL\n",
      "Classification Model Iter 620/1000 - Loss: 41.14358139038086\n",
      "CL\n",
      "Classification Model Iter 621/1000 - Loss: 41.143463134765625\n",
      "CL\n",
      "Classification Model Iter 622/1000 - Loss: 41.14377212524414\n",
      "CL\n",
      "Classification Model Iter 623/1000 - Loss: 41.143497467041016\n",
      "CL\n",
      "Classification Model Iter 624/1000 - Loss: 41.14381790161133\n",
      "CL\n",
      "Classification Model Iter 625/1000 - Loss: 41.143306732177734\n",
      "CL\n",
      "Classification Model Iter 626/1000 - Loss: 41.14368438720703\n",
      "CL\n",
      "Classification Model Iter 627/1000 - Loss: 41.14359664916992\n",
      "CL\n",
      "Classification Model Iter 628/1000 - Loss: 41.14371109008789\n",
      "CL\n",
      "Classification Model Iter 629/1000 - Loss: 41.14427185058594\n",
      "CL\n",
      "Classification Model Iter 630/1000 - Loss: 41.144290924072266\n",
      "CL\n",
      "Classification Model Iter 631/1000 - Loss: 41.14464569091797\n",
      "CL\n",
      "Classification Model Iter 632/1000 - Loss: 41.14461135864258\n",
      "CL\n",
      "Classification Model Iter 633/1000 - Loss: 41.14442443847656\n",
      "CL\n",
      "Classification Model Iter 634/1000 - Loss: 41.14432907104492\n",
      "CL\n",
      "Classification Model Iter 635/1000 - Loss: 41.14417266845703\n",
      "CL\n",
      "Classification Model Iter 636/1000 - Loss: 41.14468002319336\n",
      "CL\n",
      "Classification Model Iter 637/1000 - Loss: 41.145023345947266\n",
      "CL\n",
      "Classification Model Iter 638/1000 - Loss: 41.145206451416016\n",
      "CL\n",
      "Classification Model Iter 639/1000 - Loss: 41.145503997802734\n",
      "CL\n",
      "Classification Model Iter 640/1000 - Loss: 41.14558792114258\n",
      "CL\n",
      "Classification Model Iter 641/1000 - Loss: 41.1451530456543\n",
      "CL\n",
      "Classification Model Iter 642/1000 - Loss: 41.144893646240234\n",
      "CL\n",
      "Classification Model Iter 643/1000 - Loss: 41.14506149291992\n",
      "CL\n",
      "Classification Model Iter 644/1000 - Loss: 41.14508819580078\n",
      "CL\n",
      "Classification Model Iter 645/1000 - Loss: 41.14512634277344\n",
      "CL\n",
      "Classification Model Iter 646/1000 - Loss: 41.14487075805664\n",
      "CL\n",
      "Classification Model Iter 647/1000 - Loss: 41.14523696899414\n",
      "CL\n",
      "Classification Model Iter 648/1000 - Loss: 41.14551544189453\n",
      "CL\n",
      "Classification Model Iter 649/1000 - Loss: 41.14546585083008\n",
      "CL\n",
      "Classification Model Iter 650/1000 - Loss: 41.145538330078125\n",
      "CL\n",
      "Classification Model Iter 651/1000 - Loss: 41.145687103271484\n",
      "CL\n",
      "Classification Model Iter 652/1000 - Loss: 41.14556884765625\n",
      "CL\n",
      "Classification Model Iter 653/1000 - Loss: 41.14583969116211\n",
      "CL\n",
      "Classification Model Iter 654/1000 - Loss: 41.145545959472656\n",
      "CL\n",
      "Classification Model Iter 655/1000 - Loss: 41.145145416259766\n",
      "CL\n",
      "Classification Model Iter 656/1000 - Loss: 41.14506530761719\n",
      "CL\n",
      "Classification Model Iter 657/1000 - Loss: 41.14484786987305\n",
      "CL\n",
      "Classification Model Iter 658/1000 - Loss: 41.144771575927734\n",
      "CL\n",
      "Classification Model Iter 659/1000 - Loss: 41.14451217651367\n",
      "CL\n",
      "Classification Model Iter 660/1000 - Loss: 41.14436721801758\n",
      "CL\n",
      "Classification Model Iter 661/1000 - Loss: 41.1442756652832\n",
      "CL\n",
      "Classification Model Iter 662/1000 - Loss: 41.144287109375\n",
      "CL\n",
      "Classification Model Iter 663/1000 - Loss: 41.14419174194336\n",
      "CL\n",
      "Classification Model Iter 664/1000 - Loss: 41.144474029541016\n",
      "CL\n",
      "Classification Model Iter 665/1000 - Loss: 41.144126892089844\n",
      "CL\n",
      "Classification Model Iter 666/1000 - Loss: 41.14460754394531\n",
      "CL\n",
      "Classification Model Iter 667/1000 - Loss: 41.14482116699219\n",
      "CL\n",
      "Classification Model Iter 668/1000 - Loss: 41.14512252807617\n",
      "CL\n",
      "Classification Model Iter 669/1000 - Loss: 41.145538330078125\n",
      "CL\n",
      "Classification Model Iter 670/1000 - Loss: 41.144920349121094\n",
      "CL\n",
      "Classification Model Iter 671/1000 - Loss: 41.14482879638672\n",
      "CL\n",
      "Classification Model Iter 672/1000 - Loss: 41.144649505615234\n",
      "CL\n",
      "Classification Model Iter 673/1000 - Loss: 41.144493103027344\n",
      "CL\n",
      "Classification Model Iter 674/1000 - Loss: 41.144752502441406\n",
      "CL\n",
      "Classification Model Iter 675/1000 - Loss: 41.14500427246094\n",
      "CL\n",
      "Classification Model Iter 676/1000 - Loss: 41.14510726928711\n",
      "CL\n",
      "Classification Model Iter 677/1000 - Loss: 41.145267486572266\n",
      "CL\n",
      "Classification Model Iter 678/1000 - Loss: 41.1456298828125\n",
      "CL\n",
      "Classification Model Iter 679/1000 - Loss: 41.145267486572266\n",
      "CL\n",
      "Classification Model Iter 680/1000 - Loss: 41.14521026611328\n",
      "CL\n",
      "Classification Model Iter 681/1000 - Loss: 41.145225524902344\n",
      "CL\n",
      "Classification Model Iter 682/1000 - Loss: 41.145381927490234\n",
      "CL\n",
      "Classification Model Iter 683/1000 - Loss: 41.14481735229492\n",
      "CL\n",
      "Classification Model Iter 684/1000 - Loss: 41.14490509033203\n",
      "CL\n",
      "Classification Model Iter 685/1000 - Loss: 41.14500045776367\n",
      "CL\n",
      "Classification Model Iter 686/1000 - Loss: 41.1446418762207\n",
      "CL\n",
      "Classification Model Iter 687/1000 - Loss: 41.144195556640625\n",
      "CL\n",
      "Classification Model Iter 688/1000 - Loss: 41.14359664916992\n",
      "CL\n",
      "Classification Model Iter 689/1000 - Loss: 41.14393615722656\n",
      "CL\n",
      "Classification Model Iter 690/1000 - Loss: 41.144256591796875\n",
      "CL\n",
      "Classification Model Iter 691/1000 - Loss: 41.14375305175781\n",
      "CL\n",
      "Classification Model Iter 692/1000 - Loss: 41.14358139038086\n",
      "CL\n",
      "Classification Model Iter 693/1000 - Loss: 41.14385223388672\n",
      "CL\n",
      "Classification Model Iter 694/1000 - Loss: 41.14414978027344\n",
      "CL\n",
      "Classification Model Iter 695/1000 - Loss: 41.14451217651367\n",
      "CL\n",
      "Classification Model Iter 696/1000 - Loss: 41.144432067871094\n",
      "CL\n",
      "Classification Model Iter 697/1000 - Loss: 41.144718170166016\n",
      "CL\n",
      "Classification Model Iter 698/1000 - Loss: 41.144920349121094\n",
      "CL\n",
      "Classification Model Iter 699/1000 - Loss: 41.14516067504883\n",
      "CL\n",
      "Classification Model Iter 700/1000 - Loss: 41.14559555053711\n",
      "CL\n",
      "Classification Model Iter 701/1000 - Loss: 41.14586639404297\n",
      "CL\n",
      "Classification Model Iter 702/1000 - Loss: 41.145904541015625\n",
      "CL\n",
      "Classification Model Iter 703/1000 - Loss: 41.14621353149414\n",
      "CL\n",
      "Classification Model Iter 704/1000 - Loss: 41.14577102661133\n",
      "CL\n",
      "Classification Model Iter 705/1000 - Loss: 41.14570999145508\n",
      "CL\n",
      "Classification Model Iter 706/1000 - Loss: 41.145530700683594\n",
      "CL\n",
      "Classification Model Iter 707/1000 - Loss: 41.14577865600586\n",
      "CL\n",
      "Classification Model Iter 708/1000 - Loss: 41.145301818847656\n",
      "CL\n",
      "Classification Model Iter 709/1000 - Loss: 41.144874572753906\n",
      "CL\n",
      "Classification Model Iter 710/1000 - Loss: 41.14497756958008\n",
      "CL\n",
      "Classification Model Iter 711/1000 - Loss: 41.145137786865234\n",
      "CL\n",
      "Classification Model Iter 712/1000 - Loss: 41.1443977355957\n",
      "CL\n",
      "Classification Model Iter 713/1000 - Loss: 41.14452362060547\n",
      "CL\n",
      "Classification Model Iter 714/1000 - Loss: 41.144264221191406\n",
      "CL\n",
      "Classification Model Iter 715/1000 - Loss: 41.14415740966797\n",
      "CL\n",
      "Classification Model Iter 716/1000 - Loss: 41.14414596557617\n",
      "CL\n",
      "Classification Model Iter 717/1000 - Loss: 41.14403533935547\n",
      "CL\n",
      "Classification Model Iter 718/1000 - Loss: 41.14417266845703\n",
      "CL\n",
      "Classification Model Iter 719/1000 - Loss: 41.14414978027344\n",
      "CL\n",
      "Classification Model Iter 720/1000 - Loss: 41.14445495605469\n",
      "CL\n",
      "Classification Model Iter 721/1000 - Loss: 41.14440155029297\n",
      "CL\n",
      "Classification Model Iter 722/1000 - Loss: 41.14432907104492\n",
      "CL\n",
      "Classification Model Iter 723/1000 - Loss: 41.144744873046875\n",
      "CL\n",
      "Classification Model Iter 724/1000 - Loss: 41.14521789550781\n",
      "CL\n",
      "Classification Model Iter 725/1000 - Loss: 41.145790100097656\n",
      "CL\n",
      "Classification Model Iter 726/1000 - Loss: 41.145538330078125\n",
      "CL\n",
      "Classification Model Iter 727/1000 - Loss: 41.14543533325195\n",
      "CL\n",
      "Classification Model Iter 728/1000 - Loss: 41.145320892333984\n",
      "CL\n",
      "Classification Model Iter 729/1000 - Loss: 41.14584732055664\n",
      "CL\n",
      "Classification Model Iter 730/1000 - Loss: 41.1461296081543\n",
      "CL\n",
      "Classification Model Iter 731/1000 - Loss: 41.14627456665039\n",
      "CL\n",
      "Classification Model Iter 732/1000 - Loss: 41.146263122558594\n",
      "CL\n",
      "Classification Model Iter 733/1000 - Loss: 41.1460075378418\n",
      "CL\n",
      "Classification Model Iter 734/1000 - Loss: 41.146148681640625\n",
      "CL\n",
      "Classification Model Iter 735/1000 - Loss: 41.146522521972656\n",
      "CL\n",
      "Classification Model Iter 736/1000 - Loss: 41.14619064331055\n",
      "CL\n",
      "Classification Model Iter 737/1000 - Loss: 41.14653015136719\n",
      "CL\n",
      "Classification Model Iter 738/1000 - Loss: 41.14635467529297\n",
      "CL\n",
      "Classification Model Iter 739/1000 - Loss: 41.146602630615234\n",
      "CL\n",
      "Classification Model Iter 740/1000 - Loss: 41.14649200439453\n",
      "CL\n",
      "Classification Model Iter 741/1000 - Loss: 41.14618682861328\n",
      "CL\n",
      "Classification Model Iter 742/1000 - Loss: 41.14582443237305\n",
      "CL\n",
      "Classification Model Iter 743/1000 - Loss: 41.145626068115234\n",
      "CL\n",
      "Classification Model Iter 744/1000 - Loss: 41.14555740356445\n",
      "CL\n",
      "Classification Model Iter 745/1000 - Loss: 41.14527130126953\n",
      "CL\n",
      "Classification Model Iter 746/1000 - Loss: 41.14525604248047\n",
      "CL\n",
      "Classification Model Iter 747/1000 - Loss: 41.145477294921875\n",
      "CL\n",
      "Classification Model Iter 748/1000 - Loss: 41.14530563354492\n",
      "CL\n",
      "Classification Model Iter 749/1000 - Loss: 41.144615173339844\n",
      "CL\n",
      "Classification Model Iter 750/1000 - Loss: 41.14460754394531\n",
      "CL\n",
      "Classification Model Iter 751/1000 - Loss: 41.144683837890625\n",
      "CL\n",
      "Classification Model Iter 752/1000 - Loss: 41.14483642578125\n",
      "CL\n",
      "Classification Model Iter 753/1000 - Loss: 41.14485168457031\n",
      "CL\n",
      "Classification Model Iter 754/1000 - Loss: 41.14468002319336\n",
      "CL\n",
      "Classification Model Iter 755/1000 - Loss: 41.144535064697266\n",
      "CL\n",
      "Classification Model Iter 756/1000 - Loss: 41.144283294677734\n",
      "CL\n",
      "Classification Model Iter 757/1000 - Loss: 41.14399337768555\n",
      "CL\n",
      "Classification Model Iter 758/1000 - Loss: 41.14384841918945\n",
      "CL\n",
      "Classification Model Iter 759/1000 - Loss: 41.14379119873047\n",
      "CL\n",
      "Classification Model Iter 760/1000 - Loss: 41.143524169921875\n",
      "CL\n",
      "Classification Model Iter 761/1000 - Loss: 41.1438102722168\n",
      "CL\n",
      "Classification Model Iter 762/1000 - Loss: 41.143836975097656\n",
      "CL\n",
      "Classification Model Iter 763/1000 - Loss: 41.14421844482422\n",
      "CL\n",
      "Classification Model Iter 764/1000 - Loss: 41.14432907104492\n",
      "CL\n",
      "Classification Model Iter 765/1000 - Loss: 41.14426803588867\n",
      "CL\n",
      "Classification Model Iter 766/1000 - Loss: 41.14427185058594\n",
      "CL\n",
      "Classification Model Iter 767/1000 - Loss: 41.144493103027344\n",
      "CL\n",
      "Classification Model Iter 768/1000 - Loss: 41.14484786987305\n",
      "CL\n",
      "Classification Model Iter 769/1000 - Loss: 41.144981384277344\n",
      "CL\n",
      "Classification Model Iter 770/1000 - Loss: 41.14482498168945\n",
      "CL\n",
      "Classification Model Iter 771/1000 - Loss: 41.14453125\n",
      "CL\n",
      "Classification Model Iter 772/1000 - Loss: 41.14436721801758\n",
      "CL\n",
      "Classification Model Iter 773/1000 - Loss: 41.14482498168945\n",
      "CL\n",
      "Classification Model Iter 774/1000 - Loss: 41.14488983154297\n",
      "CL\n",
      "Classification Model Iter 775/1000 - Loss: 41.144874572753906\n",
      "CL\n",
      "Classification Model Iter 776/1000 - Loss: 41.144901275634766\n",
      "CL\n",
      "Classification Model Iter 777/1000 - Loss: 41.144256591796875\n",
      "CL\n",
      "Classification Model Iter 778/1000 - Loss: 41.143943786621094\n",
      "CL\n",
      "Classification Model Iter 779/1000 - Loss: 41.14371109008789\n",
      "CL\n",
      "Classification Model Iter 780/1000 - Loss: 41.14394760131836\n",
      "CL\n",
      "Classification Model Iter 781/1000 - Loss: 41.14422607421875\n",
      "CL\n",
      "Classification Model Iter 782/1000 - Loss: 41.14331817626953\n",
      "CL\n",
      "Classification Model Iter 783/1000 - Loss: 41.14329147338867\n",
      "CL\n",
      "Classification Model Iter 784/1000 - Loss: 41.1428108215332\n",
      "CL\n",
      "Classification Model Iter 785/1000 - Loss: 41.14323043823242\n",
      "CL\n",
      "Classification Model Iter 786/1000 - Loss: 41.143775939941406\n",
      "CL\n",
      "Classification Model Iter 787/1000 - Loss: 41.14373779296875\n",
      "CL\n",
      "Classification Model Iter 788/1000 - Loss: 41.14379119873047\n",
      "CL\n",
      "Classification Model Iter 789/1000 - Loss: 41.143741607666016\n",
      "CL\n",
      "Classification Model Iter 790/1000 - Loss: 41.14336395263672\n",
      "CL\n",
      "Classification Model Iter 791/1000 - Loss: 41.143898010253906\n",
      "CL\n",
      "Classification Model Iter 792/1000 - Loss: 41.14423751831055\n",
      "CL\n",
      "Classification Model Iter 793/1000 - Loss: 41.144344329833984\n",
      "CL\n",
      "Classification Model Iter 794/1000 - Loss: 41.144046783447266\n",
      "CL\n",
      "Classification Model Iter 795/1000 - Loss: 41.14431381225586\n",
      "CL\n",
      "Classification Model Iter 796/1000 - Loss: 41.144535064697266\n",
      "CL\n",
      "Classification Model Iter 797/1000 - Loss: 41.14481735229492\n",
      "CL\n",
      "Classification Model Iter 798/1000 - Loss: 41.14503860473633\n",
      "CL\n",
      "Classification Model Iter 799/1000 - Loss: 41.144691467285156\n",
      "CL\n",
      "Classification Model Iter 800/1000 - Loss: 41.145198822021484\n",
      "CL\n",
      "Classification Model Iter 801/1000 - Loss: 41.14576721191406\n",
      "CL\n",
      "Classification Model Iter 802/1000 - Loss: 41.14603042602539\n",
      "CL\n",
      "Classification Model Iter 803/1000 - Loss: 41.146018981933594\n",
      "CL\n",
      "Classification Model Iter 804/1000 - Loss: 41.1458625793457\n",
      "CL\n",
      "Classification Model Iter 805/1000 - Loss: 41.14564514160156\n",
      "CL\n",
      "Classification Model Iter 806/1000 - Loss: 41.14631271362305\n",
      "CL\n",
      "Classification Model Iter 807/1000 - Loss: 41.14557647705078\n",
      "CL\n",
      "Classification Model Iter 808/1000 - Loss: 41.14533996582031\n",
      "CL\n",
      "Classification Model Iter 809/1000 - Loss: 41.145294189453125\n",
      "CL\n",
      "Classification Model Iter 810/1000 - Loss: 41.14509963989258\n",
      "CL\n",
      "Classification Model Iter 811/1000 - Loss: 41.1451301574707\n",
      "CL\n",
      "Classification Model Iter 812/1000 - Loss: 41.144962310791016\n",
      "CL\n",
      "Classification Model Iter 813/1000 - Loss: 41.144866943359375\n",
      "CL\n",
      "Classification Model Iter 814/1000 - Loss: 41.14539337158203\n",
      "CL\n",
      "Classification Model Iter 815/1000 - Loss: 41.14566421508789\n",
      "CL\n",
      "Classification Model Iter 816/1000 - Loss: 41.14500427246094\n",
      "CL\n",
      "Classification Model Iter 817/1000 - Loss: 41.14540100097656\n",
      "CL\n",
      "Classification Model Iter 818/1000 - Loss: 41.14535903930664\n",
      "CL\n",
      "Classification Model Iter 819/1000 - Loss: 41.14569091796875\n",
      "CL\n",
      "Classification Model Iter 820/1000 - Loss: 41.146060943603516\n",
      "CL\n",
      "Classification Model Iter 821/1000 - Loss: 41.14632797241211\n",
      "CL\n",
      "Classification Model Iter 822/1000 - Loss: 41.14670181274414\n",
      "CL\n",
      "Classification Model Iter 823/1000 - Loss: 41.1469841003418\n",
      "CL\n",
      "Classification Model Iter 824/1000 - Loss: 41.146728515625\n",
      "CL\n",
      "Classification Model Iter 825/1000 - Loss: 41.146480560302734\n",
      "CL\n",
      "Classification Model Iter 826/1000 - Loss: 41.14607620239258\n",
      "CL\n",
      "Classification Model Iter 827/1000 - Loss: 41.14590835571289\n",
      "CL\n",
      "Classification Model Iter 828/1000 - Loss: 41.14556884765625\n",
      "CL\n",
      "Classification Model Iter 829/1000 - Loss: 41.145755767822266\n",
      "CL\n",
      "Classification Model Iter 830/1000 - Loss: 41.14562225341797\n",
      "CL\n",
      "Classification Model Iter 831/1000 - Loss: 41.14512252807617\n",
      "CL\n",
      "Classification Model Iter 832/1000 - Loss: 41.14535903930664\n",
      "CL\n",
      "Classification Model Iter 833/1000 - Loss: 41.14515686035156\n",
      "CL\n",
      "Classification Model Iter 834/1000 - Loss: 41.144596099853516\n",
      "CL\n",
      "Classification Model Iter 835/1000 - Loss: 41.144561767578125\n",
      "CL\n",
      "Classification Model Iter 836/1000 - Loss: 41.14448547363281\n",
      "CL\n",
      "Classification Model Iter 837/1000 - Loss: 41.1447639465332\n",
      "CL\n",
      "Classification Model Iter 838/1000 - Loss: 41.14482116699219\n",
      "CL\n",
      "Classification Model Iter 839/1000 - Loss: 41.144954681396484\n",
      "CL\n",
      "Classification Model Iter 840/1000 - Loss: 41.145225524902344\n",
      "CL\n",
      "Classification Model Iter 841/1000 - Loss: 41.14522933959961\n",
      "CL\n",
      "Classification Model Iter 842/1000 - Loss: 41.145755767822266\n",
      "CL\n",
      "Classification Model Iter 843/1000 - Loss: 41.145774841308594\n",
      "CL\n",
      "Classification Model Iter 844/1000 - Loss: 41.14570617675781\n",
      "CL\n",
      "Classification Model Iter 845/1000 - Loss: 41.14543914794922\n",
      "CL\n",
      "Classification Model Iter 846/1000 - Loss: 41.14543914794922\n",
      "CL\n",
      "Classification Model Iter 847/1000 - Loss: 41.14573287963867\n",
      "CL\n",
      "Classification Model Iter 848/1000 - Loss: 41.1457405090332\n",
      "CL\n",
      "Classification Model Iter 849/1000 - Loss: 41.14562225341797\n",
      "CL\n",
      "Classification Model Iter 850/1000 - Loss: 41.145545959472656\n",
      "CL\n",
      "Classification Model Iter 851/1000 - Loss: 41.146018981933594\n",
      "CL\n",
      "Classification Model Iter 852/1000 - Loss: 41.14635467529297\n",
      "CL\n",
      "Classification Model Iter 853/1000 - Loss: 41.146514892578125\n",
      "CL\n",
      "Classification Model Iter 854/1000 - Loss: 41.146484375\n",
      "CL\n",
      "Classification Model Iter 855/1000 - Loss: 41.1468505859375\n",
      "CL\n",
      "Classification Model Iter 856/1000 - Loss: 41.14693069458008\n",
      "CL\n",
      "Classification Model Iter 857/1000 - Loss: 41.14763641357422\n",
      "CL\n",
      "Classification Model Iter 858/1000 - Loss: 41.147743225097656\n",
      "CL\n",
      "Classification Model Iter 859/1000 - Loss: 41.147552490234375\n",
      "CL\n",
      "Classification Model Iter 860/1000 - Loss: 41.14727783203125\n",
      "CL\n",
      "Classification Model Iter 861/1000 - Loss: 41.147544860839844\n",
      "CL\n",
      "Classification Model Iter 862/1000 - Loss: 41.148155212402344\n",
      "CL\n",
      "Classification Model Iter 863/1000 - Loss: 41.14802169799805\n",
      "CL\n",
      "Classification Model Iter 864/1000 - Loss: 41.146934509277344\n",
      "CL\n",
      "Classification Model Iter 865/1000 - Loss: 41.14695358276367\n",
      "CL\n",
      "Classification Model Iter 866/1000 - Loss: 41.146488189697266\n",
      "CL\n",
      "Classification Model Iter 867/1000 - Loss: 41.146522521972656\n",
      "CL\n",
      "Classification Model Iter 868/1000 - Loss: 41.14629364013672\n",
      "CL\n",
      "Classification Model Iter 869/1000 - Loss: 41.14583206176758\n",
      "CL\n",
      "Classification Model Iter 870/1000 - Loss: 41.14572525024414\n",
      "CL\n",
      "Classification Model Iter 871/1000 - Loss: 41.14583969116211\n",
      "CL\n",
      "Classification Model Iter 872/1000 - Loss: 41.14603805541992\n",
      "CL\n",
      "Classification Model Iter 873/1000 - Loss: 41.145870208740234\n",
      "CL\n",
      "Classification Model Iter 874/1000 - Loss: 41.14561462402344\n",
      "CL\n",
      "Classification Model Iter 875/1000 - Loss: 41.14566421508789\n",
      "CL\n",
      "Classification Model Iter 876/1000 - Loss: 41.145633697509766\n",
      "CL\n",
      "Classification Model Iter 877/1000 - Loss: 41.146270751953125\n",
      "CL\n",
      "Classification Model Iter 878/1000 - Loss: 41.146278381347656\n",
      "CL\n",
      "Classification Model Iter 879/1000 - Loss: 41.146366119384766\n",
      "CL\n",
      "Classification Model Iter 880/1000 - Loss: 41.146766662597656\n",
      "CL\n",
      "Classification Model Iter 881/1000 - Loss: 41.146324157714844\n",
      "CL\n",
      "Classification Model Iter 882/1000 - Loss: 41.14615249633789\n",
      "CL\n",
      "Classification Model Iter 883/1000 - Loss: 41.14606475830078\n",
      "CL\n",
      "Classification Model Iter 884/1000 - Loss: 41.14613342285156\n",
      "CL\n",
      "Classification Model Iter 885/1000 - Loss: 41.14634323120117\n",
      "CL\n",
      "Classification Model Iter 886/1000 - Loss: 41.14681625366211\n",
      "CL\n",
      "Classification Model Iter 887/1000 - Loss: 41.14750289916992\n",
      "CL\n",
      "Classification Model Iter 888/1000 - Loss: 41.14800262451172\n",
      "CL\n",
      "Classification Model Iter 889/1000 - Loss: 41.148475646972656\n",
      "CL\n",
      "Classification Model Iter 890/1000 - Loss: 41.148921966552734\n",
      "CL\n",
      "Classification Model Iter 891/1000 - Loss: 41.14939880371094\n",
      "CL\n",
      "Classification Model Iter 892/1000 - Loss: 41.149688720703125\n",
      "CL\n",
      "Classification Model Iter 893/1000 - Loss: 41.149845123291016\n",
      "CL\n",
      "Classification Model Iter 894/1000 - Loss: 41.149539947509766\n",
      "CL\n",
      "Classification Model Iter 895/1000 - Loss: 41.14915466308594\n",
      "CL\n",
      "Classification Model Iter 896/1000 - Loss: 41.14902114868164\n",
      "CL\n",
      "Classification Model Iter 897/1000 - Loss: 41.14931106567383\n",
      "CL\n",
      "Classification Model Iter 898/1000 - Loss: 41.1492919921875\n",
      "CL\n",
      "Classification Model Iter 899/1000 - Loss: 41.14917755126953\n",
      "CL\n",
      "Classification Model Iter 900/1000 - Loss: 41.14826965332031\n",
      "CL\n",
      "Classification Model Iter 901/1000 - Loss: 41.147987365722656\n",
      "CL\n",
      "Classification Model Iter 902/1000 - Loss: 41.147945404052734\n",
      "CL\n",
      "Classification Model Iter 903/1000 - Loss: 41.14766311645508\n",
      "CL\n",
      "Classification Model Iter 904/1000 - Loss: 41.14748001098633\n",
      "CL\n",
      "Classification Model Iter 905/1000 - Loss: 41.14695358276367\n",
      "CL\n",
      "Classification Model Iter 906/1000 - Loss: 41.146446228027344\n",
      "CL\n",
      "Classification Model Iter 907/1000 - Loss: 41.14632034301758\n",
      "CL\n",
      "Classification Model Iter 908/1000 - Loss: 41.146121978759766\n",
      "CL\n",
      "Classification Model Iter 909/1000 - Loss: 41.145652770996094\n",
      "CL\n",
      "Classification Model Iter 910/1000 - Loss: 41.14554977416992\n",
      "CL\n",
      "Classification Model Iter 911/1000 - Loss: 41.1451530456543\n",
      "CL\n",
      "Classification Model Iter 912/1000 - Loss: 41.14521026611328\n",
      "CL\n",
      "Classification Model Iter 913/1000 - Loss: 41.145591735839844\n",
      "CL\n",
      "Classification Model Iter 914/1000 - Loss: 41.145362854003906\n",
      "CL\n",
      "Classification Model Iter 915/1000 - Loss: 41.145450592041016\n",
      "CL\n",
      "Classification Model Iter 916/1000 - Loss: 41.14525604248047\n",
      "CL\n",
      "Classification Model Iter 917/1000 - Loss: 41.14561462402344\n",
      "CL\n",
      "Classification Model Iter 918/1000 - Loss: 41.14577865600586\n",
      "CL\n",
      "Classification Model Iter 919/1000 - Loss: 41.146263122558594\n",
      "CL\n",
      "Classification Model Iter 920/1000 - Loss: 41.1458625793457\n",
      "CL\n",
      "Classification Model Iter 921/1000 - Loss: 41.14631652832031\n",
      "CL\n",
      "Classification Model Iter 922/1000 - Loss: 41.14690017700195\n",
      "CL\n",
      "Classification Model Iter 923/1000 - Loss: 41.14708709716797\n",
      "CL\n",
      "Classification Model Iter 924/1000 - Loss: 41.14706039428711\n",
      "CL\n",
      "Classification Model Iter 925/1000 - Loss: 41.14723587036133\n",
      "CL\n",
      "Classification Model Iter 926/1000 - Loss: 41.14756393432617\n",
      "CL\n",
      "Classification Model Iter 927/1000 - Loss: 41.147682189941406\n",
      "CL\n",
      "Classification Model Iter 928/1000 - Loss: 41.1478385925293\n",
      "CL\n",
      "Classification Model Iter 929/1000 - Loss: 41.14786148071289\n",
      "CL\n",
      "Classification Model Iter 930/1000 - Loss: 41.14822006225586\n",
      "CL\n",
      "Classification Model Iter 931/1000 - Loss: 41.14791488647461\n",
      "CL\n",
      "Classification Model Iter 932/1000 - Loss: 41.148681640625\n",
      "CL\n",
      "Classification Model Iter 933/1000 - Loss: 41.14884948730469\n",
      "CL\n",
      "Classification Model Iter 934/1000 - Loss: 41.148860931396484\n",
      "CL\n",
      "Classification Model Iter 935/1000 - Loss: 41.14896011352539\n",
      "CL\n",
      "Classification Model Iter 936/1000 - Loss: 41.14889907836914\n",
      "CL\n",
      "Classification Model Iter 937/1000 - Loss: 41.14915466308594\n",
      "CL\n",
      "Classification Model Iter 938/1000 - Loss: 41.14971923828125\n",
      "CL\n",
      "Classification Model Iter 939/1000 - Loss: 41.15013122558594\n",
      "CL\n",
      "Classification Model Iter 940/1000 - Loss: 41.15017318725586\n",
      "CL\n",
      "Classification Model Iter 941/1000 - Loss: 41.15020751953125\n",
      "CL\n",
      "Classification Model Iter 942/1000 - Loss: 41.149925231933594\n",
      "CL\n",
      "Classification Model Iter 943/1000 - Loss: 41.149505615234375\n",
      "CL\n",
      "Classification Model Iter 944/1000 - Loss: 41.149295806884766\n",
      "CL\n",
      "Classification Model Iter 945/1000 - Loss: 41.14948272705078\n",
      "CL\n",
      "Classification Model Iter 946/1000 - Loss: 41.1495475769043\n",
      "CL\n",
      "Classification Model Iter 947/1000 - Loss: 41.14923858642578\n",
      "CL\n",
      "Classification Model Iter 948/1000 - Loss: 41.149147033691406\n",
      "CL\n",
      "Classification Model Iter 949/1000 - Loss: 41.14882278442383\n",
      "CL\n",
      "Classification Model Iter 950/1000 - Loss: 41.14897155761719\n",
      "CL\n",
      "Classification Model Iter 951/1000 - Loss: 41.14918899536133\n",
      "CL\n",
      "Classification Model Iter 952/1000 - Loss: 41.1496467590332\n",
      "CL\n",
      "Classification Model Iter 953/1000 - Loss: 41.14958572387695\n",
      "CL\n",
      "Classification Model Iter 954/1000 - Loss: 41.149471282958984\n",
      "CL\n",
      "Classification Model Iter 955/1000 - Loss: 41.14973449707031\n",
      "CL\n",
      "Classification Model Iter 956/1000 - Loss: 41.15004348754883\n",
      "CL\n",
      "Classification Model Iter 957/1000 - Loss: 41.1507453918457\n",
      "CL\n",
      "Classification Model Iter 958/1000 - Loss: 41.15087890625\n",
      "CL\n",
      "Classification Model Iter 959/1000 - Loss: 41.1510009765625\n",
      "CL\n",
      "Classification Model Iter 960/1000 - Loss: 41.15155029296875\n",
      "CL\n",
      "Classification Model Iter 961/1000 - Loss: 41.1524543762207\n",
      "CL\n",
      "Classification Model Iter 962/1000 - Loss: 41.152565002441406\n",
      "CL\n",
      "Classification Model Iter 963/1000 - Loss: 41.152931213378906\n",
      "CL\n",
      "Classification Model Iter 964/1000 - Loss: 41.1527214050293\n",
      "CL\n",
      "Classification Model Iter 965/1000 - Loss: 41.15254211425781\n",
      "CL\n",
      "Classification Model Iter 966/1000 - Loss: 41.152740478515625\n",
      "CL\n",
      "Classification Model Iter 967/1000 - Loss: 41.15306854248047\n",
      "CL\n",
      "Classification Model Iter 968/1000 - Loss: 41.153385162353516\n",
      "CL\n",
      "Classification Model Iter 969/1000 - Loss: 41.153236389160156\n",
      "CL\n",
      "Classification Model Iter 970/1000 - Loss: 41.15342712402344\n",
      "CL\n",
      "Classification Model Iter 971/1000 - Loss: 41.153480529785156\n",
      "CL\n",
      "Classification Model Iter 972/1000 - Loss: 41.153053283691406\n",
      "CL\n",
      "Classification Model Iter 973/1000 - Loss: 41.15351104736328\n",
      "CL\n",
      "Classification Model Iter 974/1000 - Loss: 41.153568267822266\n",
      "CL\n",
      "Classification Model Iter 975/1000 - Loss: 41.15431594848633\n",
      "CL\n",
      "Classification Model Iter 976/1000 - Loss: 41.154762268066406\n",
      "CL\n",
      "Classification Model Iter 977/1000 - Loss: 41.15449523925781\n",
      "CL\n",
      "Classification Model Iter 978/1000 - Loss: 41.15428161621094\n",
      "CL\n",
      "Classification Model Iter 979/1000 - Loss: 41.15354919433594\n",
      "CL\n",
      "Classification Model Iter 980/1000 - Loss: 41.15412902832031\n",
      "CL\n",
      "Classification Model Iter 981/1000 - Loss: 41.15410614013672\n",
      "CL\n",
      "Classification Model Iter 982/1000 - Loss: 41.153831481933594\n",
      "CL\n",
      "Classification Model Iter 983/1000 - Loss: 41.15324401855469\n",
      "CL\n",
      "Classification Model Iter 984/1000 - Loss: 41.15317916870117\n",
      "CL\n",
      "Classification Model Iter 985/1000 - Loss: 41.153018951416016\n",
      "CL\n",
      "Classification Model Iter 986/1000 - Loss: 41.152984619140625\n",
      "CL\n",
      "Classification Model Iter 987/1000 - Loss: 41.15269470214844\n",
      "CL\n",
      "Classification Model Iter 988/1000 - Loss: 41.15238571166992\n",
      "CL\n",
      "Classification Model Iter 989/1000 - Loss: 41.1522102355957\n",
      "CL\n",
      "Classification Model Iter 990/1000 - Loss: 41.152679443359375\n",
      "CL\n",
      "Classification Model Iter 991/1000 - Loss: 41.152610778808594\n",
      "CL\n",
      "Classification Model Iter 992/1000 - Loss: 41.15248107910156\n",
      "CL\n",
      "Classification Model Iter 993/1000 - Loss: 41.15221405029297\n",
      "CL\n",
      "Classification Model Iter 994/1000 - Loss: 41.151756286621094\n",
      "CL\n",
      "Classification Model Iter 995/1000 - Loss: 41.151611328125\n",
      "CL\n",
      "Classification Model Iter 996/1000 - Loss: 41.15115737915039\n",
      "CL\n",
      "Classification Model Iter 997/1000 - Loss: 41.15106201171875\n",
      "CL\n",
      "Classification Model Iter 998/1000 - Loss: 41.15062713623047\n",
      "CL\n",
      "Classification Model Iter 999/1000 - Loss: 41.151084899902344\n",
      "CL\n",
      "Classification Model Iter 1000/1000 - Loss: 41.15119934082031\n",
      "NaN detected in predictions!\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "training_iter = 1000  # Number of training iterations\n",
    "lr = 1  # Learning rate\n",
    "\n",
    "# loss weights\n",
    "elbo_weight = 0.9\n",
    "custom_weight = 1-elbo_weight\n",
    "\n",
    "model_1.train()\n",
    "likelihood_1.train()\n",
    "\n",
    "optimizer_1 = torch.optim.Adam(model_1.parameters(), lr=lr)\n",
    "mll_1 = gpytorch.mlls.VariationalELBO(likelihood_1, model_1, num_data=X_train_1.size(0))\n",
    "\n",
    "# Implement Gradient Clipping\n",
    "for i in range(training_iter):\n",
    "    optimizer_1.zero_grad()\n",
    "    output_1 = model_1(X_train_1)\n",
    "    \n",
    "    # Check for NaNs in the output\n",
    "    if torch.isnan(output_1.mean).any():\n",
    "        print(f\"Warning: NaN detected in output at iteration {i + 1}\")\n",
    "        break\n",
    "\n",
    "    prediction = model_1(X_train_1)\n",
    "    predicted_probabilities = torch.softmax(prediction.mean, dim=0)\n",
    "    predicted_labels = predicted_probabilities.argmax(dim=0).detach()\n",
    "    \n",
    "    sim_score = torch.ones(1, requires_grad=True)*custom_loss(predicted_labels, y_train_1)\n",
    "    loss_1 = elbo_weight*(-mll_1(output_1, y_train_1)) + custom_weight*sim_score\n",
    "    loss_1.backward()\n",
    "    \n",
    "    # Clip gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model_1.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer_1.step()\n",
    "    print(f'Classification Model Iter {i + 1}/{training_iter} - Loss: {loss_1.item()}')\n",
    "\n",
    "# Make predictions after training\n",
    "model_1.eval()\n",
    "likelihood_1.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    predictions = likelihood_1(model_1(X_test_1)).mean\n",
    "\n",
    "# Check for NaN predictions\n",
    "if torch.isnan(predictions).any():\n",
    "    print(\"NaN detected in predictions!\")\n",
    "else:\n",
    "    print(\"Predictions are valid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71bedf33-d4cd-4ed1-bf78-55cfc3cff5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw prediction mean (pre-softmax): tensor([[11.2274, 11.2274, 11.2274, 11.2274, 11.2274],\n",
      "        [11.2274, 11.2274, 11.2274, 11.2274, 11.2274],\n",
      "        [11.2274, 11.2274, 11.2274, 11.2274, 11.2274],\n",
      "        [11.2274, 11.2274, 11.2274, 11.2274, 11.2274],\n",
      "        [11.2274, 11.2274, 11.2274, 11.2274, 11.2274],\n",
      "        [11.2274, 11.2274, 11.2274, 11.2274, 11.2274],\n",
      "        [11.2274, 11.2274, 11.2274, 11.2274, 11.2274],\n",
      "        [11.2274, 11.2274, 11.2274, 11.2274, 11.2274],\n",
      "        [11.2274, 11.2274, 11.2274, 11.2274, 11.2274],\n",
      "        [11.2274, 11.2274, 11.2274, 11.2274, 11.2274],\n",
      "        [11.2274, 11.2274, 11.2274, 11.2274, 11.2274],\n",
      "        [11.2274, 11.2274, 11.2274, 11.2274, 11.2274],\n",
      "        [11.2274, 11.2274, 11.2274, 11.2274, 11.2274],\n",
      "        [11.2274, 11.2274, 11.2274, 11.2274, 11.2274],\n",
      "        [11.2274, 11.2274, 11.2274, 11.2274, 11.2274]])\n",
      "Predicted probabilities: tensor([[0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.0667, 0.0667]])\n",
      "Predicted labels: [0 0 0 0 0]\n",
      "Small batch predictions are valid, proceeding with full test set.\n",
      "Full predicted labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Set models and likelihoods to evaluation mode\n",
    "model_1.eval()\n",
    "likelihood_1.eval()\n",
    "\n",
    "# Make predictions (use a small batch from X_test_1 for diagnosis)\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    small_X_test_1 = X_test_1[:5]  # Take the first 5 samples for a small test\n",
    "    prediction = model_1(small_X_test_1)\n",
    "    \n",
    "    # Inspect the raw prediction mean before applying softmax\n",
    "    print(f\"Raw prediction mean (pre-softmax): {prediction.mean}\")\n",
    "\n",
    "    # Apply softmax and check the outputs\n",
    "    predicted_probabilities = torch.softmax(prediction.mean, dim=0)\n",
    "    print(f\"Predicted probabilities: {predicted_probabilities}\")\n",
    "\n",
    "    predicted_labels = predicted_probabilities.argmax(dim=0).detach().numpy()\n",
    "    print(f\"Predicted labels: {predicted_labels}\")\n",
    "\n",
    "# If everything seems fine in the small batch, proceed with the full prediction\n",
    "if not torch.isnan(prediction.mean).any():\n",
    "    print(\"Small batch predictions are valid, proceeding with full test set.\")\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        full_prediction = model_1(X_test_1)\n",
    "        full_predicted_probabilities = torch.softmax(full_prediction.mean, dim=0)\n",
    "        if torch.isnan(full_predicted_probabilities).any():\n",
    "            print(\"NaN detected in full predictions!\")\n",
    "        else:\n",
    "            full_predicted_labels = full_predicted_probabilities.argmax(dim=0).detach().numpy()\n",
    "            print(f\"Full predicted labels: {full_predicted_labels}\")\n",
    "else:\n",
    "    print(\"NaN detected in small batch predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82d24953-53de-4d23-bc71-b4d4567a2a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'true')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = label_encoder.inverse_transform(full_predicted_labels)\n",
    "trues = label_encoder.inverse_transform(y_test_1)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "axs[0].hist(preds, edgecolor='white', bins=20)\n",
    "axs[0].set_title('prediction')\n",
    "axs[1].hist(trues, edgecolor='white', bins=20)\n",
    "axs[1].set_title('true')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9572ab2-b989-44a2-b38a-ce10837c2663",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd695f-7b66-46d9-a6d9-2c605f8cdca9",
   "metadata": {},
   "source": [
    "Sources:\n",
    "1. https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/ModelList_GP_Regression.html\n",
    "2. https://jamesbrind.uk/posts/2d-gaussian-process-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c0ff8-2357-4fc0-ace3-79768843c4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
