{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b011d16-5e73-46f5-9e0d-3443ec2ff42b",
   "metadata": {},
   "source": [
    "# Pytorch Multi-output GPR\n",
    "\n",
    "The purpose of this notebook is to improve the predictions of Layer 1 soil type and soil thickness of Seattle from the preliminary work demostrated in notebook `4-pytorch_gpr.ipynb`.\n",
    "\n",
    "The tactics used to improve the model include: \n",
    "1. Performing a log scale transformation of the layer thickness target (instead of a standard scaler).\n",
    "2. Evaluating class imbalance of the classifier. \n",
    "3. Performing hyperparameter tuning, looking at different kernels and learning rates.\n",
    "4. Performing cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb804bf-e961-4804-a088-3f113bb7381f",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d69f4200-2066-4eab-96a2-c7eba0136384",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch --quiet\n",
    "!pip install gpytorch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e264b31-4228-4088-9235-9c5cc3f99f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.models import ApproximateGP, ExactGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "from gpytorch.likelihoods import SoftmaxLikelihood, GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13712b6b-e473-4e08-861a-1a4885d4eeff",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa664a05-0bd2-453a-9e5d-9068ce2cd200",
   "metadata": {},
   "source": [
    "#### Load training data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f4ce29-8f04-48e9-9944-01b8f6e4d434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOREHOLE_ID</th>\n",
       "      <th>BOREHOLE_NAME</th>\n",
       "      <th>BOREHOLE_TYPE</th>\n",
       "      <th>BOREHOLE_DEPTH_FT</th>\n",
       "      <th>ELEVATION_FT</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LAYER_NUMBER</th>\n",
       "      <th>TOP_DEPTH_FT</th>\n",
       "      <th>BOTTOM_DEPTH_FT</th>\n",
       "      <th>USCS</th>\n",
       "      <th>SIMPLE_USCS</th>\n",
       "      <th>LAYER_THICKNESS_FT</th>\n",
       "      <th>geometry</th>\n",
       "      <th>MAPPED_UNIT</th>\n",
       "      <th>SLOPE</th>\n",
       "      <th>ROUGHNESS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>EB-1</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>41.0</td>\n",
       "      <td>131.30</td>\n",
       "      <td>47.575005</td>\n",
       "      <td>-122.406037</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>OH</td>\n",
       "      <td>O</td>\n",
       "      <td>2.5</td>\n",
       "      <td>POINT (-122.40604 47.575)</td>\n",
       "      <td>Ql</td>\n",
       "      <td>8.126739</td>\n",
       "      <td>36.531616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>EB-2</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>40.0</td>\n",
       "      <td>130.90</td>\n",
       "      <td>47.574982</td>\n",
       "      <td>-122.406239</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>SC</td>\n",
       "      <td>S</td>\n",
       "      <td>5.0</td>\n",
       "      <td>POINT (-122.40624 47.57498)</td>\n",
       "      <td>Ql</td>\n",
       "      <td>8.056191</td>\n",
       "      <td>38.262520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>HB-1</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>10.5</td>\n",
       "      <td>46.30</td>\n",
       "      <td>47.512358</td>\n",
       "      <td>-122.394125</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>ML</td>\n",
       "      <td>M</td>\n",
       "      <td>7.5</td>\n",
       "      <td>POINT (-122.39412 47.51236)</td>\n",
       "      <td>Qss</td>\n",
       "      <td>12.098481</td>\n",
       "      <td>45.003693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>HB-2</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>6.9</td>\n",
       "      <td>47.70</td>\n",
       "      <td>47.512384</td>\n",
       "      <td>-122.394069</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>ML</td>\n",
       "      <td>M</td>\n",
       "      <td>5.0</td>\n",
       "      <td>POINT (-122.39407 47.51238)</td>\n",
       "      <td>Qss</td>\n",
       "      <td>12.098481</td>\n",
       "      <td>45.003693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>HB-3</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>13.5</td>\n",
       "      <td>40.90</td>\n",
       "      <td>47.512208</td>\n",
       "      <td>-122.394083</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>ML</td>\n",
       "      <td>M</td>\n",
       "      <td>8.6</td>\n",
       "      <td>POINT (-122.39408 47.51221)</td>\n",
       "      <td>Qss</td>\n",
       "      <td>12.098481</td>\n",
       "      <td>45.003693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10501</th>\n",
       "      <td>153201</td>\n",
       "      <td>B-1</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>44.0</td>\n",
       "      <td>26.35</td>\n",
       "      <td>47.437003</td>\n",
       "      <td>-122.242410</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>POINT (-122.24241 47.437)</td>\n",
       "      <td>Qaw</td>\n",
       "      <td>0.373783</td>\n",
       "      <td>1.591432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10502</th>\n",
       "      <td>153202</td>\n",
       "      <td>B-2</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.45</td>\n",
       "      <td>47.437208</td>\n",
       "      <td>-122.242890</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ML</td>\n",
       "      <td>M</td>\n",
       "      <td>2.0</td>\n",
       "      <td>POINT (-122.24289 47.43721)</td>\n",
       "      <td>Qaw</td>\n",
       "      <td>0.272720</td>\n",
       "      <td>1.453522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10503</th>\n",
       "      <td>153203</td>\n",
       "      <td>B-3</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>44.0</td>\n",
       "      <td>24.46</td>\n",
       "      <td>47.437427</td>\n",
       "      <td>-122.243398</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>POINT (-122.2434 47.43743)</td>\n",
       "      <td>Qaw</td>\n",
       "      <td>0.158798</td>\n",
       "      <td>1.009704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10504</th>\n",
       "      <td>153204</td>\n",
       "      <td>B-4</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>69.0</td>\n",
       "      <td>26.78</td>\n",
       "      <td>47.436984</td>\n",
       "      <td>-122.243486</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>POINT (-122.24349 47.43698)</td>\n",
       "      <td>Qaw</td>\n",
       "      <td>0.158798</td>\n",
       "      <td>1.009704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10505</th>\n",
       "      <td>153205</td>\n",
       "      <td>B-5</td>\n",
       "      <td>Geotechnical</td>\n",
       "      <td>54.0</td>\n",
       "      <td>23.75</td>\n",
       "      <td>47.437412</td>\n",
       "      <td>-122.242456</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>POINT (-122.24246 47.43741)</td>\n",
       "      <td>Qaw</td>\n",
       "      <td>0.373783</td>\n",
       "      <td>1.591432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10506 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       BOREHOLE_ID BOREHOLE_NAME BOREHOLE_TYPE  BOREHOLE_DEPTH_FT  \\\n",
       "0                7          EB-1  Geotechnical               41.0   \n",
       "1                8          EB-2  Geotechnical               40.0   \n",
       "2               15          HB-1  Geotechnical               10.5   \n",
       "3               16          HB-2  Geotechnical                6.9   \n",
       "4               17          HB-3  Geotechnical               13.5   \n",
       "...            ...           ...           ...                ...   \n",
       "10501       153201           B-1  Geotechnical               44.0   \n",
       "10502       153202           B-2  Geotechnical               39.0   \n",
       "10503       153203           B-3  Geotechnical               44.0   \n",
       "10504       153204           B-4  Geotechnical               69.0   \n",
       "10505       153205           B-5  Geotechnical               54.0   \n",
       "\n",
       "       ELEVATION_FT   LATITUDE   LONGITUDE  LAYER_NUMBER  TOP_DEPTH_FT  \\\n",
       "0            131.30  47.575005 -122.406037             1           0.0   \n",
       "1            130.90  47.574982 -122.406239             1           0.0   \n",
       "2             46.30  47.512358 -122.394125             1           0.0   \n",
       "3             47.70  47.512384 -122.394069             1           0.0   \n",
       "4             40.90  47.512208 -122.394083             1           0.0   \n",
       "...             ...        ...         ...           ...           ...   \n",
       "10501         26.35  47.437003 -122.242410             1           0.0   \n",
       "10502         24.45  47.437208 -122.242890             1           0.0   \n",
       "10503         24.46  47.437427 -122.243398             1           0.0   \n",
       "10504         26.78  47.436984 -122.243486             1           0.0   \n",
       "10505         23.75  47.437412 -122.242456             1           0.0   \n",
       "\n",
       "       BOTTOM_DEPTH_FT USCS SIMPLE_USCS  LAYER_THICKNESS_FT  \\\n",
       "0                  2.5   OH           O                 2.5   \n",
       "1                  5.0   SC           S                 5.0   \n",
       "2                  7.5   ML           M                 7.5   \n",
       "3                  5.0   ML           M                 5.0   \n",
       "4                  8.6   ML           M                 8.6   \n",
       "...                ...  ...         ...                 ...   \n",
       "10501              0.5  NaN         NaN                 0.5   \n",
       "10502              2.0   ML           M                 2.0   \n",
       "10503              0.5  NaN         NaN                 0.5   \n",
       "10504              0.5  NaN         NaN                 0.5   \n",
       "10505              0.5  NaN         NaN                 0.5   \n",
       "\n",
       "                          geometry MAPPED_UNIT      SLOPE  ROUGHNESS  \n",
       "0        POINT (-122.40604 47.575)          Ql   8.126739  36.531616  \n",
       "1      POINT (-122.40624 47.57498)          Ql   8.056191  38.262520  \n",
       "2      POINT (-122.39412 47.51236)         Qss  12.098481  45.003693  \n",
       "3      POINT (-122.39407 47.51238)         Qss  12.098481  45.003693  \n",
       "4      POINT (-122.39408 47.51221)         Qss  12.098481  45.003693  \n",
       "...                            ...         ...        ...        ...  \n",
       "10501    POINT (-122.24241 47.437)         Qaw   0.373783   1.591432  \n",
       "10502  POINT (-122.24289 47.43721)         Qaw   0.272720   1.453522  \n",
       "10503   POINT (-122.2434 47.43743)         Qaw   0.158798   1.009704  \n",
       "10504  POINT (-122.24349 47.43698)         Qaw   0.158798   1.009704  \n",
       "10505  POINT (-122.24246 47.43741)         Qaw   0.373783   1.591432  \n",
       "\n",
       "[10506 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training data CSV file compiled in `1-data_access.ipynb`\n",
    "input_file = '../data/2.1-seattle_layer1_trainingdata.csv'\n",
    "training_data = pd.read_csv(input_file)\n",
    "\n",
    "# Create a GeoDataFrame from the DataFrame\n",
    "training_data = gpd.GeoDataFrame(training_data, geometry=gpd.points_from_xy(training_data.LONGITUDE, training_data.LATITUDE))\n",
    "    \n",
    "# Set the CRS to WGS84 (latitude and longitude)\n",
    "training_data.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb9989-deeb-486b-8e0f-fe3f457fd7f6",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b471b6f-3014-4f81-9d7b-379e2d8e682c",
   "metadata": {},
   "source": [
    "#### Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c46a3a7a-5e3a-4e14-b974-91b4a6720104",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_data[['MAPPED_UNIT', 'SLOPE', 'ROUGHNESS']]\n",
    "y_1 = training_data['SIMPLE_USCS']\n",
    "y_2 = training_data['LAYER_THICKNESS_FT']\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(sparse_output=True), ['MAPPED_UNIT']),  # One-hot encode categorical features\n",
    "        ('num', StandardScaler(), ['SLOPE', 'ROUGHNESS'])  # Standardize numerical features\n",
    "    ])\n",
    "\n",
    "# Drop rows with NaN values in X and corresponding y_1 and y_2\n",
    "X = X.dropna()\n",
    "y_1 = y_1.loc[X.index].dropna()  # Ensure y_1 is aligned with X and has no NaN values\n",
    "y_2 = y_2.loc[X.index].dropna()  # Ensure y_2 is aligned with X and has no NaN values\n",
    "\n",
    "# Handle cases where dropping NaN results in mismatched lengths\n",
    "X = X.loc[y_1.index]  # Align X with the remaining non-NaN y_1\n",
    "y_2 = y_2.loc[y_1.index]  # Align y_2 with the remaining non-NaN y_1\n",
    "\n",
    "# Apply transformations to X\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "X_tensor = torch.tensor(X_processed.todense(), dtype=torch.float32) if scipy.sparse.issparse(X_processed) else torch.tensor(X_processed, dtype=torch.float32) \n",
    "\n",
    "# Handling target for SIMPLE_USCS\n",
    "label_encoder = LabelEncoder()\n",
    "y_1_encoded = label_encoder.fit_transform(y_1)\n",
    "y_1_tensor = torch.tensor(y_1_encoded, dtype=torch.long)\n",
    "\n",
    "# Handling target for LAYER_THICKNESS_FT\n",
    "# scaler = StandardScaler()\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "y_2_scaled = log_transformer.fit_transform(y_2.values.reshape(-1, 1))\n",
    "y_2_tensor = torch.tensor(y_2_scaled, dtype=torch.float32).squeeze()\n",
    "\n",
    "# Split the data into training and test sets for both models\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_tensor, y_1_tensor, test_size=0.2, random_state=42, stratify=y_1_tensor)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_tensor, y_2_tensor, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90866c21-8195-430d-8e42-61261992ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPClassificationModel(ApproximateGP):\n",
    "    def __init__(self, train_x, num_classes):\n",
    "        variational_distribution = CholeskyVariationalDistribution(train_x.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, train_x, variational_distribution, learn_inducing_locations=True)\n",
    "        super(GPClassificationModel, self).__init__(variational_strategy)\n",
    "        \n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x).expand([self.num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aae8782-58f7-402a-a911-b0ef4dc501c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b5e49e4-f88c-492c-877b-1e2845aea0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_score_gp(model_class, X, y, param_grid, cv=5):\n",
    "    kf = KFold(n_splits=cv)\n",
    "    best_score = float('inf')\n",
    "    best_params = None\n",
    "\n",
    "    for lr in param_grid['lr']:\n",
    "        for lengthscale in param_grid['lengthscale']:\n",
    "            for outputscale in param_grid['outputscale']:\n",
    "                fold_scores = []\n",
    "                \n",
    "                for train_index, val_index in kf.split(X):\n",
    "                    # print('running:',train_index, val_index)\n",
    "                    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "                    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "                    # Initialize model with specific hyperparameters\n",
    "                    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "                    model = model_class(X_train_fold, y_train_fold, likelihood)\n",
    "                    \n",
    "                    # Set hyperparameters\n",
    "                    model.covar_module.base_kernel.lengthscale = lengthscale\n",
    "                    model.covar_module.outputscale = outputscale\n",
    "                    \n",
    "                    # Training\n",
    "                    model.train()\n",
    "                    likelihood.train()\n",
    "\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "                    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "                    \n",
    "                    training_iter = 50\n",
    "                    for i in range(training_iter):\n",
    "                        optimizer.zero_grad()\n",
    "                        output = model(X_train_fold)\n",
    "                        loss = -mll(output, y_train_fold)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    likelihood.eval()\n",
    "                    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "                        val_output = model(X_val_fold)\n",
    "                        val_loss = -mll(val_output, y_val_fold)\n",
    "                        fold_scores.append(val_loss.item())\n",
    "\n",
    "                avg_score = np.mean(fold_scores)\n",
    "                \n",
    "                if avg_score < best_score:\n",
    "                    best_score = avg_score\n",
    "                    best_params = {'lr': lr, 'lengthscale': lengthscale, 'outputscale': outputscale}\n",
    "\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d72fdb86-f9be-452d-a59e-b30abc67a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lr': [0.01, 0.05, 0.1],\n",
    "    'lengthscale': [0.1, 0.5, 1.0, 2.0],\n",
    "    'outputscale': [0.5, 1.0, 2.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09725995-8ee9-4094-a996-483cfa12c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the likelihood and model for multiclass classification\n",
    "num_classes = len(label_encoder.classes_)\n",
    "likelihood_1 = gpytorch.likelihoods.SoftmaxLikelihood(num_classes=num_classes, mixing_weights=None)\n",
    "model_1 = GPClassificationModel(X_train_1, num_classes=num_classes)\n",
    "\n",
    "# Initialize the likelihood and model for regression\n",
    "likelihood_2 = GaussianLikelihood()\n",
    "model_2 = GPRegressionModel(X_train_2, y_train_2, likelihood_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "186fdab4-64a0-4155-8381-77c30651925f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): SoftmaxLikelihood()\n",
       "  (1): GaussianLikelihood(\n",
       "    (noise_covar): HomoskedasticNoise(\n",
       "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a ModuleList to store the models\n",
    "models = torch.nn.ModuleList([model_1, model_2])\n",
    "likelihoods = torch.nn.ModuleList([likelihood_1, likelihood_2])\n",
    "\n",
    "# Set into eval mode for all models and likelihoods\n",
    "models.eval()\n",
    "likelihoods.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31ec59-1b17-4549-b339-1790ae7c1f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Model Iter 1/100 - Loss: 2.0620131492614746\n",
      "Classification Model Iter 2/100 - Loss: 3.249990463256836\n"
     ]
    }
   ],
   "source": [
    "# Training Settings\n",
    "training_iter = 100  # Number of training iterations\n",
    "lr = 0.1\n",
    "lengthscale = 0.1\n",
    "outputscale = 2.0\n",
    "\n",
    "model_1.train()\n",
    "likelihood_1.train()\n",
    "\n",
    "optimizer_1 = torch.optim.Adam(model_1.parameters(), lr=lr)\n",
    "mll_1 = gpytorch.mlls.VariationalELBO(likelihood_1, model_1, num_data=X_train_1.size(0))\n",
    "\n",
    "# Implement Gradient Clipping\n",
    "for i in range(training_iter):\n",
    "    optimizer_1.zero_grad()\n",
    "    output_1 = model_1(X_train_1)\n",
    "    \n",
    "    # Check for NaNs in the output\n",
    "    if torch.isnan(output_1.mean).any():\n",
    "        print(f\"Warning: NaN detected in output at iteration {i + 1}\")\n",
    "        break\n",
    "    \n",
    "    loss_1 = -mll_1(output_1, y_train_1)\n",
    "    loss_1.backward()\n",
    "    \n",
    "    # Clip gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model_1.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer_1.step()\n",
    "    print(f'Classification Model Iter {i + 1}/{training_iter} - Loss: {loss_1.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e94fa1-82f3-4f8f-b0ab-08485e5ba361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Regression Model Training\n",
    "# model_2.train()\n",
    "# likelihood_2.train()\n",
    "\n",
    "# optimizer_2 = torch.optim.Adam(model_2.parameters(), lr=0.1)\n",
    "# mll_2 = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood_2, model_2)\n",
    "\n",
    "# for i in range(training_iter):\n",
    "#     optimizer_2.zero_grad()\n",
    "#     output_2 = model_2(X_train_2)\n",
    "    \n",
    "#     # Calculate loss\n",
    "#     loss_2 = -mll_2(output_2, y_train_2)\n",
    "    \n",
    "#     loss_2.backward()\n",
    "#     optimizer_2.step()\n",
    "#     print(f'Regression Model Iter {i + 1}/{training_iter} - Loss: {loss_2.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff3c7d-06f1-4005-8020-a6b8c941b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params, best_score = cross_val_score_gp(GPRegressionModel, X_train_2, y_train_2, param_grid, cv=5)\n",
    "# print(f'Best Hyperparameters: {best_params}')\n",
    "# print(f'Best Cross-Validation Score: {best_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a860a-8901-428c-9c80-dba98c01b96b",
   "metadata": {},
   "source": [
    "From hyperparamer tuning:\n",
    "Best Hyperparameters: {'lr': 0.1, 'lengthscale': 0.1, 'outputscale': 2.0}\n",
    "Best Cross-Validation Score: 1.1007592916488647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3514b5-7659-4d24-85be-a31d80c0be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "training_iter = 100  # Number of training iterations\n",
    "lr = 0.1\n",
    "lengthscale = 0.1\n",
    "outputscale = 2.0\n",
    "\n",
    "# From hyperparameter tuning: \n",
    "best_params = {'lr': 0.1, 'lengthscale': 0.1, 'outputscale': 2.0}\n",
    "\n",
    "# Initialize the final model with best hyperparameters\n",
    "likelihood_2 = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model_2 = GPRegressionModel(X_train_2, y_train_2, likelihood_2)\n",
    "model_2.covar_module.base_kernel.lengthscale = best_params['lengthscale']\n",
    "model_2.covar_module.outputscale = best_params['outputscale']\n",
    "\n",
    "optimizer_2 = torch.optim.Adam(model_2.parameters(), lr=best_params['lr'])\n",
    "mll_2 = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood_2, model_2)\n",
    "\n",
    "# Train the model\n",
    "model_2.train()\n",
    "likelihood_2.train()\n",
    "for i in range(training_iter):\n",
    "    optimizer_2.zero_grad()\n",
    "    output_2 = model_2(X_train_2)\n",
    "    loss_2 = -mll_2(output_2, y_train_2)\n",
    "    loss_2.backward()\n",
    "    optimizer_2.step()\n",
    "    print(f'Regression Model Iter {i + 1}/{training_iter} - Loss: {loss_2.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a9885-3c31-4912-9bd3-59dbd8f8c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set models and likelihoods to evaluation mode\n",
    "model_1.eval()\n",
    "model_2.eval()\n",
    "likelihood_1.eval()\n",
    "likelihood_2.eval()\n",
    "\n",
    "# Initialize plots\n",
    "f, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Make predictions (use the test tensors X_train_1 and X_train_2)\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    predictions = [likelihood(model(X_train)) for model, likelihood, X_train in zip(models, likelihoods, [X_train_1, X_train_2])]\n",
    "\n",
    "# Iterate through the models and predictions\n",
    "for i, (submodel, prediction, ax) in enumerate(zip(models, predictions, axs)):\n",
    "    if i == 0:  # Classification Model\n",
    "        predicted_probabilities = torch.softmax(prediction.mean, dim=0)  # Apply softmax across the class dimension\n",
    "        predicted_labels = predicted_probabilities.argmax(dim=0).detach().numpy()  # Use argmax on the class dimension\n",
    "        print(predicted_probabilities)\n",
    "        print(predicted_labels)\n",
    "        \n",
    "        # Ensure y_train_1 is flattened to match the predicted_labels shape\n",
    "        y_train_1_flat = y_train_1.numpy().flatten()\n",
    "\n",
    "        # Plot Predicted vs Observed labels\n",
    "        if y_train_1_flat.shape == predicted_labels.shape:\n",
    "            ax.scatter(y_train_1_flat, predicted_labels, c='b', marker='o')  # Scatter plot of predicted vs observed\n",
    "            ax.plot([0, len(label_encoder.classes_) - 1], [0, len(label_encoder.classes_) - 1], 'r--')  # Diagonal line for reference\n",
    "            ax.set_title('Classification: Predicted vs Observed')\n",
    "            ax.set_xlabel('Observed Labels')\n",
    "            ax.set_ylabel('Predicted Labels')\n",
    "            ax.set_xlim([0, len(label_encoder.classes_) - 1])\n",
    "            ax.set_ylim([0, len(label_encoder.classes_) - 1])\n",
    "            ax.legend(['Reference Line', 'Predicted vs Observed'])\n",
    "        else:\n",
    "            print(\"Warning: Shapes of y_train_1 and predicted_labels do not match!\")\n",
    "\n",
    "    else:  # Regression Model\n",
    "        mean = prediction.mean\n",
    "        lower, upper = prediction.confidence_region()\n",
    "\n",
    "        # Reverse the standardization for the mean and confidence intervals\n",
    "        mean_unscaled = scaler.inverse_transform(mean.numpy().reshape(-1, 1)).flatten()\n",
    "        lower_unscaled = scaler.inverse_transform(lower.numpy().reshape(-1, 1)).flatten()\n",
    "        upper_unscaled = scaler.inverse_transform(upper.numpy().reshape(-1, 1)).flatten()\n",
    "        y_train_unscaled = scaler.inverse_transform(y_train_2.numpy().reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Plot the unscaled test data and predictions\n",
    "        ax.plot(range(len(y_train_unscaled)), y_train_unscaled, 'k*')  # Plot the unscaled test targets\n",
    "        ax.plot(range(len(mean_unscaled)), mean_unscaled, 'b')  # Unscaled predictive mean as blue line\n",
    "        # Shade in confidence interval\n",
    "        ax.fill_between(range(len(mean_unscaled)), lower_unscaled, upper_unscaled, alpha=0.5)\n",
    "        ax.set_ylim([min(lower_unscaled), max(upper_unscaled)])  # Adjust the y-axis limits\n",
    "        ax.set_title('Regression: Mean and Confidence Interval (Unscaled)')\n",
    "        ax.legend(['Observed Data (Unscaled)', 'Mean (Unscaled)', 'Confidence Interval'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e76c9-8c6f-4ce9-8b47-17235d141861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize plots\n",
    "f, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Make predictions (use the test tensors X_test_1 and X_test_2)\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    predictions = [likelihood(model(X_test)) for model, likelihood, X_test in zip(models, likelihoods, [X_test_1, X_test_2])]\n",
    "\n",
    "# Iterate through the models and predictions\n",
    "for i, (submodel, prediction, ax) in enumerate(zip(models, predictions, axs)):\n",
    "    if i == 0:  # Classification Model\n",
    "        predicted_probabilities = torch.softmax(prediction.mean, dim=0)  # Apply softmax across the class dimension\n",
    "        predicted_labels = predicted_probabilities.argmax(dim=0).detach().numpy()  # Use argmax on the class dimension\n",
    "        print(predicted_probabilities)\n",
    "        print(predicted_labels)\n",
    "\n",
    "        # Ensure y_test_1 is flattened to match the predicted_labels shape\n",
    "        y_test_1_flat = y_test_1.numpy().flatten()\n",
    "\n",
    "        # Plot Predicted vs Observed labels\n",
    "        if y_test_1_flat.shape == predicted_labels.shape:\n",
    "            ax.scatter(y_test_1_flat, predicted_labels, c='b', marker='o')  # Scatter plot of predicted vs observed\n",
    "            ax.plot([0, len(label_encoder.classes_) - 1], [0, len(label_encoder.classes_) - 1], 'r--')  # Diagonal line for reference\n",
    "            ax.set_title('Classification: Predicted vs Observed')\n",
    "            ax.set_xlabel('Observed Labels')\n",
    "            ax.set_ylabel('Predicted Labels')\n",
    "            ax.set_xlim([0, len(label_encoder.classes_) - 1])\n",
    "            ax.set_ylim([0, len(label_encoder.classes_) - 1])\n",
    "            ax.legend(['Reference Line', 'Predicted vs Observed'])\n",
    "        else:\n",
    "            print(\"Warning: Shapes of y_test_1 and predicted_labels do not match!\")\n",
    "\n",
    "    else:  # Regression Model\n",
    "        mean = prediction.mean\n",
    "        lower, upper = prediction.confidence_region()\n",
    "\n",
    "        # Reverse the standardization for the mean and confidence intervals\n",
    "        mean_unscaled = scaler.inverse_transform(mean.numpy().reshape(-1, 1)).flatten()\n",
    "        lower_unscaled = scaler.inverse_transform(lower.numpy().reshape(-1, 1)).flatten()\n",
    "        upper_unscaled = scaler.inverse_transform(upper.numpy().reshape(-1, 1)).flatten()\n",
    "        y_test_unscaled = scaler.inverse_transform(y_test_2.numpy().reshape(-1, 1)).flatten()\n",
    "\n",
    "        # # Plot the unscaled test data and predictions\n",
    "        # ax.plot(range(len(y_test_unscaled)), y_test_unscaled, 'k*')  # Plot the unscaled test targets\n",
    "        # ax.plot(range(len(mean_unscaled)), mean_unscaled, 'b')  # Unscaled predictive mean as blue line\n",
    "        # # Shade in confidence interval\n",
    "        # ax.fill_between(range(len(mean_unscaled)), lower_unscaled, upper_unscaled, alpha=0.5)\n",
    "        # ax.set_ylim([min(lower_unscaled), max(upper_unscaled)])  # Adjust the y-axis limits\n",
    "        # ax.set_title('Regression: Mean and Confidence Interval (Unscaled)')\n",
    "        # ax.legend(['Observed Data (Unscaled)', 'Prediction Mean (Unscaled)', 'Confidence Interval'])        \n",
    "        ax.scatter(y_test_unscaled, mean_unscaled, c='blue', alpha=0.6, edgecolors='k', label='Predicted vs Observed')\n",
    "        ax.set_xlim(0,60)\n",
    "        ax.set_ylim(0,60)\n",
    "        ax.set_xlabel('Observed')\n",
    "        ax.set_ylabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9572ab2-b989-44a2-b38a-ce10837c2663",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd695f-7b66-46d9-a6d9-2c605f8cdca9",
   "metadata": {},
   "source": [
    "Sources:\n",
    "1. https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/ModelList_GP_Regression.html\n",
    "2. https://jamesbrind.uk/posts/2d-gaussian-process-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c0ff8-2357-4fc0-ace3-79768843c4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
